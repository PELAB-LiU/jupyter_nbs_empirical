{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4566c1f6",
   "metadata": {},
   "source": [
    "# Notebook error analysis \n",
    "\n",
    "## Refining error types - Word-embeddings\n",
    "\n",
    "retrain word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce0449e",
   "metadata": {},
   "source": [
    "### Prepare dataset for re-training word embeddings\n",
    "Construct a dataset for retraining word embeddings. It needs all possible data (as much data as possible), no filtering, keep the preprocess part the same. Using traceback because it may cover more words.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac9328ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path_default=WindowsPath('C:/Users/yirwa29/Downloads/Dataset-Nb')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import config, util\n",
    "\n",
    "path_err_k = config.path_kaggle_error_process.joinpath('nberror_k_eid_p.xlsx')\n",
    "df_err_k = pd.read_excel(path_err_k)\n",
    "path_err_g = config.path_github_error_process.joinpath('nberror_g_all_eid_p.xlsx')\n",
    "df_err_g = pd.read_excel(path_err_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3c3c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_err_evalue = pd.concat([df_err_k[[\"fname\",\"eid\",\"ename\",\"ename_mapped\",\"evalue\"]], df_err_g[[\"fname\",\"eid\",\"ename\",\"ename_mapped\",\"evalue\"]]])\n",
    "df_err_traceback = pd.concat([df_err_k[[\"fname\",\"eid\",\"ename\",\"ename_mapped\",\"traceback\"]], df_err_g[[\"fname\",\"eid\",\"ename\",\"ename_mapped\",\"traceback\"]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96a9bbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yirwa29\\AppData\\Local\\anaconda3\\envs\\chatgpt\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cluster_util\n",
    "\n",
    "df_err_traceback[\"traceback_tokenized\"] = df_err_traceback.traceback.apply(cluster_util.preprocess_text)\n",
    "df_err_evalue[\"evalue_tokenized\"] = df_err_evalue.evalue.apply(cluster_util.preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86127c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_err_evalue.to_excel(config.path_default.joinpath('nberror_evalue.xlsx'), index=False, engine='xlsxwriter')\n",
    "df_err_traceback.to_excel(config.path_default.joinpath('nberror_traceback.xlsx'), index=False, engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d0bad2",
   "metadata": {},
   "source": [
    "### Re-training word embeddings with Word2Vec and FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a985a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path_default=WindowsPath('C:/Users/yirwa29/Downloads/Dataset-Nb')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import config, util\n",
    "import retrain_word2vec\n",
    "\n",
    "df_err_evalue = pd.read_excel(config.path_default.joinpath('nberror_evalue.xlsx'))\n",
    "df_err_traceback = pd.read_excel(config.path_default.joinpath('nberror_traceback.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6172f6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_traindata = list()\n",
    "tokenized_traindata.extend(df_err_evalue.evalue_tokenized.to_list())\n",
    "tokenized_traindata.extend(df_err_traceback.traceback_tokenized.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77988245",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_traindata = [str(ele).split(\" \") for ele in tokenized_traindata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99b37891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(retrain_word2vec)\n",
    "import retrain_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5292dabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has finished. Model saved in file.\n"
     ]
    }
   ],
   "source": [
    "import cluster_util\n",
    "\n",
    "# w2v_model = retrain_word2vec.retrain_word2vec(tokenized_traindata, config.path_w2v_models, vector_size=200)\n",
    "\n",
    "# glove_vectors = cluster_util.load_glove(config.path_default.joinpath(\"glove.6B/glove.6B.200d.txt\"))\n",
    "# sw2v_model = retrain_word2vec.finetune_word2vec(tokenized_traindata, glove_vectors, config.path_w2v_models, \n",
    "#                                                         modelname=\"nberr_word2vec_glove_finetune.model\",vector_size=200)\n",
    "\n",
    "# sw2v_model = retrain_word2vec.retrain_subword2vec(tokenized_traindata, config.path_w2v_models, vector_size=200)\n",
    "\n",
    "binpath = r\"C:\\Users\\yirwa29\\Downloads\\Dataset-Nb/wiki.en.bin\"\n",
    "sw2v_model = retrain_word2vec.finetune_subword2vec(tokenized_traindata, binpath, config.path_w2v_models, vector_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b65a2627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('dtensor', 0.9035197496414185),\n",
       " ('qtensor', 0.8982171416282654),\n",
       " ('adptensor', 0.8850652575492859),\n",
       " ('imtensor', 0.8759932518005371),\n",
       " ('sttensor', 0.8726184368133545),\n",
       " ('ytensor', 0.8705863356590271),\n",
       " ('ckkstensor', 0.8650293350219727),\n",
       " ('tensorz', 0.8645163774490356),\n",
       " ('dltensor', 0.8611711263656616),\n",
       " ('xtensor', 0.8575398325920105)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(sw2v_model.wv[\"tensor\"]))\n",
    "sw2v_model.wv.most_similar([\"tensor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31700d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## usage\n",
    "# import cluster_util,retrain_word2vec,config\n",
    "# w2v_model = retrain_word2vec.load_word2vec(config.path_w2v_models)\n",
    "# cluster_util.vectorizer_word2vec(\"tensor shape mismatch\", w2v_model.wv, w2v_model.vector_size, aggregation='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eeff72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
