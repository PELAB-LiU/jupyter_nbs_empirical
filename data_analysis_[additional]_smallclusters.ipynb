{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74352832",
   "metadata": {},
   "source": [
    "## Check crashes in small clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a16127f",
   "metadata": {},
   "source": [
    "Get crashes in smaller clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aceb91d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path_default=WindowsPath('C:/Users/yirwa29/Downloads/data_jupyter_nbs_empirical')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import utils.config as config\n",
    "import utils.util as util\n",
    "import numpy as np\n",
    "\n",
    "df_err_grouped_k = pd.read_excel(config.path_default.joinpath('Clustering/clusters_Kaggle.xlsx'))\n",
    "df_err_grouped_g = pd.read_excel(config.path_default.joinpath('Clustering/clusters_GitHub.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8bc5e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_clusters_g = pd.read_excel(config.path_default.joinpath('Sampling/cluster_size_samples_g.xlsx'))\n",
    "selected_clusters_k = pd.read_excel(config.path_default.joinpath('Sampling/cluster_size_samples_k.xlsx'))\n",
    "\n",
    "# big clusters (sample size >= 5)\n",
    "clusters_big_k = selected_clusters_k[selected_clusters_k.sample_size>=5].cluster_id[1:]\n",
    "clusters_big_g = selected_clusters_g[selected_clusters_g.sample_size>=5].cluster_id[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a04fcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get crashes from small clusters (sample size < 5)\n",
    "df_err_grouped_k_small = df_err_grouped_k[~df_err_grouped_k.pregroup_cluster.isin(clusters_big_k)]\n",
    "df_err_grouped_g_small = df_err_grouped_g[~df_err_grouped_g.pregroup_cluster.isin(clusters_big_g)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03e08383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select 10 small clusters from each source\n",
    "sample_k_small_clusters = np.random.choice(df_err_grouped_k_small.pregroup_cluster.unique(), size=10, replace=False)\n",
    "sample_g_small_clusters = np.random.choice(df_err_grouped_g_small.pregroup_cluster.unique(), size=10, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a31762d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m+++++1/10: Error examples for Cluster 7167 in Kaggle error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/1 with eid 35f8aa8f-f76a-3505-8641-8aad4332d21c.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "TypeError                                 Traceback (most recent call last)\n",
      "Cell In[76], line 30\n",
      "     21 param_grid = {\n",
      "     22     'lstm__units': [10, 20, 30],\n",
      "     23     'lstm__dropout': [0.2, 0.3, 0.4],\n",
      "   (...)\n",
      "     26     'batch_size': [32, 64]\n",
      "     27 }\n",
      "     29 # Call the function to find the best parameters\n",
      "---> 30 best_params, best_model = find_best_lstm_parameters(X_train, y_train, param_grid)\n",
      "     32 # Print the best parameters\n",
      "     33 print(\"Best Parameters:\")\n",
      "\n",
      "Cell In[76], line 11, in find_best_lstm_parameters(X_train, y_train, param_grid)\n",
      "      8 lstm.compile(loss=huber_loss, optimizer='adam')\n",
      "     10 grid_search = GridSearchCV(estimator=lstm, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3)\n",
      "---> 11 grid_search.fit(X_train, y_train)\n",
      "     13 best_params = grid_search.best_params_\n",
      "     14 best_model = grid_search.best_estimator_\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_search.py:788, in BaseSearchCV.fit(self, X, y, groups, **fit_params)\n",
      "    785 cv_orig = check_cv(self.cv, y, classifier=is_classifier(estimator))\n",
      "    786 n_splits = cv_orig.get_n_splits(X, y, groups)\n",
      "--> 788 base_estimator = clone(self.estimator)\n",
      "    790 parallel = Parallel(n_jobs=self.n_jobs, pre_dispatch=self.pre_dispatch)\n",
      "    792 fit_and_score_kwargs = dict(\n",
      "    793     scorer=scorers,\n",
      "    794     fit_params=fit_params,\n",
      "   (...)\n",
      "    800     verbose=self.verbose,\n",
      "    801 )\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/sklearn/base.py:79, in clone(estimator, safe)\n",
      "     73             raise TypeError(\n",
      "     74                 \"Cannot clone object. \"\n",
      "     75                 + \"You should provide an instance of \"\n",
      "     76                 + \"scikit-learn estimator instead of a class.\"\n",
      "     77             )\n",
      "     78         else:\n",
      "---> 79             raise TypeError(\n",
      "     80                 \"Cannot clone object '%s' (type %s): \"\n",
      "     81                 \"it does not seem to be a scikit-learn \"\n",
      "     82                 \"estimator as it does not implement a \"\n",
      "     83                 \"'get_params' method.\" % (repr(estimator), type(estimator))\n",
      "     84             )\n",
      "     86 klass = estimator.__class__\n",
      "     87 new_object_params = estimator.get_params(deep=False)\n",
      "\n",
      "TypeError: Cannot clone object '<keras.engine.sequential.Sequential object at 0x797dbbc75960>' (type <class 'keras.engine.sequential.Sequential'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' method.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 7167 in Kaggle error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++2/10: Error examples for Cluster 7030 in Kaggle error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/1 with eid bf5921ee-e2d4-3a8b-bcb8-388874231340.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "/tmp/ipykernel_27/1224909377.py in <module>\n",
      "----> 1 train_hdf_cite = pd.read_hdf(train_cite_path)\n",
      "      2 print_info(train_hdf_cite,'Train HDF Input Data')\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/io/pytables.py in read_hdf(path_or_buf, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)\n",
      "    437             if len(groups) == 0:\n",
      "    438                 raise ValueError(\n",
      "--> 439                     \"Dataset(s) incompatible with Pandas data types, \"\n",
      "    440                     \"not table, or no datasets found in HDF5 file.\"\n",
      "    441                 )\n",
      "\n",
      "ValueError: Dataset(s) incompatible with Pandas data types, not table, or no datasets found in HDF5 file.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 7030 in Kaggle error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++3/10: Error examples for Cluster 720 in Kaggle error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/5 with eid 8992eaae-d9db-30e9-b0d5-4b9e87435a63.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "Cell In[37], line 4\n",
      "      1 num_epochs = 30\n",
      "      3 # Train the model\n",
      "----> 4 history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)\n",
      "     67     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "     68     # To get the full stack trace, call:\n",
      "     69     # `tf.debugging.disable_traceback_filtering()`\n",
      "---> 70     raise e.with_traceback(filtered_tb) from None\n",
      "     71 finally:\n",
      "     72     del filtered_tb\n",
      "\n",
      "File /tmp/__autograph_generated_filer31o7nqy.py:15, in outer_factory.<locals>.inner_factory.<locals>.tf__test_function(iterator)\n",
      "     13 try:\n",
      "     14     do_return = True\n",
      "---> 15     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\n",
      "     16 except:\n",
      "     17     do_return = False\n",
      "\n",
      "ValueError: in user code:\n",
      "\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1852, in test_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1836, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1824, in run_step  **\n",
      "        outputs = model.test_step(data)\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1788, in test_step\n",
      "        y_pred = self(x, training=False)\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 298, in assert_input_compatibility\n",
      "        raise ValueError(\n",
      "\n",
      "    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 32), found shape=(None, 38)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 2/5 with eid a7326eac-57f1-3343-aa8f-ee5364545b78.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "Cell In[47], line 16\n",
      "      9 mushroom_model_01.compile(\n",
      "     10     loss=tf.keras.losses.mae, # mae = mean absolute error aka L1 loss, shape is v-like\n",
      "     11     optimizer=tf.keras.optimizers.Adam(),\n",
      "     12     metrics=[\"mae\"]\n",
      "     13 )\n",
      "     15 # Step three: Fit the model\n",
      "---> 16 mushroom_model_01_history = mushroom_model_01.fit(X_train, y_train, epochs=100)\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)\n",
      "     67     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "     68     # To get the full stack trace, call:\n",
      "     69     # `tf.debugging.disable_traceback_filtering()`\n",
      "---> 70     raise e.with_traceback(filtered_tb) from None\n",
      "     71 finally:\n",
      "     72     del filtered_tb\n",
      "\n",
      "File /tmp/__autograph_generated_filev_nhl5vl.py:15, in outer_factory.<locals>.inner_factory.<locals>.tf__train_function(iterator)\n",
      "     13 try:\n",
      "     14     do_return = True\n",
      "---> 15     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\n",
      "     16 except:\n",
      "     17     do_return = False\n",
      "\n",
      "ValueError: in user code:\n",
      "\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n",
      "        outputs = model.train_step(data)\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1050, in train_step\n",
      "        y_pred = self(x, training=True)\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 298, in assert_input_compatibility\n",
      "        raise ValueError(\n",
      "\n",
      "    ValueError: Input 0 of layer \"sequential_8\" is incompatible with the layer: expected shape=(None, 6499), found shape=(None, 117)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 3/5 with eid 8fabf81c-f8e1-3192-86f6-1ffbb488c51b.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "/tmp/ipykernel_23/2794559914.py in <module>\n",
      "      5 sample_y = np.expand_dims(y, axis=0)\n",
      "      6 model = get_denoising_autoencoder(sample_y.shape)\n",
      "----> 7 model = model.fit(x_train, x_train, epochs=10, batch_size=32, validation_split=0.2)\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\n",
      "   1182                 _r=1):\n",
      "   1183               callbacks.on_train_batch_begin(step)\n",
      "-> 1184               tmp_logs = self.train_function(iterator)\n",
      "   1185               if data_handler.should_sync:\n",
      "   1186                 context.async_wait()\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\n",
      "    883 \n",
      "    884       with OptionalXlaContext(self._jit_compile):\n",
      "--> 885         result = self._call(*args, **kwds)\n",
      "    886 \n",
      "    887       new_tracing_count = self.experimental_get_tracing_count()\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\n",
      "    931       # This is the first call of __call__, so we have to initialize.\n",
      "    932       initializers = []\n",
      "--> 933       self._initialize(args, kwds, add_initializers_to=initializers)\n",
      "    934     finally:\n",
      "    935       # At this point we know that the initialization is complete (or less\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\n",
      "    758     self._concrete_stateful_fn = (\n",
      "    759         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n",
      "--> 760             *args, **kwds))\n",
      "    761 \n",
      "    762     def invalid_creator_scope(*unused_args, **unused_kwds):\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\n",
      "   3064       args, kwargs = None, None\n",
      "   3065     with self._lock:\n",
      "-> 3066       graph_function, _ = self._maybe_define_function(args, kwargs)\n",
      "   3067     return graph_function\n",
      "   3068 \n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\n",
      "   3461 \n",
      "   3462           self._function_cache.missed.add(call_context_key)\n",
      "-> 3463           graph_function = self._create_graph_function(args, kwargs)\n",
      "   3464           self._function_cache.primary[cache_key] = graph_function\n",
      "   3465 \n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\n",
      "   3306             arg_names=arg_names,\n",
      "   3307             override_flat_arg_shapes=override_flat_arg_shapes,\n",
      "-> 3308             capture_by_value=self._capture_by_value),\n",
      "   3309         self._function_attributes,\n",
      "   3310         function_spec=self.function_spec,\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\n",
      "   1005         _, original_func = tf_decorator.unwrap(python_func)\n",
      "   1006 \n",
      "-> 1007       func_outputs = python_func(*func_args, **func_kwargs)\n",
      "   1008 \n",
      "   1009       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\n",
      "    666         # the function a weak reference to itself to avoid a reference cycle.\n",
      "    667         with OptionalXlaContext(compile_with_xla):\n",
      "--> 668           out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "    669         return out\n",
      "    670 \n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\n",
      "    992           except Exception as e:  # pylint:disable=broad-except\n",
      "    993             if hasattr(e, \"ag_error_metadata\"):\n",
      "--> 994               raise e.ag_error_metadata.to_exception(e)\n",
      "    995             else:\n",
      "    996               raise\n",
      "\n",
      "ValueError: in user code:\n",
      "\n",
      "    /opt/conda/lib/python3.7/site-packages/keras/engine/training.py:853 train_function  *\n",
      "        return step_function(self, iterator)\n",
      "    /opt/conda/lib/python3.7/site-packages/keras/engine/training.py:842 step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n",
      "        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n",
      "    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n",
      "        return self._call_for_each_replica(fn, args, kwargs)\n",
      "    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n",
      "        return fn(*args, **kwargs)\n",
      "    /opt/conda/lib/python3.7/site-packages/keras/engine/training.py:835 run_step  **\n",
      "        outputs = model.train_step(data)\n",
      "    /opt/conda/lib/python3.7/site-packages/keras/engine/training.py:787 train_step\n",
      "        y_pred = self(x, training=True)\n",
      "    /opt/conda/lib/python3.7/site-packages/keras/engine/base_layer.py:1020 __call__\n",
      "        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n",
      "    /opt/conda/lib/python3.7/site-packages/keras/engine/input_spec.py:269 assert_input_compatibility\n",
      "        ', found shape=' + display_shape(x.shape))\n",
      "\n",
      "    ValueError: Input 0 is incompatible with layer model_9: expected shape=(None, 1, 110250), found shape=(None, 110250)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 4/5 with eid 1669cf5e-cc65-3807-a4c1-d25a1a55029b.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "Cell In[61], line 1\n",
      "----> 1 model.fit(x_train,y_train,epochs=10)\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)\n",
      "     67     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "     68     # To get the full stack trace, call:\n",
      "     69     # `tf.debugging.disable_traceback_filtering()`\n",
      "---> 70     raise e.with_traceback(filtered_tb) from None\n",
      "     71 finally:\n",
      "     72     del filtered_tb\n",
      "\n",
      "File /tmp/__autograph_generated_fileb23jlov0.py:15, in outer_factory.<locals>.inner_factory.<locals>.tf__train_function(iterator)\n",
      "     13 try:\n",
      "     14     do_return = True\n",
      "---> 15     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\n",
      "     16 except:\n",
      "     17     do_return = False\n",
      "\n",
      "ValueError: in user code:\n",
      "\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n",
      "        outputs = model.train_step(data)\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1023, in train_step\n",
      "        y_pred = self(x, training=True)\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n",
      "        raise ValueError(\n",
      "\n",
      "    ValueError: Input 0 of layer \"model_2\" is incompatible with the layer: expected shape=(None, 28, 28), found shape=(None, 5)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 5/5 with eid 8b4e2778-1564-3de5-8dc3-4ca38d77c20c.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "/tmp/ipykernel_23/2009021976.py in <module>\n",
      "      6 img = cv2.resize(img,(600,600))\n",
      "      7 img = img.reshape(1,600,600,3)\n",
      "----> 8 predictions = model(x)\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)\n",
      "     68             # To get the full stack trace, call:\n",
      "     69             # `tf.debugging.disable_traceback_filtering()`\n",
      "---> 70             raise e.with_traceback(filtered_tb) from None\n",
      "     71         finally:\n",
      "     72             del filtered_tb\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/keras/engine/input_spec.py in assert_input_compatibility(input_spec, inputs, layer_name)\n",
      "    294                     if spec_dim != dim:\n",
      "    295                         raise ValueError(\n",
      "--> 296                             f'Input {input_index} of layer \"{layer_name}\" is '\n",
      "    297                             \"incompatible with the layer: \"\n",
      "    298                             f\"expected shape={spec.shape}, \"\n",
      "\n",
      "ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 600, 600, 3), found shape=(600, 600, 3)\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 720 in Kaggle error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++4/10: Error examples for Cluster 7266 in Kaggle error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/3 with eid c87617a0-2200-3ca8-9919-f6228eb7ed48.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "TypeError                                 Traceback (most recent call last)\n",
      "TypeError: float() argument must be a string or a number, not 'list'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "ValueError                                Traceback (most recent call last)\n",
      "/tmp/ipykernel_208/3356561118.py in <module>\n",
      "      1 clf = LogisticRegression(solver='liblinear')\n",
      "----> 2 clf.fit(X_train, y_train)\n",
      "      3 y_pred = clf.predict(X_test)\n",
      "      4 print(classification_report(y_test, y_pred))\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py in fit(self, X, y, sample_weight)\n",
      "   1512             dtype=_dtype,\n",
      "   1513             order=\"C\",\n",
      "-> 1514             accept_large_sparse=solver not in [\"liblinear\", \"sag\", \"saga\"],\n",
      "   1515         )\n",
      "   1516         check_classification_targets(y)\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py in _validate_data(self, X, y, reset, validate_separately, **check_params)\n",
      "    579                 y = check_array(y, **check_y_params)\n",
      "    580             else:\n",
      "--> 581                 X, y = check_X_y(X, y, **check_params)\n",
      "    582             out = X, y\n",
      "    583 \n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\n",
      "    974         ensure_min_samples=ensure_min_samples,\n",
      "    975         ensure_min_features=ensure_min_features,\n",
      "--> 976         estimator=estimator,\n",
      "    977     )\n",
      "    978 \n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\n",
      "    744                     array = array.astype(dtype, casting=\"unsafe\", copy=False)\n",
      "    745                 else:\n",
      "--> 746                     array = np.asarray(array, order=order, dtype=dtype)\n",
      "    747             except ComplexWarning as complex_warning:\n",
      "    748                 raise ValueError(\n",
      "\n",
      "ValueError: setting an array element with a sequence.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 2/3 with eid 3d1a2d5d-3f92-3c69-ad41-7cab6d50a89f.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "TypeError                                 Traceback (most recent call last)\n",
      "TypeError: float() argument must be a string or a real number, not 'list'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "ValueError                                Traceback (most recent call last)\n",
      "Cell In[113], line 12\n",
      "      9 numerical_features = movies2[['genres_bin','director_bin','words_bin','cast_bin']]\n",
      "     11 # Step 3: Calculate Correlation Matrix\n",
      "---> 12 correlation_matrix = numerical_features.corr()\n",
      "     14 # Step 4: Create a Correlation Heatmap\n",
      "     15 plt.figure(figsize=(12, 8))\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:10054, in DataFrame.corr(self, method, min_periods, numeric_only)\n",
      "  10052 cols = data.columns\n",
      "  10053 idx = cols.copy()\n",
      "> 10054 mat = data.to_numpy(dtype=float, na_value=np.nan, copy=False)\n",
      "  10056 if method == \"pearson\":\n",
      "  10057     correl = libalgos.nancorr(mat, minp=min_periods)\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:1838, in DataFrame.to_numpy(self, dtype, copy, na_value)\n",
      "   1836 if dtype is not None:\n",
      "   1837     dtype = np.dtype(dtype)\n",
      "-> 1838 result = self._mgr.as_array(dtype=dtype, copy=copy, na_value=na_value)\n",
      "   1839 if result.dtype is not dtype:\n",
      "   1840     result = np.array(result, dtype=dtype, copy=False)\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/pandas/core/internals/managers.py:1724, in BlockManager.as_array(self, dtype, copy, na_value)\n",
      "   1722     arr = np.asarray(blk.get_values())\n",
      "   1723     if dtype:\n",
      "-> 1724         arr = arr.astype(dtype, copy=False)\n",
      "   1726 if copy:\n",
      "   1727     arr = arr.copy()\n",
      "\n",
      "ValueError: setting an array element with a sequence.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 3/3 with eid 42c60a69-6b56-3cdf-8382-5bfe1a8d5ca2.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "TypeError                                 Traceback (most recent call last)\n",
      "TypeError: float() argument must be a string or a number, not 'list'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "ValueError                                Traceback (most recent call last)\n",
      "/tmp/ipykernel_27/2197631068.py in <module>\n",
      "      4 fig, ax = plt.subplots(figsize=(10, 6))\n",
      "      5 for column in max_data.columns:\n",
      "----> 6     ax.plot(max_data[column], label=column)\n",
      "      7 ax.set_xlabel('Score group')\n",
      "      8 ax.set_ylabel('Variable max value')\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/axes/_axes.py in plot(self, scalex, scaley, data, *args, **kwargs)\n",
      "   1635         lines = [*self._get_lines(*args, data=data, **kwargs)]\n",
      "   1636         for line in lines:\n",
      "-> 1637             self.add_line(line)\n",
      "   1638         self._request_autoscale_view(scalex=scalex, scaley=scaley)\n",
      "   1639         return lines\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/axes/_base.py in add_line(self, line)\n",
      "   2286             line.set_clip_path(self.patch)\n",
      "   2287 \n",
      "-> 2288         self._update_line_limits(line)\n",
      "   2289         if not line.get_label():\n",
      "   2290             line.set_label(f'_child{len(self._children)}')\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/axes/_base.py in _update_line_limits(self, line)\n",
      "   2309         Figures out the data limit of the given line, updating self.dataLim.\n",
      "   2310         \"\"\"\n",
      "-> 2311         path = line.get_path()\n",
      "   2312         if path.vertices.size == 0:\n",
      "   2313             return\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/lines.py in get_path(self)\n",
      "    997         \"\"\"Return the `~matplotlib.path.Path` associated with this line.\"\"\"\n",
      "    998         if self._invalidy or self._invalidx:\n",
      "--> 999             self.recache()\n",
      "   1000         return self._path\n",
      "   1001 \n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/lines.py in recache(self, always)\n",
      "    655         if always or self._invalidy:\n",
      "    656             yconv = self.convert_yunits(self._yorig)\n",
      "--> 657             y = _to_unmasked_float_array(yconv).ravel()\n",
      "    658         else:\n",
      "    659             y = self._y\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/cbook/__init__.py in _to_unmasked_float_array(x)\n",
      "   1296         return np.ma.asarray(x, float).filled(np.nan)\n",
      "   1297     else:\n",
      "-> 1298         return np.asarray(x, float)\n",
      "   1299 \n",
      "   1300 \n",
      "\n",
      "ValueError: setting an array element with a sequence.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 7266 in Kaggle error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++5/10: Error examples for Cluster 7381 in Kaggle error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/5 with eid 4a4b96af-3242-38f5-ae88-2d3081ca4f6f.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "TypeError                                 Traceback (most recent call last)\n",
      "Cell In[73], line 3\n",
      "      1 a = np.array([1,2,3])\n",
      "      2 b= np.array([5,4,2])\n",
      "----> 3 print( np.min(a,b) )\n",
      "      5 # 5000, 20000, >20000 \n",
      "      6 # 7500\n",
      "      7 # 30 40 30\n",
      "\n",
      "File <__array_function__ internals>:180, in amin(*args, **kwargs)\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2918, in amin(a, axis, out, keepdims, initial, where)\n",
      "   2802 @array_function_dispatch(_amin_dispatcher)\n",
      "   2803 def amin(a, axis=None, out=None, keepdims=np._NoValue, initial=np._NoValue,\n",
      "   2804          where=np._NoValue):\n",
      "   2805     \"\"\"\n",
      "   2806     Return the minimum of an array or minimum along an axis.\n",
      "   2807 \n",
      "   (...)\n",
      "   2916     6\n",
      "   2917     \"\"\"\n",
      "-> 2918     return _wrapreduction(a, np.minimum, 'min', axis, None, out,\n",
      "   2919                           keepdims=keepdims, initial=initial, where=where)\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86, in _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs)\n",
      "     83         else:\n",
      "     84             return reduction(axis=axis, out=out, **passkwargs)\n",
      "---> 86 return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "\n",
      "TypeError: only integer scalar arrays can be converted to a scalar index\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 2/5 with eid 6893ad84-c5a9-3cd9-a7c6-c951a71b9c29.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "TypeError                                 Traceback (most recent call last)\n",
      "Cell In[89], line 4\n",
      "      2 for i in [300,400]:\n",
      "      3     plt.imshow(train_images[i])\n",
      "----> 4     plt.title(labels[train_labels[i]])\n",
      "      5     plt.show()\n",
      "\n",
      "TypeError: only integer scalar arrays can be converted to a scalar index\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 3/5 with eid 742aa349-be15-3da6-be0e-026b70dc146f.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "TypeError                                 Traceback (most recent call last)\n",
      "/tmp/ipykernel_23/922550585.py in <module>\n",
      "      6     ax = plt.subplot(3, 3, i + 1)\n",
      "      7     plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
      "----> 8     plt.title(class_names[labels[i]])\n",
      "      9     plt.axis(\"off\")\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in __index__(self)\n",
      "   1061 \n",
      "   1062   def __index__(self):\n",
      "-> 1063     return self._numpy().__index__()\n",
      "   1064 \n",
      "   1065   def __bool__(self):\n",
      "\n",
      "TypeError: only integer scalar arrays can be converted to a scalar index\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 4/5 with eid d069ad30-b5c2-3eeb-b471-08edad9f9cb5.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "TypeError                                 Traceback (most recent call last)\n",
      "Cell In[102], line 3\n",
      "      1 # faire le code pour afficher l'image choisie\n",
      "      2 plt.imshow(X_test[0])\n",
      "----> 3 plt.title(Classes[y[0]])\n",
      "\n",
      "TypeError: only integer scalar arrays can be converted to a scalar index\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 5/5 with eid 127ce557-6d79-3486-bb70-0a08287a1e5c.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "TypeError                                 Traceback (most recent call last)\n",
      "Cell In[16], line 168\n",
      "    166 print('\\n--- Predicting image from test set ---')\n",
      "    167 image_idx = 40                                                          # index of image to predict\n",
      "--> 168 predict(model, image_idx)\n",
      "    170 print('\\n--- Plotting weight distributions ---')\n",
      "    171 plot_weights(model)\n",
      "\n",
      "Cell In[16], line 71, in predict(model, image_idx)\n",
      "     68 image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
      "     69 pred = np.argmax(model.predict(image))\n",
      "---> 71 plot_sample(dataset['test_images'][image_idx], classes[dataset['test_labels'][image_idx]], classes[pred])\n",
      "     73 # extracting the output and appending to outputs\n",
      "     74 feature_maps = []\n",
      "\n",
      "TypeError: only integer scalar arrays can be converted to a scalar index\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 7381 in Kaggle error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++6/10: Error examples for Cluster 7414 in Kaggle error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/2 with eid dc0d4bb1-cb07-3cbc-8beb-9c4595761a57.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "RuntimeError                              Traceback (most recent call last)\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py in save(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\n",
      "    422         with _open_zipfile_writer(f) as opened_zipfile:\n",
      "--> 423             _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n",
      "    424             return\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py in _save(obj, zip_file, pickle_module, pickle_protocol)\n",
      "    649         num_bytes = storage.nbytes()\n",
      "--> 650         zip_file.write_record(name, storage.data_ptr(), num_bytes)\n",
      "    651 \n",
      "\n",
      "RuntimeError: [enforce fail at inline_container.cc:445] . PytorchStreamWriter failed writing file data/1: file write failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "RuntimeError                              Traceback (most recent call last)\n",
      "/tmp/ipykernel_23/2831218013.py in <module>\n",
      "      1 ## Save model with epoch - 3\n",
      "----> 2 model.save_pretrained(\"ner_model_12\")\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/modeling_utils.py in save_pretrained(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, **kwargs)\n",
      "   1756                 safe_save_file(shard, os.path.join(save_directory, shard_file), metadata={\"format\": \"pt\"})\n",
      "   1757             else:\n",
      "-> 1758                 save_function(shard, os.path.join(save_directory, shard_file))\n",
      "   1759 \n",
      "   1760         if index is None:\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py in save(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\n",
      "    422         with _open_zipfile_writer(f) as opened_zipfile:\n",
      "    423             _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n",
      "--> 424             return\n",
      "    425     else:\n",
      "    426         with _open_file_like(f, 'wb') as opened_file:\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py in __exit__(self, *args)\n",
      "    288 \n",
      "    289     def __exit__(self, *args) -> None:\n",
      "--> 290         self.file_like.write_end_of_file()\n",
      "    291 \n",
      "    292 \n",
      "\n",
      "RuntimeError: [enforce fail at inline_container.cc:325] . unexpected pos 33536 vs 33440\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 2/2 with eid 0f63eec0-9add-3172-8c93-f19fedd36c31.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "RuntimeError                              Traceback (most recent call last)\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/serialization.py:441, in save(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\n",
      "    440 with _open_zipfile_writer(f) as opened_zipfile:\n",
      "--> 441     _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n",
      "    442     return\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/serialization.py:668, in _save(obj, zip_file, pickle_module, pickle_protocol)\n",
      "    667 num_bytes = storage.nbytes()\n",
      "--> 668 zip_file.write_record(name, storage.data_ptr(), num_bytes)\n",
      "\n",
      "RuntimeError: [enforce fail at inline_container.cc:471] . PytorchStreamWriter failed writing file data/561: file write failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "RuntimeError                              Traceback (most recent call last)\n",
      "Cell In[12], line 2\n",
      "      1 #wandb 90ffc765c04159b8633c7c01c449e5fd5597e3ef\n",
      "----> 2 trainer.train()\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280, in SFTTrainer.train(self, *args, **kwargs)\n",
      "    277 if self.neftune_noise_alpha is not None and not self._trainer_supports_neftune:\n",
      "    278     self.model = self._trl_activate_neftune(self.model)\n",
      "--> 280 output = super().train(*args, **kwargs)\n",
      "    282 # After training we make sure to retrieve back the original forward pass method\n",
      "    283 # for the embedding layer by removing the forward post hook.\n",
      "    284 if self.neftune_noise_alpha is not None and not self._trainer_supports_neftune:\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1537, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\n",
      "   1535         hf_hub_utils.enable_progress_bars()\n",
      "   1536 else:\n",
      "-> 1537     return inner_training_loop(\n",
      "   1538         args=args,\n",
      "   1539         resume_from_checkpoint=resume_from_checkpoint,\n",
      "   1540         trial=trial,\n",
      "   1541         ignore_keys_for_eval=ignore_keys_for_eval,\n",
      "   1542     )\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1914, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\n",
      "   1911     self.state.epoch = epoch + (step + 1 + steps_skipped) / steps_in_epoch\n",
      "   1912     self.control = self.callback_handler.on_step_end(args, self.state, self.control)\n",
      "-> 1914     self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n",
      "   1915 else:\n",
      "   1916     self.control = self.callback_handler.on_substep_end(args, self.state, self.control)\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2279, in Trainer._maybe_log_save_evaluate(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\n",
      "   2276         self.lr_scheduler.step(metrics[metric_to_check])\n",
      "   2278 if self.control.should_save:\n",
      "-> 2279     self._save_checkpoint(model, trial, metrics=metrics)\n",
      "   2280     self.control = self.callback_handler.on_save(self.args, self.state, self.control)\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2359, in Trainer._save_checkpoint(self, model, trial, metrics)\n",
      "   2355 self.save_model(staging_output_dir, _internal_call=True)\n",
      "   2357 if not self.args.save_only_model:\n",
      "   2358     # Save optimizer and scheduler\n",
      "-> 2359     self._save_optimizer_and_scheduler(staging_output_dir)\n",
      "   2360     # Save RNG state\n",
      "   2361     self._save_rng_state(staging_output_dir)\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2462, in Trainer._save_optimizer_and_scheduler(self, output_dir)\n",
      "   2457     save_fsdp_optimizer(\n",
      "   2458         self.accelerator.state.fsdp_plugin, self.accelerator, self.optimizer, self.model, output_dir\n",
      "   2459     )\n",
      "   2460 elif self.args.should_save:\n",
      "   2461     # deepspeed.save_checkpoint above saves model/optim/sched\n",
      "-> 2462     torch.save(self.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME))\n",
      "   2464 # Save SCHEDULER & SCALER\n",
      "   2465 is_deepspeed_custom_scheduler = self.is_deepspeed_enabled and not isinstance(\n",
      "   2466     self.lr_scheduler, DeepSpeedSchedulerWrapper\n",
      "   2467 )\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/serialization.py:440, in save(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\n",
      "    437 _check_save_filelike(f)\n",
      "    439 if _use_new_zipfile_serialization:\n",
      "--> 440     with _open_zipfile_writer(f) as opened_zipfile:\n",
      "    441         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n",
      "    442         return\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/serialization.py:291, in _open_zipfile_writer_file.__exit__(self, *args)\n",
      "    290 def __exit__(self, *args) -> None:\n",
      "--> 291     self.file_like.write_end_of_file()\n",
      "\n",
      "RuntimeError: [enforce fail at inline_container.cc:337] . unexpected pos 589963136 vs 589963024\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 7414 in Kaggle error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++7/10: Error examples for Cluster 6655 in Kaggle error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/5 with eid 5953c2df-2e0d-3b17-893a-99786b7002fa.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "AssertionError                            Traceback (most recent call last)\n",
      "/tmp/ipykernel_27/303648191.py in <module>\n",
      "      1 tokenized_homodimer['position_ids'] = position_ids.unsqueeze(0)\n",
      "      2 \n",
      "----> 3 tokenized_homodimer = {key: tensor.cuda() for key, tensor in tokenized_homodimer.items()}\n",
      "\n",
      "/tmp/ipykernel_27/303648191.py in <dictcomp>(.0)\n",
      "      1 tokenized_homodimer['position_ids'] = position_ids.unsqueeze(0)\n",
      "      2 \n",
      "----> 3 tokenized_homodimer = {key: tensor.cuda() for key, tensor in tokenized_homodimer.items()}\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/torch/cuda/__init__.py in _lazy_init()\n",
      "    219                 \"multiprocessing, you must use the 'spawn' start method\")\n",
      "    220         if not hasattr(torch._C, '_cuda_getDeviceCount'):\n",
      "--> 221             raise AssertionError(\"Torch not compiled with CUDA enabled\")\n",
      "    222         if _cudart is None:\n",
      "    223             raise AssertionError(\n",
      "\n",
      "AssertionError: Torch not compiled with CUDA enabled\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 2/5 with eid a59dc217-5834-3653-b35a-6a9e6c26e18b.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "AssertionError                            Traceback (most recent call last)\n",
      "/tmp/ipykernel_27/696859457.py in <module>\n",
      "      5 with torch.no_grad():\n",
      "      6     for input_ids in tqdm(ecoli_tokenized):\n",
      "----> 7         input_ids = torch.tensor(input_ids, device='cuda').unsqueeze(0)\n",
      "      8         output = model(input_ids)\n",
      "      9         outputs.append({key: val.cpu() for key, val in output.items()})\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/torch/cuda/__init__.py in _lazy_init()\n",
      "    219                 \"multiprocessing, you must use the 'spawn' start method\")\n",
      "    220         if not hasattr(torch._C, '_cuda_getDeviceCount'):\n",
      "--> 221             raise AssertionError(\"Torch not compiled with CUDA enabled\")\n",
      "    222         if _cudart is None:\n",
      "    223             raise AssertionError(\n",
      "\n",
      "AssertionError: Torch not compiled with CUDA enabled\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 3/5 with eid 25110a65-51b4-3279-85e8-5fc1e28c3897.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "AssertionError                            Traceback (most recent call last)\n",
      "/tmp/ipykernel_28/2861775736.py in <module>\n",
      "      3 with torch.no_grad():\n",
      "      4     for x_data, y_data in tqdm(test_loader):\n",
      "----> 5         logits = model(x_data.to(\"cuda\"))\n",
      "      6 \n",
      "      7         true_probabilities.append(y_data.numpy())\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/torch/cuda/__init__.py in _lazy_init()\n",
      "    208                 \"multiprocessing, you must use the 'spawn' start method\")\n",
      "    209         if not hasattr(torch._C, '_cuda_getDeviceCount'):\n",
      "--> 210             raise AssertionError(\"Torch not compiled with CUDA enabled\")\n",
      "    211         if _cudart is None:\n",
      "    212             raise AssertionError(\n",
      "\n",
      "AssertionError: Torch not compiled with CUDA enabled\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 4/5 with eid 7b7b8c4e-4374-38c5-8ec6-bfd4c4a58755.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "AssertionError                            Traceback (most recent call last)\n",
      "/tmp/ipykernel_27/2796195007.py in <module>\n",
      "    502 \n",
      "    503 if __name__ == \"__main__\":\n",
      "--> 504     main()\n",
      "\n",
      "/tmp/ipykernel_27/2796195007.py in main()\n",
      "     57     best_ssim = 0.0\n",
      "     58 \n",
      "---> 59     train_prefetcher, test_prefetcher = load_dataset()\n",
      "     60     print(\"Load all datasets successfully.\")\n",
      "     61 \n",
      "\n",
      "/tmp/ipykernel_27/2796195007.py in load_dataset()\n",
      "    209 \n",
      "    210     # Place all data on the preprocessing data loader\n",
      "--> 211     train_prefetcher = CUDAPrefetcher(train_dataloader, srgan_config.device)\n",
      "    212     test_prefetcher = CUDAPrefetcher(test_dataloader, srgan_config.device)\n",
      "    213 \n",
      "\n",
      "/kaggle/input/d/sahilchawla7/srganimplementation/dataset.py in __init__(self, dataloader, device)\n",
      "    202 \n",
      "    203         self.data = iter(dataloader)\n",
      "--> 204         self.stream = torch.cuda.Stream(device)\n",
      "    205         self.preload()\n",
      "    206 \n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/torch/cuda/streams.py in __new__(cls, device, priority, **kwargs)\n",
      "     34             return super(Stream, cls).__new__(cls, priority=priority, **kwargs)\n",
      "     35         else:\n",
      "---> 36             with torch.cuda.device(device):\n",
      "     37                 return super(Stream, cls).__new__(cls, priority=priority, **kwargs)\n",
      "     38 \n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/torch/cuda/__init__.py in __enter__(self)\n",
      "    285         if self.idx == -1:\n",
      "    286             return\n",
      "--> 287         self.prev_idx = torch.cuda.current_device()\n",
      "    288         if self.prev_idx != self.idx:\n",
      "    289             torch.cuda.set_device(self.idx)\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/torch/cuda/__init__.py in current_device()\n",
      "    550 def current_device() -> int:\n",
      "    551     r\"\"\"Returns the index of a currently selected device.\"\"\"\n",
      "--> 552     _lazy_init()\n",
      "    553     return torch._C._cuda_getDevice()\n",
      "    554 \n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/torch/cuda/__init__.py in _lazy_init()\n",
      "    219                 \"multiprocessing, you must use the 'spawn' start method\")\n",
      "    220         if not hasattr(torch._C, '_cuda_getDeviceCount'):\n",
      "--> 221             raise AssertionError(\"Torch not compiled with CUDA enabled\")\n",
      "    222         if _cudart is None:\n",
      "    223             raise AssertionError(\n",
      "\n",
      "AssertionError: Torch not compiled with CUDA enabled\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 5/5 with eid 44dcc18c-520e-31ed-85a4-f1a07abd7bfa.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "AssertionError                            Traceback (most recent call last)\n",
      "<timed exec> in <module>\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in cuda(self, device)\n",
      "    745             Module: self\n",
      "    746         \"\"\"\n",
      "--> 747         return self._apply(lambda t: t.cuda(device))\n",
      "    748 \n",
      "    749     def ipu(self: T, device: Optional[Union[int, device]] = None) -> T:\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)\n",
      "    637     def _apply(self, fn):\n",
      "    638         for module in self.children():\n",
      "--> 639             module._apply(fn)\n",
      "    640 \n",
      "    641         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)\n",
      "    637     def _apply(self, fn):\n",
      "    638         for module in self.children():\n",
      "--> 639             module._apply(fn)\n",
      "    640 \n",
      "    641         def compute_should_use_set_data(tensor, tensor_applied):\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)\n",
      "    660             # `with torch.no_grad():`\n",
      "    661             with torch.no_grad():\n",
      "--> 662                 param_applied = fn(param)\n",
      "    663             should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      "    664             if should_use_set_data:\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in <lambda>(t)\n",
      "    745             Module: self\n",
      "    746         \"\"\"\n",
      "--> 747         return self._apply(lambda t: t.cuda(device))\n",
      "    748 \n",
      "    749     def ipu(self: T, device: Optional[Union[int, device]] = None) -> T:\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/torch/cuda/__init__.py in _lazy_init()\n",
      "    219                 \"multiprocessing, you must use the 'spawn' start method\")\n",
      "    220         if not hasattr(torch._C, '_cuda_getDeviceCount'):\n",
      "--> 221             raise AssertionError(\"Torch not compiled with CUDA enabled\")\n",
      "    222         if _cudart is None:\n",
      "    223             raise AssertionError(\n",
      "\n",
      "AssertionError: Torch not compiled with CUDA enabled\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 6655 in Kaggle error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++8/10: Error examples for Cluster 90 in Kaggle error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/1 with eid 46ee0e09-7f5e-387f-b7b7-8d929a9e43c1.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "/tmp/ipykernel_27/1248221608.py in <module>\n",
      "----> 1 listings.to_sql('listings',con=credentials,index=False)\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py in to_sql(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\n",
      "   2880             chunksize=chunksize,\n",
      "   2881             dtype=dtype,\n",
      "-> 2882             method=method,\n",
      "   2883         )\n",
      "   2884 \n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/io/sql.py in to_sql(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method, engine, **engine_kwargs)\n",
      "    726         method=method,\n",
      "    727         engine=engine,\n",
      "--> 728         **engine_kwargs,\n",
      "    729     )\n",
      "    730 \n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/io/sql.py in to_sql(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method, engine, **engine_kwargs)\n",
      "   1756             index_label=index_label,\n",
      "   1757             schema=schema,\n",
      "-> 1758             dtype=dtype,\n",
      "   1759         )\n",
      "   1760 \n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/io/sql.py in prep_table(self, frame, name, if_exists, index, index_label, schema, dtype)\n",
      "   1648             dtype=dtype,\n",
      "   1649         )\n",
      "-> 1650         table.create()\n",
      "   1651         return table\n",
      "   1652 \n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/io/sql.py in create(self)\n",
      "    856         if self.exists():\n",
      "    857             if self.if_exists == \"fail\":\n",
      "--> 858                 raise ValueError(f\"Table '{self.name}' already exists.\")\n",
      "    859             elif self.if_exists == \"replace\":\n",
      "    860                 self.pd_sql.drop_table(self.name, self.schema)\n",
      "\n",
      "ValueError: Table 'listings' already exists.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 90 in Kaggle error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++9/10: Error examples for Cluster 1994 in Kaggle error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/1 with eid 5e3e7188-e5d0-306f-b7a8-0e0e2c4b8f51.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "RuntimeError                              Traceback (most recent call last)\n",
      "Cell In[2], line 88\n",
      "     85 real_images_tensor_2 = torch.stack([torch.from_numpy(img) for img in real_images_2]).to(device)\n",
      "     87 # Save the real images in a grid format\n",
      "---> 88 utils.save_image(real_images_tensor_1, 'real_images_1.png', nrow=NUM_DISPLAY_IMAGES)\n",
      "     89 utils.save_image(real_images_tensor_2, 'real_images_2.png', nrow=NUM_DISPLAY_IMAGES)\n",
      "     91 real_images_tensor_1 = real_images_tensor_1 / 255\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115, in context_decorator.<locals>.decorate_context(*args, **kwargs)\n",
      "    112 @functools.wraps(func)\n",
      "    113 def decorate_context(*args, **kwargs):\n",
      "    114     with ctx_factory():\n",
      "--> 115         return func(*args, **kwargs)\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torchvision/utils.py:149, in save_image(tensor, fp, format, **kwargs)\n",
      "    147 grid = make_grid(tensor, **kwargs)\n",
      "    148 # Add 0.5 after unnormalizing to [0, 255] to round to the nearest integer\n",
      "--> 149 ndarr = grid.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to(\"cpu\", torch.uint8).numpy()\n",
      "    150 im = Image.fromarray(ndarr)\n",
      "    151 im.save(fp, format=format)\n",
      "\n",
      "RuntimeError: result type Float can't be cast to the desired output type Byte\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 1994 in Kaggle error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++10/10: Error examples for Cluster 7204 in Kaggle error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/2 with eid 279ee8f5-0f61-3bdc-bd47-d4e23c1a7456.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "/tmp/ipykernel_23/1562205175.py in <module>\n",
      "      6 \n",
      "      7 # Load the weights\n",
      "----> 8 caption_model.load_weights('model_weights.h5')\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)\n",
      "     68             # To get the full stack trace, call:\n",
      "     69             # `tf.debugging.disable_traceback_filtering()`\n",
      "---> 70             raise e.with_traceback(filtered_tb) from None\n",
      "     71         finally:\n",
      "     72             del filtered_tb\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/keras/engine/training.py in load_weights(self, filepath, by_name, skip_mismatch, options)\n",
      "   3021             if not self._is_graph_network and not self.built:\n",
      "   3022                 raise ValueError(\n",
      "-> 3023                     \"Unable to load weights saved in HDF5 format into a \"\n",
      "   3024                     \"subclassed Model which has not created its variables yet. \"\n",
      "   3025                     \"Call the Model first, then load the weights.\"\n",
      "\n",
      "ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 2/2 with eid 4ccbedaa-a78e-3c44-a394-e6bb8ddb57a1.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "Cell In[22], line 12\n",
      "      1 # modl.save_weights(\"action.h5\")\n",
      "      2 modl=model = Transformer(\n",
      "      3     num_hid=200,\n",
      "      4     num_head=4,\n",
      "   (...)\n",
      "     10     num_classes=62\n",
      "     11 )\n",
      "---> 12 modl.load_weights('action.h5')\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)\n",
      "     67     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "     68     # To get the full stack trace, call:\n",
      "     69     # `tf.debugging.disable_traceback_filtering()`\n",
      "---> 70     raise e.with_traceback(filtered_tb) from None\n",
      "     71 finally:\n",
      "     72     del filtered_tb\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/keras/saving/legacy/save.py:476, in load_weights(model, filepath, by_name, skip_mismatch, options)\n",
      "    471     raise ImportError(\n",
      "    472         \"`load_weights` requires h5py package when loading weights \"\n",
      "    473         \"from HDF5. Try installing h5py.\"\n",
      "    474     )\n",
      "    475 if not model._is_graph_network and not model.built:\n",
      "--> 476     raise ValueError(\n",
      "    477         \"Unable to load weights saved in HDF5 format into a \"\n",
      "    478         \"subclassed Model which has not created its variables yet. \"\n",
      "    479         \"Call the Model first, then load the weights.\"\n",
      "    480     )\n",
      "    481 model._assert_weights_created()\n",
      "    482 with h5py.File(filepath, \"r\") as f:\n",
      "\n",
      "ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 7204 in Kaggle error notebooks end.+++++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# randomly select min(5,cluster size) crashes from each cluster\n",
    "# Kaggle\n",
    "for j in range(len(sample_k_small_clusters)):\n",
    "    skc = sample_k_small_clusters[j]\n",
    "    print('\\033[1m'+\"+++++{}/{}: Error examples for Cluster {} in Kaggle error notebooks.+++++\\n\".format(j+1, 10, skc)+'\\033[0m')\n",
    "    df_target = df_err_grouped_k_small[df_err_grouped_k_small.pregroup_cluster==skc]\n",
    "    n = min(5, len(df_target))\n",
    "    tmp = df_target.sample(n=n, random_state=10)\n",
    "    for i in range(len(tmp)):\n",
    "        print(\"-----Error example {}/{} with eid {}.-----\\n\".format(i+1, n, tmp.iloc[i].eid))\n",
    "        util.print_traceback(tmp.iloc[i].traceback)\n",
    "        print(\"---------------------------------------------------------------------------\\n\")\n",
    "    print(\"+++++Error examples for Cluster {} in Kaggle error notebooks end.+++++\\n\".format(skc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40f087f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m+++++1/10: Error examples for Cluster 5563 in GitHub error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/1 with eid 23475959-9147-358d-a8e9-c546adf5caa0.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "D:\\Users\\zanny\\Anaconda3\\lib\\site-packages\\gmaps\\geotraitlets.py in _validate_latitude(latitude)\n",
      "    298     try:\n",
      "--> 299         latitude = float(latitude)\n",
      "    300     except (TypeError, ValueError):\n",
      "\n",
      "ValueError: could not convert string to float: \n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "TraitError                                Traceback (most recent call last)\n",
      "D:\\Users\\zanny\\Anaconda3\\lib\\site-packages\\traitlets\\traitlets.py in validate_elements(self, obj, value)\n",
      "   2443             try:\n",
      "-> 2444                 v = t._validate(obj, v)\n",
      "   2445             except TraitError:\n",
      "\n",
      "D:\\Users\\zanny\\Anaconda3\\lib\\site-packages\\traitlets\\traitlets.py in _validate(self, obj, value)\n",
      "    590         if hasattr(self, 'validate'):\n",
      "--> 591             value = self.validate(obj, value)\n",
      "    592         if obj._cross_validation_lock is False:\n",
      "\n",
      "D:\\Users\\zanny\\Anaconda3\\lib\\site-packages\\gmaps\\geotraitlets.py in validate(self, obj, value)\n",
      "     65     def validate(self, obj, value):\n",
      "---> 66         _validate_latitude(value)\n",
      "     67         return value\n",
      "\n",
      "D:\\Users\\zanny\\Anaconda3\\lib\\site-packages\\gmaps\\geotraitlets.py in _validate_latitude(latitude)\n",
      "    302             '{} is not a valid latitude. '\n",
      "--> 303             'Latitudes must be floats'.format(latitude)\n",
      "    304         )\n",
      "\n",
      "TraitError:  is not a valid latitude. Latitudes must be floats\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "TraitError                                Traceback (most recent call last)\n",
      "<ipython-input-14-8c06a207b734> in <module>\n",
      "      3 for index,row in hotels_df.iterrows():\n",
      "      4     hotel_loc.append( tuple([row[\"Hotel Lat\"],row[\"Hotel Lng\"]] ))\n",
      "----> 5 markers=gmaps.marker_layer(hotel_loc)\n",
      "      6 fig.add_layer(markers)\n",
      "      7 \n",
      "\n",
      "D:\\Users\\zanny\\Anaconda3\\lib\\site-packages\\gmaps\\marker.py in marker_layer(locations, hover_text, label, info_box_content, display_info_box)\n",
      "    553     marker_options = _marker_layer_options(\n",
      "    554         locations, hover_text, label, info_box_content, display_info_box)\n",
      "--> 555     markers = [Marker(**option) for option in marker_options]\n",
      "    556     return Markers(markers=markers)\n",
      "\n",
      "D:\\Users\\zanny\\Anaconda3\\lib\\site-packages\\gmaps\\marker.py in <listcomp>(.0)\n",
      "    553     marker_options = _marker_layer_options(\n",
      "    554         locations, hover_text, label, info_box_content, display_info_box)\n",
      "--> 555     markers = [Marker(**option) for option in marker_options]\n",
      "    556     return Markers(markers=markers)\n",
      "\n",
      "D:\\Users\\zanny\\Anaconda3\\lib\\site-packages\\gmaps\\marker.py in __init__(self, location, **kwargs)\n",
      "    190         kwargs = _resolve_info_box_kwargs(**kwargs)\n",
      "    191         kwargs['location'] = location\n",
      "--> 192         super(Marker, self).__init__(**kwargs)\n",
      "    193 \n",
      "    194 \n",
      "\n",
      "D:\\Users\\zanny\\Anaconda3\\lib\\site-packages\\ipywidgets\\widgets\\widget.py in __init__(self, **kwargs)\n",
      "    410         \"\"\"Public constructor\"\"\"\n",
      "    411         self._model_id = kwargs.pop('model_id', None)\n",
      "--> 412         super(Widget, self).__init__(**kwargs)\n",
      "    413 \n",
      "    414         Widget._call_widget_constructed(self)\n",
      "\n",
      "D:\\Users\\zanny\\Anaconda3\\lib\\site-packages\\traitlets\\traitlets.py in __init__(self, *args, **kwargs)\n",
      "    995             for key, value in kwargs.items():\n",
      "    996                 if self.has_trait(key):\n",
      "--> 997                     setattr(self, key, value)\n",
      "    998                 else:\n",
      "    999                     # passthrough args that don't set traits to super\n",
      "\n",
      "D:\\Users\\zanny\\Anaconda3\\lib\\site-packages\\traitlets\\traitlets.py in __set__(self, obj, value)\n",
      "    583             raise TraitError('The \"%s\" trait is read-only.' % self.name)\n",
      "    584         else:\n",
      "--> 585             self.set(obj, value)\n",
      "    586 \n",
      "    587     def _validate(self, obj, value):\n",
      "\n",
      "D:\\Users\\zanny\\Anaconda3\\lib\\site-packages\\traitlets\\traitlets.py in set(self, obj, value)\n",
      "    557 \n",
      "    558     def set(self, obj, value):\n",
      "--> 559         new_value = self._validate(obj, value)\n",
      "    560         try:\n",
      "    561             old_value = obj._trait_values[self.name]\n",
      "\n",
      "D:\\Users\\zanny\\Anaconda3\\lib\\site-packages\\traitlets\\traitlets.py in _validate(self, obj, value)\n",
      "    589             return value\n",
      "    590         if hasattr(self, 'validate'):\n",
      "--> 591             value = self.validate(obj, value)\n",
      "    592         if obj._cross_validation_lock is False:\n",
      "    593             value = self._cross_validate(obj, value)\n",
      "\n",
      "D:\\Users\\zanny\\Anaconda3\\lib\\site-packages\\gmaps\\geotraitlets.py in validate(self, obj, value)\n",
      "    100                 )\n",
      "    101             latitude, longitude = value\n",
      "--> 102             return super(Point, self).validate(obj, (latitude, longitude))\n",
      "    103 \n",
      "    104 \n",
      "\n",
      "D:\\Users\\zanny\\Anaconda3\\lib\\site-packages\\traitlets\\traitlets.py in validate(self, obj, value)\n",
      "   2240             return value\n",
      "   2241 \n",
      "-> 2242         value = self.validate_elements(obj, value)\n",
      "   2243 \n",
      "   2244         return value\n",
      "\n",
      "D:\\Users\\zanny\\Anaconda3\\lib\\site-packages\\traitlets\\traitlets.py in validate_elements(self, obj, value)\n",
      "   2444                 v = t._validate(obj, v)\n",
      "   2445             except TraitError:\n",
      "-> 2446                 self.element_error(obj, v, t)\n",
      "   2447             else:\n",
      "   2448                 validated.append(v)\n",
      "\n",
      "D:\\Users\\zanny\\Anaconda3\\lib\\site-packages\\traitlets\\traitlets.py in element_error(self, obj, element, validator)\n",
      "   2231         e = \"Element of the '%s' trait of %s instance must be %s, but a value of %s was specified.\" \\\n",
      "   2232             % (self.name, class_of(obj), validator.info(), repr_type(element))\n",
      "-> 2233         raise TraitError(e)\n",
      "   2234 \n",
      "   2235     def validate(self, obj, value):\n",
      "\n",
      "TraitError: Element of the 'location' trait of a Marker instance must be a valid latitude (-90 <= latitude <= 90), but a value of '' <class 'str'> was specified.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 5563 in GitHub error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++2/10: Error examples for Cluster 7307 in GitHub error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/5 with eid bb051f97-d8fd-30f6-888a-0fdcf8630ec0.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "<ipython-input-11-1ccdfb571467> in <module>\n",
      "      1 # quick test\n",
      "----> 2 rf.predict(new_data)\n",
      "\n",
      "~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\ensemble\\_forest.py in predict(self, X)\n",
      "    627             The predicted classes.\n",
      "    628         \"\"\"\n",
      "--> 629         proba = self.predict_proba(X)\n",
      "    630 \n",
      "    631         if self.n_outputs_ == 1:\n",
      "\n",
      "~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\ensemble\\_forest.py in predict_proba(self, X)\n",
      "    671         check_is_fitted(self)\n",
      "    672         # Check data\n",
      "--> 673         X = self._validate_X_predict(X)\n",
      "    674 \n",
      "    675         # Assign chunk of trees to jobs\n",
      "\n",
      "~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\ensemble\\_forest.py in _validate_X_predict(self, X)\n",
      "    419         check_is_fitted(self)\n",
      "    420 \n",
      "--> 421         return self.estimators_[0]._validate_X_predict(X, check_input=True)\n",
      "    422 \n",
      "    423     @property\n",
      "\n",
      "~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\tree\\_classes.py in _validate_X_predict(self, X, check_input)\n",
      "    397                              \"match the input. Model n_features is %s and \"\n",
      "    398                              \"input n_features is %s \"\n",
      "--> 399                              % (self.n_features_, n_features))\n",
      "    400 \n",
      "    401         return X\n",
      "\n",
      "ValueError: Number of features of the model must match the input. Model n_features is 11 and input n_features is 10 \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 2/5 with eid 640a63f9-5885-300e-9b98-349108875afe.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "<ipython-input-62-8fe9d11ccaba> in <module>()\n",
      "----> 1 predict_new('/home/veerlosar/Downloads/ERHS/happy_maria.wav')\n",
      "\n",
      "<ipython-input-61-0439e75445cb> in predict_new(filename)\n",
      "      8     #df = df.drop([0, 1], axis=1)\n",
      "      9     for model in models:\n",
      "---> 10         print('{}: '.format(model), '\\n', model.predict(df))\n",
      "     11 \n",
      "\n",
      "~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py in predict(self, X)\n",
      "    600             The predicted classes.\n",
      "    601         \"\"\"\n",
      "--> 602         pred = self.decision_function(X)\n",
      "    603 \n",
      "    604         if self.n_classes_ == 2:\n",
      "\n",
      "~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py in decision_function(self, X)\n",
      "    668             # The weights are all 1. for SAMME.R\n",
      "    669             pred = sum(_samme_proba(estimator, n_classes, X)\n",
      "--> 670                        for estimator in self.estimators_)\n",
      "    671         else:   # self.algorithm == \"SAMME\"\n",
      "    672             pred = sum((estimator.predict(X) == classes).T * w\n",
      "\n",
      "~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py in <genexpr>(.0)\n",
      "    668             # The weights are all 1. for SAMME.R\n",
      "    669             pred = sum(_samme_proba(estimator, n_classes, X)\n",
      "--> 670                        for estimator in self.estimators_)\n",
      "    671         else:   # self.algorithm == \"SAMME\"\n",
      "    672             pred = sum((estimator.predict(X) == classes).T * w\n",
      "\n",
      "~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py in _samme_proba(estimator, n_classes, X)\n",
      "    283 \n",
      "    284     \"\"\"\n",
      "--> 285     proba = estimator.predict_proba(X)\n",
      "    286 \n",
      "    287     # Displace zero probabilities so the log is defined.\n",
      "\n",
      "~/anaconda3/lib/python3.7/site-packages/sklearn/tree/tree.py in predict_proba(self, X, check_input)\n",
      "    819         \"\"\"\n",
      "    820         check_is_fitted(self, 'tree_')\n",
      "--> 821         X = self._validate_X_predict(X, check_input)\n",
      "    822         proba = self.tree_.predict(X)\n",
      "    823 \n",
      "\n",
      "~/anaconda3/lib/python3.7/site-packages/sklearn/tree/tree.py in _validate_X_predict(self, X, check_input)\n",
      "    382                              \"match the input. Model n_features is %s and \"\n",
      "    383                              \"input n_features is %s \"\n",
      "--> 384                              % (self.n_features_, n_features))\n",
      "    385 \n",
      "    386         return X\n",
      "\n",
      "ValueError: Number of features of the model must match the input. Model n_features is 986 and input n_features is 988 \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 3/5 with eid 61f1ffa4-87ff-320d-b912-2787cc41e0e7.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "<ipython-input-110-3562157ad581> in <module>\n",
      "----> 1 preds = model.predict(test_sampled)\n",
      "      2 \n",
      "      3 # Store them in the submission dataframe and save\n",
      "      4 ss['Yield'] = preds\n",
      "      5 ss.to_csv('../data/processed/rfr_march_filter_cloud.csv', index=False)\n",
      "\n",
      "~/Documents/github/zindi_yield_forecast/env/lib/python3.7/site-packages/sklearn/ensemble/_forest.py in predict(self, X)\n",
      "    781         check_is_fitted(self)\n",
      "    782         # Check data\n",
      "--> 783         X = self._validate_X_predict(X)\n",
      "    784 \n",
      "    785         # Assign chunk of trees to jobs\n",
      "\n",
      "~/Documents/github/zindi_yield_forecast/env/lib/python3.7/site-packages/sklearn/ensemble/_forest.py in _validate_X_predict(self, X)\n",
      "    419         check_is_fitted(self)\n",
      "    420 \n",
      "--> 421         return self.estimators_[0]._validate_X_predict(X, check_input=True)\n",
      "    422 \n",
      "    423     @property\n",
      "\n",
      "~/Documents/github/zindi_yield_forecast/env/lib/python3.7/site-packages/sklearn/tree/_classes.py in _validate_X_predict(self, X, check_input)\n",
      "    397                              \"match the input. Model n_features is %s and \"\n",
      "    398                              \"input n_features is %s \"\n",
      "--> 399                              % (self.n_features_, n_features))\n",
      "    400 \n",
      "    401         return X\n",
      "\n",
      "ValueError: Number of features of the model must match the input. Model n_features is 48 and input n_features is 28 \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 4/5 with eid ee9f5639-45f8-3edf-b1d5-5640bd49a79f.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "<ipython-input-125-8ad6987857bd> in <module>\n",
      "      1 # Mak predictions on updated test data\n",
      "----> 2 test_preds = ideal_model.predict(df_test)\n",
      "\n",
      "D:\\INSTALLS\\programs\\works\\ANACONDA\\MYWORKS\\heart-disease-project\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py in predict(self, X)\n",
      "    781         check_is_fitted(self)\n",
      "    782         # Check data\n",
      "--> 783         X = self._validate_X_predict(X)\n",
      "    784 \n",
      "    785         # Assign chunk of trees to jobs\n",
      "\n",
      "D:\\INSTALLS\\programs\\works\\ANACONDA\\MYWORKS\\heart-disease-project\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py in _validate_X_predict(self, X)\n",
      "    419         check_is_fitted(self)\n",
      "    420 \n",
      "--> 421         return self.estimators_[0]._validate_X_predict(X, check_input=True)\n",
      "    422 \n",
      "    423     @property\n",
      "\n",
      "D:\\INSTALLS\\programs\\works\\ANACONDA\\MYWORKS\\heart-disease-project\\env\\lib\\site-packages\\sklearn\\tree\\_classes.py in _validate_X_predict(self, X, check_input)\n",
      "    394         n_features = X.shape[1]\n",
      "    395         if self.n_features_ != n_features:\n",
      "--> 396             raise ValueError(\"Number of features of the model must \"\n",
      "    397                              \"match the input. Model n_features is %s and \"\n",
      "    398                              \"input n_features is %s \"\n",
      "\n",
      "ValueError: Number of features of the model must match the input. Model n_features is 102 and input n_features is 101 \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 5/5 with eid 3744cb38-ddb9-3ea7-ae40-08b2777b033c.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "<ipython-input-63-7e33a4360c67> in <module>()\n",
      "----> 1 write_to_submission_file(xval_best_forest.predict(test_df), 'mypredict_tree.csv')\n",
      "\n",
      "c:\\users\\mkapchenko\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\metaestimators.py in <lambda>(*args, **kwargs)\n",
      "    114 \n",
      "    115         # lambda, but not partial, allows help() to work with update_wrapper\n",
      "--> 116         out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "    117         # update the docstring of the returned function\n",
      "    118         update_wrapper(out, self.fn)\n",
      "\n",
      "c:\\users\\mkapchenko\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\model_selection\\_search.py in predict(self, X)\n",
      "    475         \"\"\"\n",
      "    476         self._check_is_fitted('predict')\n",
      "--> 477         return self.best_estimator_.predict(X)\n",
      "    478 \n",
      "    479     @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))\n",
      "\n",
      "c:\\users\\mkapchenko\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_forest.py in predict(self, X)\n",
      "    610             The predicted classes.\n",
      "    611         \"\"\"\n",
      "--> 612         proba = self.predict_proba(X)\n",
      "    613 \n",
      "    614         if self.n_outputs_ == 1:\n",
      "\n",
      "c:\\users\\mkapchenko\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_forest.py in predict_proba(self, X)\n",
      "    654         check_is_fitted(self)\n",
      "    655         # Check data\n",
      "--> 656         X = self._validate_X_predict(X)\n",
      "    657 \n",
      "    658         # Assign chunk of trees to jobs\n",
      "\n",
      "c:\\users\\mkapchenko\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_forest.py in _validate_X_predict(self, X)\n",
      "    410         check_is_fitted(self)\n",
      "    411 \n",
      "--> 412         return self.estimators_[0]._validate_X_predict(X, check_input=True)\n",
      "    413 \n",
      "    414     @property\n",
      "\n",
      "c:\\users\\mkapchenko\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\tree\\_classes.py in _validate_X_predict(self, X, check_input)\n",
      "    389                              \"match the input. Model n_features is %s and \"\n",
      "    390                              \"input n_features is %s \"\n",
      "--> 391                              % (self.n_features_, n_features))\n",
      "    392 \n",
      "    393         return X\n",
      "\n",
      "ValueError: Number of features of the model must match the input. Model n_features is 18 and input n_features is 25 \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 7307 in GitHub error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++3/10: Error examples for Cluster 3271 in GitHub error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/1 with eid a0b2bf65-e615-31bc-bf6c-b82c39ee3839.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "RuntimeError                              Traceback (most recent call last)\n",
      "/Users/akshatchannashetti/Documents/VSCode_Projects/MSOE_ML/keras/DenseNet121TestKeras/classoutput_Akshat.ipynb Cell 1' in <module>\n",
      "      <a href='vscode-notebook-cell:/Users/akshatchannashetti/Documents/VSCode_Projects/MSOE_ML/keras/DenseNet121TestKeras/classoutput_Akshat.ipynb#ch0000000?line=8'>9</a> from tensorflow.python.compiler.mlcompute import mlcompute\n",
      "     <a href='vscode-notebook-cell:/Users/akshatchannashetti/Documents/VSCode_Projects/MSOE_ML/keras/DenseNet121TestKeras/classoutput_Akshat.ipynb#ch0000000?line=10'>11</a> # Select CPU device.\n",
      "---> <a href='vscode-notebook-cell:/Users/akshatchannashetti/Documents/VSCode_Projects/MSOE_ML/keras/DenseNet121TestKeras/classoutput_Akshat.ipynb#ch0000000?line=11'>12</a> mlcompute.set_mlc_device(device_name='gpu')\n",
      "\n",
      "File ~/miniforge3/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/compiler/mlcompute/mlcompute.py:178, in set_mlc_device(device_name)\n",
      "    <a href='file:///Users/akshatchannashetti/miniforge3/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/compiler/mlcompute/mlcompute.py?line=173'>174</a> try:\n",
      "    <a href='file:///Users/akshatchannashetti/miniforge3/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/compiler/mlcompute/mlcompute.py?line=174'>175</a>   # Ensure that this function is called once before all TF initialization,\n",
      "    <a href='file:///Users/akshatchannashetti/miniforge3/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/compiler/mlcompute/mlcompute.py?line=175'>176</a>   # but unlike other similar functions, this code lives outside Context()\n",
      "    <a href='file:///Users/akshatchannashetti/miniforge3/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/compiler/mlcompute/mlcompute.py?line=176'>177</a>   context.context()._handle\n",
      "--> <a href='file:///Users/akshatchannashetti/miniforge3/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/compiler/mlcompute/mlcompute.py?line=177'>178</a>   raise RuntimeError(\"ML Compute device must be set before initialization.\")\n",
      "    <a href='file:///Users/akshatchannashetti/miniforge3/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/compiler/mlcompute/mlcompute.py?line=178'>179</a> except AssertionError:\n",
      "    <a href='file:///Users/akshatchannashetti/miniforge3/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/compiler/mlcompute/mlcompute.py?line=179'>180</a>   pass\n",
      "\n",
      "RuntimeError: ML Compute device must be set before initialization.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 3271 in GitHub error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++4/10: Error examples for Cluster 4638 in GitHub error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/1 with eid 8cf52f31-71f3-33c1-a7e9-e8af399868aa.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "FileNotFoundError                         Traceback (most recent call last)\n",
      "<ipython-input-22-bf97e643a75f> in <module>()\n",
      "----> 1 output = pd.read_csv(\"logs/3-TAFT/fc_output_Taft Ave._2015.csv\", skipinitialspace=True)\n",
      "      2 output.head()\n",
      "      3 \n",
      "      4 startIndex = 5844\n",
      "      5 endIndex = 6899\n",
      "\n",
      "c:\\users\\ronnie nieva\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\n",
      "    707                     skip_blank_lines=skip_blank_lines)\n",
      "    708 \n",
      "--> 709         return _read(filepath_or_buffer, kwds)\n",
      "    710 \n",
      "    711     parser_f.__name__ = name\n",
      "\n",
      "c:\\users\\ronnie nieva\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py in _read(filepath_or_buffer, kwds)\n",
      "    447 \n",
      "    448     # Create the parser.\n",
      "--> 449     parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "    450 \n",
      "    451     if chunksize or iterator:\n",
      "\n",
      "c:\\users\\ronnie nieva\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py in __init__(self, f, engine, **kwds)\n",
      "    816             self.options['has_index_names'] = kwds['has_index_names']\n",
      "    817 \n",
      "--> 818         self._make_engine(self.engine)\n",
      "    819 \n",
      "    820     def close(self):\n",
      "\n",
      "c:\\users\\ronnie nieva\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py in _make_engine(self, engine)\n",
      "   1047     def _make_engine(self, engine='c'):\n",
      "   1048         if engine == 'c':\n",
      "-> 1049             self._engine = CParserWrapper(self.f, **self.options)\n",
      "   1050         else:\n",
      "   1051             if engine == 'python':\n",
      "\n",
      "c:\\users\\ronnie nieva\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py in __init__(self, src, **kwds)\n",
      "   1693         kwds['allow_leading_cols'] = self.index_col is not False\n",
      "   1694 \n",
      "-> 1695         self._reader = parsers.TextReader(src, **kwds)\n",
      "   1696 \n",
      "   1697         # XXX\n",
      "\n",
      "pandas\\_libs\\parsers.pyx in pandas._libs.parsers.TextReader.__cinit__()\n",
      "\n",
      "pandas\\_libs\\parsers.pyx in pandas._libs.parsers.TextReader._setup_parser_source()\n",
      "\n",
      "FileNotFoundError: File b'logs/3-TAFT/fc_output_Taft Ave._2015.csv' does not exist\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 4638 in GitHub error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++5/10: Error examples for Cluster 5355 in GitHub error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/2 with eid 8bd2f20c-544b-3f65-a241-123f147f6ba7.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ImportError                               Traceback (most recent call last)\n",
      "<ipython-input-4-38d4b0363d82> in <module>()\n",
      "----> 1 import pandas\n",
      "\n",
      "~/.local/lib/python3.6/site-packages/pandas/__init__.py in <module>()\n",
      "     17 if missing_dependencies:\n",
      "     18     raise ImportError(\n",
      "---> 19         \"Missing required dependencies {0}\".format(missing_dependencies))\n",
      "     20 del hard_dependencies, dependency, missing_dependencies\n",
      "     21 \n",
      "\n",
      "ImportError: Missing required dependencies ['numpy']\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 2/2 with eid 3dc7b0aa-893a-3fcd-88d4-deec74d90537.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ImportError                               Traceback (most recent call last)\n",
      "<ipython-input-7-99027f507b07> in <module>()\n",
      "----> 1 import pandas as pd\n",
      "      2 import numpy as np\n",
      "      3 import torch\n",
      "      4 import os\n",
      "      5 from PIL import Image\n",
      "\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\pandas\\__init__.py in <module>()\n",
      "     17 if missing_dependencies:\n",
      "     18     raise ImportError(\n",
      "---> 19         \"Missing required dependencies {0}\".format(missing_dependencies))\n",
      "     20 del hard_dependencies, dependency, missing_dependencies\n",
      "     21 \n",
      "\n",
      "ImportError: Missing required dependencies ['numpy']\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 5355 in GitHub error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++6/10: Error examples for Cluster 331 in GitHub error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/1 with eid 1da5f28c-4cc6-3805-859d-f0b2f839efc7.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "InputError                                Traceback (most recent call last)\n",
      "<timed eval> in <module>()\n",
      "\n",
      "<ipython-input-27-8233a95d6c7a> in migration_example()\n",
      "     39     # And then iterate over these replicates\n",
      "     40     T = np.zeros(num_replicates)\n",
      "---> 41     for i, tree_sequence in enumerate(replicates):\n",
      "     42         tree = next(tree_sequence.trees())\n",
      "     43         # Convert the TMRCA to coalecent units.\n",
      "\n",
      "~/src/anaconda3/envs/sprw/lib/python3.6/site-packages/msprime/trees.py in _replicate_generator(sim, rng, mutation_rate, num_replicates, provenance_dict)\n",
      "    601     while j < num_replicates:\n",
      "    602         j += 1\n",
      "--> 603         sim.run()\n",
      "    604         tree_sequence = sim.get_tree_sequence()\n",
      "    605         tree_sequence.generate_mutations(mutation_rate, rng)\n",
      "\n",
      "~/src/anaconda3/envs/sprw/lib/python3.6/site-packages/msprime/trees.py in run(self)\n",
      "   1167             raise ValueError(\"A random generator instance must be set\")\n",
      "   1168         if self._ll_sim is None:\n",
      "-> 1169             self._ll_sim = self.create_ll_instance()\n",
      "   1170         self._ll_sim.run()\n",
      "   1171 \n",
      "\n",
      "~/src/anaconda3/envs/sprw/lib/python3.6/site-packages/msprime/trees.py in create_ll_instance(self)\n",
      "   1157             avl_node_block_size=self._avl_node_block_size,\n",
      "   1158             node_mapping_block_size=self._node_mapping_block_size,\n",
      "-> 1159             coalescence_record_block_size=self._coalescence_record_block_size)\n",
      "   1160         return ll_sim\n",
      "   1161 \n",
      "\n",
      "InputError: Bad parameter value provided\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 331 in GitHub error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++7/10: Error examples for Cluster 1914 in GitHub error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/1 with eid f07b74cd-1986-3c9d-9f64-8e5f081a5996.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "<ipython-input-25-fe790fd59c77> in <module>\n",
      "----> 1 raw.set_montage(montage)\n",
      "\n",
      "<decorator-gen-23> in set_montage(self, montage, match_case, match_alias, on_missing, verbose)\n",
      "\n",
      "~/github/mne-python/mne/io/meas_info.py in set_montage(self, montage, match_case, match_alias, on_missing, verbose)\n",
      "    168         from ..channels.montage import _set_montage\n",
      "    169         info = self if isinstance(self, Info) else self.info\n",
      "--> 170         _set_montage(info, montage, match_case, match_alias, on_missing)\n",
      "    171         return self\n",
      "    172 \n",
      "\n",
      "~/github/mne-python/mne/channels/montage.py in _set_montage(***failed resolving arguments***)\n",
      "    878                 'in your analyses.'\n",
      "    879             )\n",
      "--> 880             _on_missing(on_missing, missing_coord_msg)\n",
      "    881 \n",
      "    882             # set ch coordinates and names from digmontage or nan coords\n",
      "\n",
      "~/github/mne-python/mne/utils/check.py in _on_missing(on_missing, msg, name, error_klass)\n",
      "    745     on_missing = 'warn' if on_missing == 'warning' else on_missing\n",
      "    746     if on_missing == 'raise':\n",
      "--> 747         raise error_klass(msg)\n",
      "    748     elif on_missing == 'warn':\n",
      "    749         warn(msg)\n",
      "\n",
      "ValueError: DigMontage is only a subset of info. There are 123 channel positions not present in the DigMontage. The required channels are:\n",
      "\n",
      "['Fp1-REF', 'Fp2-REF', 'F3 -REF', 'F4 -REF', 'C3 -REF', 'C4 -REF', 'P3 -REF', 'P4 -REF', 'O1 -REF', 'O2 -REF', 'F7 -REF', 'F8 -REF', 'T7 -REF', 'T8 -REF', 'P7 -REF', 'P8 -REF', 'Fz -REF', 'Cz -REF', 'Pz -REF', 'F1 -REF', 'F2 -REF', '22', '23', 'P1 -REF', 'P2 -REF', 'AF3-REF', 'AF4-REF', '28', '29', '30', '31', '32', '33', 'FT7-REF', 'FT8-REF', '36', '37', 'P5 -REF', 'P6 -REF', 'FC5-REF', 'FC6-REF', '42', '43', 'C5 -REF', 'C6 -REF', '46', '47', '48', '49', '50', '51', 'TP7-REF', 'TP8-REF', 'PO5-REF', 'PO6-REF', '56', '57', 'AF7-REF', 'AF8-REF', '60', '61', 'FpZ-REF', '63', 'FCZ-REF', 'CPZ-REF', 'POZ-REF', 'OZ -REF', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', 'PO3-REF', 'PO4-REF', '80', '81', 'CP1-REF', 'CP2-REF', '84', '85', '86', '87', '88', '89', 'CP3-REF', 'CP4-REF', '92', '93', '94', '95', 'C1 -REF', 'C2 -REF', 'F5 -REF', 'F6 -REF', 'FC3-REF', 'FC4-REF', 'FC1-REF', 'FC2-REF', '104', '105', '106', '107', '108', '109', '110', '111', 'CP5-REF', 'CP6-REF', 'PO7-REF', 'PO8-REF', '116', '117', '118', '119', '120', '121', '122', 'DC1'].\n",
      "\n",
      "Consider using inst.set_channel_types if these are not EEG channels, or use the on_missing parameter if the channel positions are allowed to be unknown in your analyses.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 1914 in GitHub error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++8/10: Error examples for Cluster 3206 in GitHub error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/1 with eid cdbab305-95af-37d0-98a0-6606fb581982.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "TypeError                                 Traceback (most recent call last)\n",
      "<ipython-input-4-23d58953eccb> in <module>\n",
      "----> 1 assignment = maup.assign(units, districts)\n",
      "\n",
      "c:\\dev\\maup\\maup\\crs.py in wrapped(*args, **kwargs)\n",
      "      9             raise TypeError(\n",
      "     10                 \"the source and target geometries must have the same CRS. {} {}\".format(\n",
      "---> 11                     geoms1.crs, geoms2.crs\n",
      "     12                 )\n",
      "     13             )\n",
      "\n",
      "TypeError: the source and target geometries must have the same CRS. {'proj': 'aea', 'lat_1': 29.5, 'lat_2': 45.5, 'lat_0': 37.5, 'lon_0': -96, 'x_0': 0, 'y_0': 0, 'ellps': 'GRS80', 'units': 'm', 'no_defs': True} {'init': 'epsg:4269'}\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 3206 in GitHub error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++9/10: Error examples for Cluster 7314 in GitHub error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/5 with eid 68245f6c-632a-35fa-8002-9c364879aaaa.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "RuntimeError                              Traceback (most recent call last)\n",
      "<ipython-input-18-a28fed9171f7> in <module>\n",
      "----> 1 er = EmotionRecognition(device='cpu')\n",
      "      2 #model = torch.load('pickle',map_location ='cpu')\n",
      "      3 cam = cv2.VideoCapture(0)\n",
      "      4 \n",
      "      5 while True:\n",
      "\n",
      "~\\anaconda3\\envs\\ML\\lib\\site-packages\\facial_emotion_recognition\\facial_emotion_recognition.py in __init__(self, device, gpu_id)\n",
      "     32         self.mtcnn = MTCNN(keep_all=True, device=self.device)\n",
      "     33 \n",
      "---> 34         model_dict = torch.load(os.path.join(os.path.dirname(__file__), 'model', 'model.pkl'))\n",
      "     35         print(f'[*] Accuracy: {model_dict[\"accuracy\"]}')\n",
      "     36         self.emotions = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\n",
      "\n",
      "~\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\serialization.py in load(f, map_location, pickle_module, **pickle_load_args)\n",
      "    591     \"\"\"Get layout extension object from its string representation.\n",
      "    592     \"\"\"\n",
      "--> 593     cache = _get_layout.cache\n",
      "    594     if not cache:\n",
      "    595         for v in torch.__dict__.values():\n",
      "\n",
      "~\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\serialization.py in _legacy_load(f, map_location, pickle_module, **pickle_load_args)\n",
      "    770     for key in deserialized_storage_keys:\n",
      "    771         assert key in deserialized_objects\n",
      "--> 772         deserialized_objects[key]._set_from_file(f, offset, f_should_read_directly)\n",
      "    773         if offset is not None:\n",
      "    774             offset = f.tell()\n",
      "\n",
      "~\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\serialization.py in persistent_load(saved_id)\n",
      "    726                     deserialized_objects[view_key] = storage[offset:offset + view_size]\n",
      "    727                 return deserialized_objects[view_key]\n",
      "--> 728             else:\n",
      "    729                 return storage\n",
      "    730         else:\n",
      "\n",
      "~\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\serialization.py in default_restore_location(storage, location)\n",
      "    173     for _, _, fn in _package_registry:\n",
      "    174         result = fn(storage, location)\n",
      "--> 175         if result is not None:\n",
      "    176             return result\n",
      "    177     raise RuntimeError(\"don't know how to restore data location of \"\n",
      "\n",
      "~\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\serialization.py in _cuda_deserialize(obj, location)\n",
      "    149     if location.startswith('cuda'):\n",
      "    150         device = validate_cuda_device(location)\n",
      "--> 151         if getattr(obj, \"_torch_load_uninitialized\", False):\n",
      "    152             storage_type = getattr(torch.cuda, type(obj).__name__)\n",
      "    153             with torch.cuda.device(device):\n",
      "\n",
      "~\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\serialization.py in validate_cuda_device(location)\n",
      "    133     if not torch.cuda.is_available():\n",
      "    134         raise RuntimeError('Attempting to deserialize object on a CUDA '\n",
      "--> 135                            'device but torch.cuda.is_available() is False. '\n",
      "    136                            'If you are running on a CPU-only machine, '\n",
      "    137                            'please use torch.load with map_location=torch.device(\\'cpu\\') '\n",
      "\n",
      "RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 2/5 with eid 2416fed3-efcc-3e70-a826-5b29cf5ab83e.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "RuntimeError                              Traceback (most recent call last)\n",
      "<ipython-input-29-ff9ff0700986> in <module>\n",
      "     16 model = torch.nn.DataParallel(model)\n",
      "     17 \n",
      "---> 18 checkpoint = torch.load(params.checkpoint_path)\n",
      "     19 model.load_state_dict(checkpoint['model'])\n",
      "     20 model.eval()\n",
      "\n",
      "~/miniconda/envs/bts/lib/python3.6/site-packages/torch/serialization.py in load(f, map_location, pickle_module, **pickle_load_args)\n",
      "    591                     return torch.jit.load(opened_file)\n",
      "    592                 return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\n",
      "--> 593         return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "    594 \n",
      "    595 \n",
      "\n",
      "~/miniconda/envs/bts/lib/python3.6/site-packages/torch/serialization.py in _legacy_load(f, map_location, pickle_module, **pickle_load_args)\n",
      "    770     unpickler = pickle_module.Unpickler(f, **pickle_load_args)\n",
      "    771     unpickler.persistent_load = persistent_load\n",
      "--> 772     result = unpickler.load()\n",
      "    773 \n",
      "    774     deserialized_storage_keys = pickle_module.load(f, **pickle_load_args)\n",
      "\n",
      "~/miniconda/envs/bts/lib/python3.6/site-packages/torch/serialization.py in persistent_load(saved_id)\n",
      "    726                 obj = data_type(size)\n",
      "    727                 obj._torch_load_uninitialized = True\n",
      "--> 728                 deserialized_objects[root_key] = restore_location(obj, location)\n",
      "    729             storage = deserialized_objects[root_key]\n",
      "    730             if view_metadata is not None:\n",
      "\n",
      "~/miniconda/envs/bts/lib/python3.6/site-packages/torch/serialization.py in default_restore_location(storage, location)\n",
      "    173 def default_restore_location(storage, location):\n",
      "    174     for _, _, fn in _package_registry:\n",
      "--> 175         result = fn(storage, location)\n",
      "    176         if result is not None:\n",
      "    177             return result\n",
      "\n",
      "~/miniconda/envs/bts/lib/python3.6/site-packages/torch/serialization.py in _cuda_deserialize(obj, location)\n",
      "    149 def _cuda_deserialize(obj, location):\n",
      "    150     if location.startswith('cuda'):\n",
      "--> 151         device = validate_cuda_device(location)\n",
      "    152         if getattr(obj, \"_torch_load_uninitialized\", False):\n",
      "    153             storage_type = getattr(torch.cuda, type(obj).__name__)\n",
      "\n",
      "~/miniconda/envs/bts/lib/python3.6/site-packages/torch/serialization.py in validate_cuda_device(location)\n",
      "    133 \n",
      "    134     if not torch.cuda.is_available():\n",
      "--> 135         raise RuntimeError('Attempting to deserialize object on a CUDA '\n",
      "    136                            'device but torch.cuda.is_available() is False. '\n",
      "    137                            'If you are running on a CPU-only machine, '\n",
      "\n",
      "RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 3/5 with eid 9d3fb913-efdc-3e8b-b519-4704a4052d58.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "RuntimeError                              Traceback (most recent call last)\n",
      "~/MBERT-VQA/rsvqa/eval.py in <module>\n",
      "    121 \n",
      "    122     print('Loading model at ', args.model_dir)\n",
      "--> 123     model.load_state_dict(torch.load(args.model_dir)) #,map_location=torch.device('cpu')\n",
      "    124 \n",
      "    125     model.to(device)\n",
      "\n",
      "~/.conda/envs/myenv/lib/python3.8/site-packages/torch/serialization.py in load(f, map_location, pickle_module, **pickle_load_args)\n",
      "    605                     opened_file.seek(orig_position)\n",
      "    606                     return torch.jit.load(opened_file)\n",
      "--> 607                 return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\n",
      "    608         return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "    609 \n",
      "\n",
      "~/.conda/envs/myenv/lib/python3.8/site-packages/torch/serialization.py in _load(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\n",
      "    880     unpickler = UnpicklerWrapper(data_file, **pickle_load_args)\n",
      "    881     unpickler.persistent_load = persistent_load\n",
      "--> 882     result = unpickler.load()\n",
      "    883 \n",
      "    884     torch._utils._validate_loaded_sparse_tensors()\n",
      "\n",
      "~/.conda/envs/myenv/lib/python3.8/site-packages/torch/serialization.py in persistent_load(saved_id)\n",
      "    855         data_type, key, location, size = data\n",
      "    856         if key not in loaded_storages:\n",
      "--> 857             load_tensor(data_type, size, key, _maybe_decode_ascii(location))\n",
      "    858         storage = loaded_storages[key]\n",
      "    859         return storage\n",
      "\n",
      "~/.conda/envs/myenv/lib/python3.8/site-packages/torch/serialization.py in load_tensor(data_type, size, key, location)\n",
      "    844 \n",
      "    845         storage = zip_file.get_storage_from_record(name, size, dtype).storage()\n",
      "--> 846         loaded_storages[key] = restore_location(storage, location)\n",
      "    847 \n",
      "    848     def persistent_load(saved_id):\n",
      "\n",
      "~/.conda/envs/myenv/lib/python3.8/site-packages/torch/serialization.py in default_restore_location(storage, location)\n",
      "    173 def default_restore_location(storage, location):\n",
      "    174     for _, _, fn in _package_registry:\n",
      "--> 175         result = fn(storage, location)\n",
      "    176         if result is not None:\n",
      "    177             return result\n",
      "\n",
      "~/.conda/envs/myenv/lib/python3.8/site-packages/torch/serialization.py in _cuda_deserialize(obj, location)\n",
      "    149 def _cuda_deserialize(obj, location):\n",
      "    150     if location.startswith('cuda'):\n",
      "--> 151         device = validate_cuda_device(location)\n",
      "    152         if getattr(obj, \"_torch_load_uninitialized\", False):\n",
      "    153             storage_type = getattr(torch.cuda, type(obj).__name__)\n",
      "\n",
      "~/.conda/envs/myenv/lib/python3.8/site-packages/torch/serialization.py in validate_cuda_device(location)\n",
      "    133 \n",
      "    134     if not torch.cuda.is_available():\n",
      "--> 135         raise RuntimeError('Attempting to deserialize object on a CUDA '\n",
      "    136                            'device but torch.cuda.is_available() is False. '\n",
      "    137                            'If you are running on a CPU-only machine, '\n",
      "\n",
      "RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 4/5 with eid ac7e3e13-85b9-3706-9619-cec9a7c23ff8.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "RuntimeError                              Traceback (most recent call last)\n",
      "<ipython-input-13-30c1fa6c7551> in <module>\n",
      "----> 1 model.load_state_dict(torch.load('model_cifar.pt'))\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py in load(f, map_location, pickle_module, **pickle_load_args)\n",
      "    583                     return torch.jit.load(opened_file)\n",
      "    584                 return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\n",
      "--> 585         return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "    586 \n",
      "    587 \n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py in _legacy_load(f, map_location, pickle_module, **pickle_load_args)\n",
      "    763     unpickler = pickle_module.Unpickler(f, **pickle_load_args)\n",
      "    764     unpickler.persistent_load = persistent_load\n",
      "--> 765     result = unpickler.load()\n",
      "    766 \n",
      "    767     deserialized_storage_keys = pickle_module.load(f, **pickle_load_args)\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py in persistent_load(saved_id)\n",
      "    719                 obj = data_type(size)\n",
      "    720                 obj._torch_load_uninitialized = True\n",
      "--> 721                 deserialized_objects[root_key] = restore_location(obj, location)\n",
      "    722             storage = deserialized_objects[root_key]\n",
      "    723             if view_metadata is not None:\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py in default_restore_location(storage, location)\n",
      "    172 def default_restore_location(storage, location):\n",
      "    173     for _, _, fn in _package_registry:\n",
      "--> 174         result = fn(storage, location)\n",
      "    175         if result is not None:\n",
      "    176             return result\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py in _cuda_deserialize(obj, location)\n",
      "    148 def _cuda_deserialize(obj, location):\n",
      "    149     if location.startswith('cuda'):\n",
      "--> 150         device = validate_cuda_device(location)\n",
      "    151         if getattr(obj, \"_torch_load_uninitialized\", False):\n",
      "    152             storage_type = getattr(torch.cuda, type(obj).__name__)\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py in validate_cuda_device(location)\n",
      "    132 \n",
      "    133     if not torch.cuda.is_available():\n",
      "--> 134         raise RuntimeError('Attempting to deserialize object on a CUDA '\n",
      "    135                            'device but torch.cuda.is_available() is False. '\n",
      "    136                            'If you are running on a CPU-only machine, '\n",
      "\n",
      "RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 5/5 with eid d282068e-9f02-3236-bf50-e31c1cd4a088.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "RuntimeError                              Traceback (most recent call last)\n",
      "<ipython-input-8-608110c7f3b1> in <module>\n",
      "     15         return out\n",
      "     16 \n",
      "---> 17 clf = Classifier(lstm_dim=lstm_dim).to(device)\n",
      "\n",
      "<ipython-input-8-608110c7f3b1> in __init__(self, lstm_dim)\n",
      "      3         super(Classifier, self).__init__()\n",
      "      4         self.embed = nn.Embedding(vocab_size, embedding_size)\n",
      "----> 5         self.embed.load_state_dict(torch.load(embedding_path))\n",
      "      6         self.embed.requires_grad = False\n",
      "      7 \n",
      "\n",
      "~/miniconda3/lib/python3.8/site-packages/torch/serialization.py in load(f, map_location, pickle_module, **pickle_load_args)\n",
      "    592                     opened_file.seek(orig_position)\n",
      "    593                     return torch.jit.load(opened_file)\n",
      "--> 594                 return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\n",
      "    595         return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "    596 \n",
      "\n",
      "~/miniconda3/lib/python3.8/site-packages/torch/serialization.py in _load(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\n",
      "    851     unpickler = pickle_module.Unpickler(data_file, **pickle_load_args)\n",
      "    852     unpickler.persistent_load = persistent_load\n",
      "--> 853     result = unpickler.load()\n",
      "    854 \n",
      "    855     torch._utils._validate_loaded_sparse_tensors()\n",
      "\n",
      "~/miniconda3/lib/python3.8/site-packages/torch/serialization.py in persistent_load(saved_id)\n",
      "    843         data_type, key, location, size = data\n",
      "    844         if key not in loaded_storages:\n",
      "--> 845             load_tensor(data_type, size, key, _maybe_decode_ascii(location))\n",
      "    846         storage = loaded_storages[key]\n",
      "    847         return storage\n",
      "\n",
      "~/miniconda3/lib/python3.8/site-packages/torch/serialization.py in load_tensor(data_type, size, key, location)\n",
      "    832 \n",
      "    833         storage = zip_file.get_storage_from_record(name, size, dtype).storage()\n",
      "--> 834         loaded_storages[key] = restore_location(storage, location)\n",
      "    835 \n",
      "    836     def persistent_load(saved_id):\n",
      "\n",
      "~/miniconda3/lib/python3.8/site-packages/torch/serialization.py in default_restore_location(storage, location)\n",
      "    173 def default_restore_location(storage, location):\n",
      "    174     for _, _, fn in _package_registry:\n",
      "--> 175         result = fn(storage, location)\n",
      "    176         if result is not None:\n",
      "    177             return result\n",
      "\n",
      "~/miniconda3/lib/python3.8/site-packages/torch/serialization.py in _cuda_deserialize(obj, location)\n",
      "    149 def _cuda_deserialize(obj, location):\n",
      "    150     if location.startswith('cuda'):\n",
      "--> 151         device = validate_cuda_device(location)\n",
      "    152         if getattr(obj, \"_torch_load_uninitialized\", False):\n",
      "    153             storage_type = getattr(torch.cuda, type(obj).__name__)\n",
      "\n",
      "~/miniconda3/lib/python3.8/site-packages/torch/serialization.py in validate_cuda_device(location)\n",
      "    133 \n",
      "    134     if not torch.cuda.is_available():\n",
      "--> 135         raise RuntimeError('Attempting to deserialize object on a CUDA '\n",
      "    136                            'device but torch.cuda.is_available() is False. '\n",
      "    137                            'If you are running on a CPU-only machine, '\n",
      "\n",
      "RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 7314 in GitHub error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++10/10: Error examples for Cluster 5100 in GitHub error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/1 with eid 55a2b37e-6c65-3209-97a8-5daf81541d71.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "NonsquareTechnosphere                     Traceback (most recent call last)\n",
      "<ipython-input-41-3e65273e23a2> in <module>\n",
      "      1 lca = bc.LCA(fu, data_objs=data_objs)\n",
      "----> 2 lca.lci()\n",
      "\n",
      "~/Code/bw2/calc/bw2calc/lca.py in lci(self, factorize)\n",
      "    329 \n",
      "    330         \"\"\"\n",
      "--> 331         self.load_lci_data()\n",
      "    332         self.build_demand_array()\n",
      "    333         if factorize:\n",
      "\n",
      "~/Code/bw2/calc/bw2calc/lca.py in load_lci_data(self)\n",
      "    175 \n",
      "    176         if len(self.technosphere_mm.row_mapper) != len(self.technosphere_mm.col_mapper):\n",
      "--> 177             raise NonsquareTechnosphere(\n",
      "    178                 (\n",
      "    179                     \"Technosphere matrix is not square: {} activities (columns) and {} products (rows). \"\n",
      "\n",
      "NonsquareTechnosphere: Technosphere matrix is not square: 4 activities (columns) and 5 products (rows). Use LeastSquaresLCA to solve this system, or fix the input data\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 5100 in GitHub error notebooks end.+++++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GitHub\n",
    "for j in range(len(sample_g_small_clusters)):\n",
    "    sgc = sample_g_small_clusters[j]\n",
    "    print('\\033[1m'+\"+++++{}/{}: Error examples for Cluster {} in GitHub error notebooks.+++++\\n\".format(j+1, 10, sgc)+'\\033[0m')\n",
    "    df_target = df_err_grouped_g_small[df_err_grouped_g_small.pregroup_cluster==sgc]\n",
    "    n = min(5, len(df_target))\n",
    "    tmp = df_target.sample(n=n, random_state=30)\n",
    "    for i in range(len(tmp)):\n",
    "        print(\"-----Error example {}/{} with eid {}.-----\\n\".format(i+1, n, tmp.iloc[i].eid))\n",
    "        util.print_traceback(tmp.iloc[i].traceback)\n",
    "        print(\"---------------------------------------------------------------------------\\n\")\n",
    "    print(\"+++++Error examples for Cluster {} in GitHub error notebooks end.+++++\\n\".format(sgc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72061266",
   "metadata": {},
   "source": [
    "Potentially interesting clusters:\n",
    "\n",
    "Round 1:\n",
    "Kaggle:\n",
    "\n",
    "         Cluster 7348 (size 4): \"ValueError: Cannot assign value to variable ' embedding_2/embeddings:0': Shape mismatch.The variable shape (514157, 300), and the assigned value shape (514157,) are incompatible.\" shape mismatch during model construction\n",
    "         Cluster 7274 (size 2): \"InternalError: RET_CHECK failure (tensorflow/core/tpu/graph_rewrite/distributed_tpu_rewrite_pass.cc:2008) arg_shape.handle_type != DT_INVALID  input edge: [id=881 sequential_keras_layer_2728:0 -> cluster_train_function:63] [Op:__inference_train_function_3219]\" due to (probably) improper training dataset size\n",
    "         Cluster 7345 (size 4): \"RuntimeError: stack expects each tensor to be equal size, but got [19] at entry 0 and [21] at entry 1\"\n",
    "\t\n",
    "GitHub:\n",
    "\n",
    "         Cluster 251 (size 13): \"RuntimeError: Calculated padded input size per channel: (1 x 1). Kernel size: (3 x 3). Kernel size can't be greater than actual input size\" (torch)\n",
    "         Cluster 7037 (size 25): \"ValueError: all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 2 dimension(s)\" (numpy)\n",
    "        \n",
    "Round 2:\n",
    "Kaggle:\n",
    "\n",
    "         (In the sample) Cluster 720 (size 37): \"ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 32), found shape=(None, 38)\"\n",
    "         (In the sample) Cluster 6655 (size 5): \"AssertionError: Torch not compiled with CUDA enabled\"\n",
    "         Cluster 7024 (size 1): \"ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.\" initialization error\n",
    "        \n",
    "GitHub:\n",
    "\n",
    "         Cluster 7307 (size 63): \"ValueError: Number of features of the model must match the input. Model n_features is 11 and input n_features is 10 \" tensorshape mismatch in model construction\n",
    "         (In the sample) Cluster 7314 (size 21): \"RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fd456da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yirwa29\\AppData\\Local\\anaconda3\\envs\\chatgpt\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# check if any of them are included in the samples\n",
    "df_mlerr_labels = pd.read_excel(config.path_default.joinpath('Manual_labeing/cluster_sampled_labeled.xlsx'),\n",
    "                                sheet_name = \"Del-All\",\n",
    "                                keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06f9000b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7348 cluster size\n",
      "    Kaggle: 4\n",
      "    GitHub: 1\n",
      "    If being sampled: False\n",
      "7274 cluster size\n",
      "    Kaggle: 2\n",
      "    GitHub: 0\n",
      "    If being sampled: False\n",
      "7345 cluster size\n",
      "    Kaggle: 4\n",
      "    GitHub: 12\n",
      "    If being sampled: False\n",
      "251 cluster size\n",
      "    Kaggle: 0\n",
      "    GitHub: 13\n",
      "    If being sampled: False\n",
      "7037 cluster size\n",
      "    Kaggle: 1\n",
      "    GitHub: 25\n",
      "    If being sampled: False\n"
     ]
    }
   ],
   "source": [
    "for i in [7348,7274,7345,251,7037]:\n",
    "    print(\"{} cluster size\".format(i))\n",
    "    print(\"    Kaggle: {}\".format(sum(df_err_grouped_k.pregroup_cluster==i)))\n",
    "    print(\"    GitHub: {}\".format(sum(df_err_grouped_g.pregroup_cluster==i)))\n",
    "    print(\"    If being sampled: {}\".format(sum(df_mlerr_labels.pregroup_cluster==i)>0))\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66697aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720 cluster size\n",
      "    Kaggle: 37\n",
      "    GitHub: 155\n",
      "    If being sampled: True\n",
      "6655 cluster size\n",
      "    Kaggle: 5\n",
      "    GitHub: 82\n",
      "    If being sampled: True\n",
      "7024 cluster size\n",
      "    Kaggle: 1\n",
      "    GitHub: 3\n",
      "    If being sampled: False\n",
      "7307 cluster size\n",
      "    Kaggle: 2\n",
      "    GitHub: 63\n",
      "    If being sampled: False\n",
      "7314 cluster size\n",
      "    Kaggle: 5\n",
      "    GitHub: 21\n",
      "    If being sampled: True\n"
     ]
    }
   ],
   "source": [
    "for i in [720,6655,7024,7307,7314]:\n",
    "    print(\"{} cluster size\".format(i))\n",
    "    print(\"    Kaggle: {}\".format(sum(df_err_grouped_k.pregroup_cluster==i)))\n",
    "    print(\"    GitHub: {}\".format(sum(df_err_grouped_g.pregroup_cluster==i)))\n",
    "    print(\"    If being sampled: {}\".format(sum(df_mlerr_labels.pregroup_cluster==i)>0))\n",
    "#     print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
