{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74352832",
   "metadata": {},
   "source": [
    "## Check crashes in small clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a16127f",
   "metadata": {},
   "source": [
    "Get crashes in smaller clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aceb91d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import utils.config as config\n",
    "import utils.util as util\n",
    "import numpy as np\n",
    "\n",
    "df_err_grouped_k = pd.read_excel(config.path_default.joinpath('Clustering/clusters_Kaggle.xlsx'))\n",
    "df_err_grouped_g = pd.read_excel(config.path_default.joinpath('Clustering/clusters_GitHub.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8bc5e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_clusters_g = pd.read_excel(config.path_default.joinpath('Sampling/cluster_size_samples_g.xlsx'))\n",
    "selected_clusters_k = pd.read_excel(config.path_default.joinpath('Sampling/cluster_size_samples_k.xlsx'))\n",
    "\n",
    "# big clusters (sample size >= 10)\n",
    "clusters_big_k = selected_clusters_k[selected_clusters_k.sample_size>=10].cluster_id[1:]\n",
    "clusters_big_g = selected_clusters_g[selected_clusters_g.sample_size>=10].cluster_id[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a04fcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get crashes from small clusters (sample size < 10)\n",
    "df_err_grouped_k_small = df_err_grouped_k[~df_err_grouped_k.pregroup_cluster.isin(clusters_big_k)]\n",
    "df_err_grouped_g_small = df_err_grouped_g[~df_err_grouped_g.pregroup_cluster.isin(clusters_big_g)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e08383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select 10 small clusters from each source\n",
    "sample_k_small_clusters = np.random.choice(df_err_grouped_k_small.pregroup_cluster.unique(), size=10, replace=False)\n",
    "sample_g_small_clusters = np.random.choice(df_err_grouped_g_small.pregroup_cluster.unique(), size=10, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a31762d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m+++++1/10: Error examples for Cluster 6376 in Kaggle error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/1 with eid 454f3361-031b-373d-8f77-b40c3b5dc05f.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "Cell In[23], line 3\n",
      "      1 lst = []\n",
      "      2 for l in image_df['Label'].unique():\n",
      "----> 3     lst.append(image_df[image_df['Label'] == l] .sample(100, random_state = 0))\n",
      "      4 # # Concatenate the DataFrames\n",
      "      5 image_df = pd.concat(lst)\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/pandas/core/generic.py:5858, in NDFrame.sample(self, n, frac, replace, weights, random_state, axis, ignore_index)\n",
      "   5855 if weights is not None:\n",
      "   5856     weights = sample.preprocess_weights(self, weights, axis)\n",
      "-> 5858 sampled_indices = sample.sample(obj_len, size, replace, weights, rs)\n",
      "   5859 result = self.take(sampled_indices, axis=axis)\n",
      "   5861 if ignore_index:\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/pandas/core/sample.py:151, in sample(obj_len, size, replace, weights, random_state)\n",
      "    148     else:\n",
      "    149         raise ValueError(\"Invalid weights: weights sum to zero\")\n",
      "--> 151 return random_state.choice(obj_len, size=size, replace=replace, p=weights).astype(\n",
      "    152     np.intp, copy=False\n",
      "    153 )\n",
      "\n",
      "File mtrand.pyx:965, in numpy.random.mtrand.RandomState.choice()\n",
      "\n",
      "ValueError: Cannot take a larger sample than population when 'replace=False'\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 6376 in Kaggle error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++2/10: Error examples for Cluster 7347 in Kaggle error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/3 with eid 3bdd4e66-d6aa-3e42-bf59-f33c57c0d744.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "Exception                                 Traceback (most recent call last)\n",
      "/tmp/ipykernel_23/2066791573.py in <module>\n",
      "----> 1 env = amp_pd_peptide.make_env()   # initialize the environment\n",
      "      2 iter_test = env.iter_test()\n",
      "\n",
      "/kaggle/input/amp-pd/amp_pd_peptide/competition.cpython-37m-x86_64-linux-gnu.so in amp_pd_peptide.competition.make_env()\n",
      "\n",
      "Exception: You can only call `make_env()` once.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 2/3 with eid fdd6973e-c05f-3d90-951a-cd98254a788d.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "Exception                                 Traceback (most recent call last)\n",
      "/tmp/ipykernel_24/115935697.py in <module>\n",
      "      2 \n",
      "      3 import jo_wilder\n",
      "----> 4 env = jo_wilder.make_env()\n",
      "      5 iter_test = env.iter_test()\n",
      "\n",
      "/kaggle/input/predict-student-performance-from-game-play/jo_wilder/competition.cpython-37m-x86_64-linux-gnu.so in jo_wilder.competition.make_env()\n",
      "\n",
      "Exception: You can only call `make_env()` once.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 3/3 with eid 528b2081-063a-3690-9bfe-c420db3ee87d.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "Exception                                 Traceback (most recent call last)\n",
      "/tmp/ipykernel_25/3006567047.py in <module>\n",
      "      1 import jo_wilder\n",
      "      2 \n",
      "----> 3 env = jo_wilder.make_env()\n",
      "      4 iter_test = env.iter_test()\n",
      "\n",
      "/kaggle/input/predict-student-performance-from-game-play/jo_wilder/competition.cpython-37m-x86_64-linux-gnu.so in jo_wilder.competition.make_env()\n",
      "\n",
      "Exception: You can only call `make_env()` once.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 7347 in Kaggle error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++3/10: Error examples for Cluster 268 in Kaggle error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/5 with eid 7380ea61-54de-3f17-a2bb-a568a8ba237e.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "IndexError                                Traceback (most recent call last)\n",
      "Cell In[56], line 47\n",
      "     45 # Evaluate the model on the test data\n",
      "     46 results = model.evaluate(test_data.batch(32), verbose=0)\n",
      "---> 47 print('Test loss:', results[0])\n",
      "     48 print('Test accuracy:', results[1])\n",
      "     50 # Convert the model to a quantized model\n",
      "\n",
      "IndexError: list index out of range\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 2/5 with eid 13947cba-ba20-3d15-8b12-59bd79ba0413.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "IndexError                                Traceback (most recent call last)\n",
      "/tmp/ipykernel_27/380501505.py in <module>\n",
      "      3     nmap = binary_image[cluster_of_interest[0]:cluster_of_interest[len(cluster_of_interest)-1],:]\n",
      "      4     path = np.array(astar(nmap, (int(nmap.shape[0]/2), 0), (int(nmap.shape[0]/2),nmap.shape[1]-1)))\n",
      "----> 5     offset_from_top = cluster_of_interest[1]\n",
      "      6     path[:,0] += offset_from_top\n",
      "      7     line_segments.append(path)\n",
      "\n",
      "IndexError: list index out of range\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 3/5 with eid 417fdb41-2668-3379-9ea1-5cefcc60e114.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "IndexError                                Traceback (most recent call last)\n",
      "Cell In[150], line 7\n",
      "      1 model = tf.keras.Sequential([\n",
      "      2     tf.keras.Input(shape=(hsi_data.shape[1], hsi_data.shape[2], hsi_data.shape[3])),\n",
      "      3     Conv2D(16, 3, padding='same', activation='relu'),\n",
      "      4     Conv2D(32, 3, padding='same', activation='relu'),\n",
      "      5     Flatten(),\n",
      "      6     Dense(128, activation='relu'),\n",
      "----> 7     Dense(labels.shape[1], activation='softmax')\n",
      "      8 ])\n",
      "\n",
      "IndexError: tuple index out of range\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 4/5 with eid 2b843df1-6579-315e-bd3a-b751c40b62dc.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "IndexError                                Traceback (most recent call last)\n",
      "Cell In[28], line 3\n",
      "      1 model = YOLO(\"yolov8n.pt\")\n",
      "      2 image = np.asarray('/kaggle/input/planesnet/scenes/scenes/scene_1.png')\n",
      "----> 3 results = model.predict(image)\n",
      "      4 plot_bboxes(image, results[0].boxes.data, score=False)\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/ultralytics/engine/model.py:242, in Model.predict(self, source, stream, predictor, **kwargs)\n",
      "    240 if prompts and hasattr(self.predictor, 'set_prompts'):  # for SAM-type models\n",
      "    241     self.predictor.set_prompts(prompts)\n",
      "--> 242 return self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/ultralytics/engine/predictor.py:196, in BasePredictor.__call__(self, source, model, stream, *args, **kwargs)\n",
      "    194     return self.stream_inference(source, model, *args, **kwargs)\n",
      "    195 else:\n",
      "--> 196     return list(self.stream_inference(source, model, *args, **kwargs))\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:35, in _wrap_generator.<locals>.generator_context(*args, **kwargs)\n",
      "     32 try:\n",
      "     33     # Issuing `None` to a generator fires it up\n",
      "     34     with ctx_factory():\n",
      "---> 35         response = gen.send(None)\n",
      "     37     while True:\n",
      "     38         try:\n",
      "     39             # Forward the response to our caller and get its next request\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/ultralytics/engine/predictor.py:255, in BasePredictor.stream_inference(self, source, model, *args, **kwargs)\n",
      "    253 # Preprocess\n",
      "    254 with profilers[0]:\n",
      "--> 255     im = self.preprocess(im0s)\n",
      "    257 # Inference\n",
      "    258 with profilers[1]:\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/ultralytics/engine/predictor.py:120, in BasePredictor.preprocess(self, im)\n",
      "    118 not_tensor = not isinstance(im, torch.Tensor)\n",
      "    119 if not_tensor:\n",
      "--> 120     im = np.stack(self.pre_transform(im))\n",
      "    121     im = im[..., ::-1].transpose((0, 3, 1, 2))  # BGR to RGB, BHWC to BCHW, (n, 3, h, w)\n",
      "    122     im = np.ascontiguousarray(im)  # contiguous\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/ultralytics/engine/predictor.py:149, in BasePredictor.pre_transform(self, im)\n",
      "    147 same_shapes = all(x.shape == im[0].shape for x in im)\n",
      "    148 letterbox = LetterBox(self.imgsz, auto=same_shapes and self.model.pt, stride=self.model.stride)\n",
      "--> 149 return [letterbox(image=x) for x in im]\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/ultralytics/engine/predictor.py:149, in <listcomp>(.0)\n",
      "    147 same_shapes = all(x.shape == im[0].shape for x in im)\n",
      "    148 letterbox = LetterBox(self.imgsz, auto=same_shapes and self.model.pt, stride=self.model.stride)\n",
      "--> 149 return [letterbox(image=x) for x in im]\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/ultralytics/data/augment.py:668, in LetterBox.__call__(self, labels, image)\n",
      "    665     new_shape = (new_shape, new_shape)\n",
      "    667 # Scale ratio (new / old)\n",
      "--> 668 r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
      "    669 if not self.scaleup:  # only scale down, do not scale up (for better val mAP)\n",
      "    670     r = min(r, 1.0)\n",
      "\n",
      "IndexError: tuple index out of range\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 5/5 with eid 09f2d057-01dd-3edf-9e55-5a4d37b9d3c3.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "IndexError                                Traceback (most recent call last)\n",
      "/tmp/ipykernel_27/4063913641.py in <module>\n",
      "     20 print('Number of words in first document: ', len(train['Text'][0]))\n",
      "     21 print('Number of words in second document: ', len(train['Text'][1]))\n",
      "---> 22 print('Size of vector embeddings: ', train_vec.shape[1])\n",
      "     23 print('Shape of vectors embeddings matrix: ', train_vec.shape)\n",
      "\n",
      "IndexError: tuple index out of range\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 268 in Kaggle error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++4/10: Error examples for Cluster 7261 in Kaggle error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/1 with eid 6c625563-efee-366a-af89-268f5ce00154.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "OSError                                   Traceback (most recent call last)\n",
      "Cell In[76], line 15\n",
      "     13 keras = tf.keras\n",
      "     14 # Load your pre-trained model (replace 'model_path' with your model's file path)\n",
      "---> 15 model = keras.models.load_model('/kaggle/working/models/simple_model/saved_model.pb')\n",
      "     17 # Create a figure to display images\n",
      "     18 plt.figure(figsize=(15, 10))\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/keras/saving/saving_api.py:212, in load_model(filepath, custom_objects, compile, safe_mode, **kwargs)\n",
      "    204     return saving_lib.load_model(\n",
      "    205         filepath,\n",
      "    206         custom_objects=custom_objects,\n",
      "    207         compile=compile,\n",
      "    208         safe_mode=safe_mode,\n",
      "    209     )\n",
      "    211 # Legacy case.\n",
      "--> 212 return legacy_sm_saving_lib.load_model(\n",
      "    213     filepath, custom_objects=custom_objects, compile=compile, **kwargs\n",
      "    214 )\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)\n",
      "     67     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "     68     # To get the full stack trace, call:\n",
      "     69     # `tf.debugging.disable_traceback_filtering()`\n",
      "---> 70     raise e.with_traceback(filtered_tb) from None\n",
      "     71 finally:\n",
      "     72     del filtered_tb\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/h5py/_hl/files.py:567, in File.__init__(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\n",
      "    558     fapl = make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n",
      "    559                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n",
      "    560                      alignment_threshold=alignment_threshold,\n",
      "    561                      alignment_interval=alignment_interval,\n",
      "    562                      meta_block_size=meta_block_size,\n",
      "    563                      **kwds)\n",
      "    564     fcpl = make_fcpl(track_order=track_order, fs_strategy=fs_strategy,\n",
      "    565                      fs_persist=fs_persist, fs_threshold=fs_threshold,\n",
      "    566                      fs_page_size=fs_page_size)\n",
      "--> 567     fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "    569 if isinstance(libver, tuple):\n",
      "    570     self._libver = libver\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/h5py/_hl/files.py:231, in make_fid(name, mode, userblock_size, fapl, fcpl, swmr)\n",
      "    229     if swmr and swmr_support:\n",
      "    230         flags |= h5f.ACC_SWMR_READ\n",
      "--> 231     fid = h5f.open(name, flags, fapl=fapl)\n",
      "    232 elif mode == 'r+':\n",
      "    233     fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "\n",
      "File h5py/_objects.pyx:54, in h5py._objects.with_phil.wrapper()\n",
      "\n",
      "File h5py/_objects.pyx:55, in h5py._objects.with_phil.wrapper()\n",
      "\n",
      "File h5py/h5f.pyx:106, in h5py.h5f.open()\n",
      "\n",
      "OSError: Unable to open file (file signature not found)\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 7261 in Kaggle error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++5/10: Error examples for Cluster 7348 in Kaggle error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/4 with eid bf3aaf37-2d2f-3837-8f13-8df8dd472f6d.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "Cell In[21], line 33\n",
      "     30 labels = df['label'].astype('category').cat.codes\n",
      "     32 # Example RNN model with LSTM and Dense layer for multi-class classification\n",
      "---> 33 model = tf.keras.Sequential([\n",
      "     34     tf.keras.layers.Embedding(input_dim=len(nlp.vocab.vectors), output_dim=nlp.vocab.vectors_length, input_length=padded_sequences.shape[1],\n",
      "     35                               weights=[nlp.vocab.vectors]),\n",
      "     36     tf.keras.layers.LSTM(units=32),\n",
      "     37     tf.keras.layers.Dense(units=len(df['label'].unique()), activation='softmax')\n",
      "     38 ])\n",
      "     40 # Compile and train the model\n",
      "     41 model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:205, in no_automatic_dependency_tracking.<locals>._method_wrapper(self, *args, **kwargs)\n",
      "    203 self._self_setattr_tracking = False  # pylint: disable=protected-access\n",
      "    204 try:\n",
      "--> 205   result = method(self, *args, **kwargs)\n",
      "    206 finally:\n",
      "    207   self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)\n",
      "     67     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "     68     # To get the full stack trace, call:\n",
      "     69     # `tf.debugging.disable_traceback_filtering()`\n",
      "---> 70     raise e.with_traceback(filtered_tb) from None\n",
      "     71 finally:\n",
      "     72     del filtered_tb\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/keras/backend.py:4302, in batch_set_value(tuples)\n",
      "   4300 if tf.executing_eagerly() or tf.inside_function():\n",
      "   4301     for x, value in tuples:\n",
      "-> 4302         x.assign(np.asarray(value, dtype=dtype_numpy(x)))\n",
      "   4303 else:\n",
      "   4304     with get_graph().as_default():\n",
      "\n",
      "ValueError: Cannot assign value to variable ' embedding_2/embeddings:0': Shape mismatch.The variable shape (514157, 300), and the assigned value shape (514157,) are incompatible.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 2/4 with eid 18997cdb-3d48-3b32-b9a5-02b4ecfea79d.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "Cell In[153], line 1\n",
      "----> 1 model = create_model(input_shape=(HEIGHT, WIDTH, CANAL), n_out=N_CLASSES)\n",
      "      3 for layer in model.layers:\n",
      "      4     layer.trainable = False\n",
      "\n",
      "Cell In[143], line 6, in create_model(input_shape, n_out)\n",
      "      2 input_tensor = Input(shape=input_shape)\n",
      "      3 base_model = applications.ResNet50(weights=None, \n",
      "      4                                    include_top=False,\n",
      "      5                                    input_tensor=input_tensor)\n",
      "----> 6 base_model.load_weights('../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
      "      8 x = GlobalAveragePooling2D()(base_model.output)\n",
      "      9 x = Dropout(0.5)(x)\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)\n",
      "     67     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "     68     # To get the full stack trace, call:\n",
      "     69     # `tf.debugging.disable_traceback_filtering()`\n",
      "---> 70     raise e.with_traceback(filtered_tb) from None\n",
      "     71 finally:\n",
      "     72     del filtered_tb\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/keras/backend.py:4360, in _assign_value_to_variable(variable, value)\n",
      "   4357     variable.assign(d_value)\n",
      "   4358 else:\n",
      "   4359     # For the normal tf.Variable assign\n",
      "-> 4360     variable.assign(value)\n",
      "\n",
      "ValueError: Cannot assign value to variable ' conv3_block1_0_conv/kernel:0': Shape mismatch.The variable shape (1, 1, 256, 512), and the assigned value shape (512, 128, 1, 1) are incompatible.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 3/4 with eid d9719bdc-895b-35e8-953a-47bc3418bd95.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "Cell In[153], line 1\n",
      "----> 1 model = create_model(input_shape=(HEIGHT, WIDTH, CANAL), n_out=N_CLASSES)\n",
      "      3 for layer in model.layers:\n",
      "      4     layer.trainable = False\n",
      "\n",
      "Cell In[143], line 6, in create_model(input_shape, n_out)\n",
      "      2 input_tensor = Input(shape=input_shape)\n",
      "      3 base_model = applications.ResNet50(weights=None, \n",
      "      4                                    include_top=False,\n",
      "      5                                    input_tensor=input_tensor)\n",
      "----> 6 base_model.load_weights('../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
      "      8 x = GlobalAveragePooling2D()(base_model.output)\n",
      "      9 x = Dropout(0.5)(x)\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)\n",
      "     67     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "     68     # To get the full stack trace, call:\n",
      "     69     # `tf.debugging.disable_traceback_filtering()`\n",
      "---> 70     raise e.with_traceback(filtered_tb) from None\n",
      "     71 finally:\n",
      "     72     del filtered_tb\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/keras/backend.py:4360, in _assign_value_to_variable(variable, value)\n",
      "   4357     variable.assign(d_value)\n",
      "   4358 else:\n",
      "   4359     # For the normal tf.Variable assign\n",
      "-> 4360     variable.assign(value)\n",
      "\n",
      "ValueError: Cannot assign value to variable ' conv3_block1_0_conv/kernel:0': Shape mismatch.The variable shape (1, 1, 256, 512), and the assigned value shape (512, 128, 1, 1) are incompatible.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 4/4 with eid 15f4a20a-4551-3aaa-9680-384bcf91fdd2.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "Cell In[54], line 2\n",
      "      1 # this will give error as the shape can't be changed\n",
      "----> 2 a.assign([1.0, 2.0, 3.0])\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/resource_variable_ops.py:980, in BaseResourceVariable.assign(self, value, use_locking, name, read_value)\n",
      "    978   else:\n",
      "    979     tensor_name = \" \" + str(self.name)\n",
      "--> 980   raise ValueError(\n",
      "    981       (f\"Cannot assign value to variable '{tensor_name}': Shape mismatch.\"\n",
      "    982        f\"The variable shape {self._shape}, and the \"\n",
      "    983        f\"assigned value shape {value_tensor.shape} are incompatible.\"))\n",
      "    984 kwargs = {}\n",
      "    985 if forward_compat.forward_compatible(2022, 3, 23):\n",
      "    986   # If the shape is fully defined, we do a runtime check with the shape of\n",
      "    987   # value.\n",
      "\n",
      "ValueError: Cannot assign value to variable ' Variable:0': Shape mismatch.The variable shape (2,), and the assigned value shape (3,) are incompatible.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 7348 in Kaggle error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++6/10: Error examples for Cluster 7274 in Kaggle error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/2 with eid 74ecaf90-f871-3838-a27a-11f845eed8aa.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "InternalError                             Traceback (most recent call last)\n",
      "Cell In[17], line 1\n",
      "----> 1 model.fit(train_dataset,\n",
      "      2           steps_per_epoch=steps_per_epoch,\n",
      "      3           validation_data=test_dataset,\n",
      "      4           validation_steps=validation_steps)\n",
      "\n",
      "File /usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)\n",
      "     67     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "     68     # To get the full stack trace, call:\n",
      "     69     # `tf.debugging.disable_traceback_filtering()`\n",
      "---> 70     raise e.with_traceback(filtered_tb) from None\n",
      "     71 finally:\n",
      "     72     del filtered_tb\n",
      "\n",
      "File /usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\n",
      "     50 try:\n",
      "     51   ctx.ensure_initialized()\n",
      "---> 52   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "     53                                       inputs, attrs, num_outputs)\n",
      "     54 except core._NotOkStatusException as e:\n",
      "     55   if name is not None:\n",
      "\n",
      "InternalError: RET_CHECK failure (tensorflow/core/tpu/graph_rewrite/distributed_tpu_rewrite_pass.cc:2008) arg_shape.handle_type != DT_INVALID  input edge: [id=881 sequential_keras_layer_2728:0 -> cluster_train_function:63] [Op:__inference_train_function_3219]\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 2/2 with eid 6fedf07c-053b-355d-9e75-02ae1a08acce.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "InternalError                             Traceback (most recent call last)\n",
      "Cell In[8], line 1\n",
      "----> 1 history = model.fit(partial_x_train,\n",
      "      2                     partial_y_train,\n",
      "      3                     epochs=40,\n",
      "      4                     batch_size=64,\n",
      "      5                     validation_data=(x_val, y_val),\n",
      "      6                     verbose=1)\n",
      "\n",
      "File /usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)\n",
      "     67     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "     68     # To get the full stack trace, call:\n",
      "     69     # `tf.debugging.disable_traceback_filtering()`\n",
      "---> 70     raise e.with_traceback(filtered_tb) from None\n",
      "     71 finally:\n",
      "     72     del filtered_tb\n",
      "\n",
      "File /usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\n",
      "     50 try:\n",
      "     51   ctx.ensure_initialized()\n",
      "---> 52   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "     53                                       inputs, attrs, num_outputs)\n",
      "     54 except core._NotOkStatusException as e:\n",
      "     55   if name is not None:\n",
      "\n",
      "InternalError: RET_CHECK failure (tensorflow/core/tpu/graph_rewrite/distributed_tpu_rewrite_pass.cc:2008) arg_shape.handle_type != DT_INVALID  input edge: [id=1905 Func/while/body/_1/input/_141:0 -> while/cluster_while_body_8972:63] [Op:__inference_train_function_10287]\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 7274 in Kaggle error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++7/10: Error examples for Cluster 7012 in Kaggle error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/1 with eid 2388c58c-7015-3b0c-b558-6166d3d861ff.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "TypeError                                 Traceback (most recent call last)\n",
      "/tmp/ipykernel_23/420082554.py in <module>\n",
      "     12 import tensorflow as tf\n",
      "     13 from tensorflow import keras\n",
      "---> 14 from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
      "     15 \n",
      "     16 get_ipython().run_line_magic('matplotlib', 'inline')\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/keras/api/_v2/keras/__init__.py in <module>\n",
      "     10 import sys as _sys\n",
      "     11 \n",
      "---> 12 from keras import __version__\n",
      "     13 from keras.api._v2.keras import __internal__\n",
      "     14 from keras.api._v2.keras import activations\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/keras/__init__.py in <module>\n",
      "     19 \"\"\"\n",
      "     20 from keras import distribute\n",
      "---> 21 from keras import models\n",
      "     22 from keras.engine.input_layer import Input\n",
      "     23 from keras.engine.sequential import Sequential\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/keras/models/__init__.py in <module>\n",
      "     16 \n",
      "     17 \n",
      "---> 18 from keras.engine.functional import Functional\n",
      "     19 from keras.engine.sequential import Sequential\n",
      "     20 from keras.engine.training import Model\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/keras/engine/functional.py in <module>\n",
      "     22 import warnings\n",
      "     23 \n",
      "---> 24 import tensorflow.compat.v2 as tf\n",
      "     25 \n",
      "     26 from keras import backend\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/_api/v2/compat/__init__.py in <module>\n",
      "     35 import sys as _sys\n",
      "     36 \n",
      "---> 37 from . import v1\n",
      "     38 from . import v2\n",
      "     39 from tensorflow.python.compat.compat import forward_compatibility_horizon\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/_api/v2/compat/v1/__init__.py in <module>\n",
      "     28 from . import autograph\n",
      "     29 from . import bitwise\n",
      "---> 30 from . import compat\n",
      "     31 from . import config\n",
      "     32 from . import data\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py in <module>\n",
      "     35 import sys as _sys\n",
      "     36 \n",
      "---> 37 from . import v1\n",
      "     38 from . import v2\n",
      "     39 from tensorflow.python.compat.compat import forward_compatibility_horizon\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py in <module>\n",
      "    103 from tensorflow.python.eager.backprop import GradientTape\n",
      "    104 from tensorflow.python.eager.context import executing_eagerly_v1 as executing_eagerly\n",
      "--> 105 from tensorflow.python.eager.polymorphic_function.polymorphic_function import function\n",
      "    106 from tensorflow.python.eager.wrap_function import wrap_function\n",
      "    107 from tensorflow.python.framework.constant_op import constant_v1 as constant\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py in <module>\n",
      "     75 from tensorflow.python.eager import monitoring\n",
      "     76 from tensorflow.python.eager.polymorphic_function import function_spec as function_spec_lib\n",
      "---> 77 from tensorflow.python.eager.polymorphic_function import monomorphic_function\n",
      "     78 from tensorflow.python.eager.polymorphic_function import tracing_compiler\n",
      "     79 from tensorflow.python.framework import composite_tensor\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py in <module>\n",
      "   2284 \n",
      "   2285 \n",
      "-> 2286 _pywrap_utils.RegisterType(\"Tensor\", ops.Tensor)\n",
      "   2287 _pywrap_utils.RegisterType(\"EagerTensor\", ops.EagerTensor)\n",
      "   2288 _pywrap_utils.RegisterType(\"IndexedSlices\", indexed_slices.IndexedSlices)\n",
      "\n",
      "TypeError: Value already registered for Tensor\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 7012 in Kaggle error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++8/10: Error examples for Cluster 7345 in Kaggle error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/4 with eid 40f957bc-dc49-3f40-a2a6-1bfde98580b4.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "RuntimeError                              Traceback (most recent call last)\n",
      "/tmp/ipykernel_27/535995891.py in <module>\n",
      "----> 1 train(st_train,5)\n",
      "\n",
      "/tmp/ipykernel_27/4138098979.py in train(traindata, epochs)\n",
      "      8         bloss = []\n",
      "      9         numb = 0\n",
      "---> 10         for idx ,(cx, cy)  in enumerate(datal):\n",
      "     11             optimizer.zero_grad()\n",
      "     12 #             print(cx.shape,cy.shape)\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py in __next__(self)\n",
      "    626                 # TODO(https://github.com/pytorch/pytorch/issues/76750)\n",
      "    627                 self._reset()  # type: ignore[call-arg]\n",
      "--> 628             data = self._next_data()\n",
      "    629             self._num_yielded += 1\n",
      "    630             if self._dataset_kind == _DatasetKind.Iterable and \\\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py in _next_data(self)\n",
      "    669     def _next_data(self):\n",
      "    670         index = self._next_index()  # may raise StopIteration\n",
      "--> 671         data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "    672         if self._pin_memory:\n",
      "    673             data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py in fetch(self, possibly_batched_index)\n",
      "     59         else:\n",
      "     60             data = self.dataset[possibly_batched_index]\n",
      "---> 61         return self.collate_fn(data)\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py in default_collate(batch)\n",
      "    263             >>> default_collate(batch)  # Handle `CustomType` automatically\n",
      "    264     \"\"\"\n",
      "--> 265     return collate(batch, collate_fn_map=default_collate_fn_map)\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py in collate(batch, collate_fn_map)\n",
      "    141 \n",
      "    142         if isinstance(elem, tuple):\n",
      "--> 143             return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n",
      "    144         else:\n",
      "    145             try:\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py in <listcomp>(.0)\n",
      "    141 \n",
      "    142         if isinstance(elem, tuple):\n",
      "--> 143             return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n",
      "    144         else:\n",
      "    145             try:\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py in collate(batch, collate_fn_map)\n",
      "    118     if collate_fn_map is not None:\n",
      "    119         if elem_type in collate_fn_map:\n",
      "--> 120             return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n",
      "    121 \n",
      "    122         for collate_type in collate_fn_map:\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py in collate_tensor_fn(batch, collate_fn_map)\n",
      "    161         storage = elem.storage()._new_shared(numel, device=elem.device)\n",
      "    162         out = elem.new(storage).resize_(len(batch), *list(elem.size()))\n",
      "--> 163     return torch.stack(batch, 0, out=out)\n",
      "    164 \n",
      "    165 \n",
      "\n",
      "RuntimeError: stack expects each tensor to be equal size, but got [19] at entry 0 and [21] at entry 1\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 2/4 with eid 386ee160-9a07-33f9-9d9c-2211c532f6c1.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "RuntimeError                              Traceback (most recent call last)\n",
      "Cell In[6], line 1\n",
      "----> 1 show_batch(dataloader)\n",
      "\n",
      "Cell In[5], line 7, in show_batch(dl)\n",
      "      6 def show_batch(dl):\n",
      "----> 7     for images, _ in dl:\n",
      "      8         show_images(images)\n",
      "      9         break\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634, in _BaseDataLoaderIter.__next__(self)\n",
      "    631 if self._sampler_iter is None:\n",
      "    632     # TODO(https://github.com/pytorch/pytorch/issues/76750)\n",
      "    633     self._reset()  # type: ignore[call-arg]\n",
      "--> 634 data = self._next_data()\n",
      "    635 self._num_yielded += 1\n",
      "    636 if self._dataset_kind == _DatasetKind.Iterable and \\\n",
      "    637         self._IterableDataset_len_called is not None and \\\n",
      "    638         self._num_yielded > self._IterableDataset_len_called:\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678, in _SingleProcessDataLoaderIter._next_data(self)\n",
      "    676 def _next_data(self):\n",
      "    677     index = self._next_index()  # may raise StopIteration\n",
      "--> 678     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "    679     if self._pin_memory:\n",
      "    680         data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54, in _MapDatasetFetcher.fetch(self, possibly_batched_index)\n",
      "     52 else:\n",
      "     53     data = self.dataset[possibly_batched_index]\n",
      "---> 54 return self.collate_fn(data)\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:264, in default_collate(batch)\n",
      "    203 def default_collate(batch):\n",
      "    204     r\"\"\"\n",
      "    205         Function that takes in a batch of data and puts the elements within the batch\n",
      "    206         into a tensor with an additional outer dimension - batch size. The exact output type can be\n",
      "   (...)\n",
      "    262             >>> default_collate(batch)  # Handle `CustomType` automatically\n",
      "    263     \"\"\"\n",
      "--> 264     return collate(batch, collate_fn_map=default_collate_fn_map)\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142, in collate(batch, collate_fn_map)\n",
      "    139 transposed = list(zip(*batch))  # It may be accessed twice, so we use a list.\n",
      "    141 if isinstance(elem, tuple):\n",
      "--> 142     return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n",
      "    143 else:\n",
      "    144     try:\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142, in <listcomp>(.0)\n",
      "    139 transposed = list(zip(*batch))  # It may be accessed twice, so we use a list.\n",
      "    141 if isinstance(elem, tuple):\n",
      "--> 142     return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n",
      "    143 else:\n",
      "    144     try:\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:119, in collate(batch, collate_fn_map)\n",
      "    117 if collate_fn_map is not None:\n",
      "    118     if elem_type in collate_fn_map:\n",
      "--> 119         return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n",
      "    121     for collate_type in collate_fn_map:\n",
      "    122         if isinstance(elem, collate_type):\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:162, in collate_tensor_fn(batch, collate_fn_map)\n",
      "    160     storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n",
      "    161     out = elem.new(storage).resize_(len(batch), *list(elem.size()))\n",
      "--> 162 return torch.stack(batch, 0, out=out)\n",
      "\n",
      "RuntimeError: stack expects each tensor to be equal size, but got [3, 123, 123] at entry 0 and [3, 73, 73] at entry 1\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 3/4 with eid f3fdb47d-1969-3bf2-9e73-fd5dce0a9d59.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "RuntimeError                              Traceback (most recent call last)\n",
      "Cell In[8], line 7\n",
      "      4 mean_discriminator_loss = 0\n",
      "      5 for epoch in range(n_epochs):\n",
      "      6     # Dataloader returns the batches\n",
      "----> 7     for real, _ in tqdm(dataloader):\n",
      "      8         cur_batch_size = len(real)\n",
      "      9         real = real.to(device)\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/tqdm/notebook.py:249, in tqdm_notebook.__iter__(self)\n",
      "    247 try:\n",
      "    248     it = super(tqdm_notebook, self).__iter__()\n",
      "--> 249     for obj in it:\n",
      "    250         # return super(tqdm...) will not catch exception\n",
      "    251         yield obj\n",
      "    252 # NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/tqdm/std.py:1182, in tqdm.__iter__(self)\n",
      "   1179 time = self._time\n",
      "   1181 try:\n",
      "-> 1182     for obj in iterable:\n",
      "   1183         yield obj\n",
      "   1184         # Update and possibly print the progressbar.\n",
      "   1185         # Note: does not call self.update(1) for speed optimisation.\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634, in _BaseDataLoaderIter.__next__(self)\n",
      "    631 if self._sampler_iter is None:\n",
      "    632     # TODO(https://github.com/pytorch/pytorch/issues/76750)\n",
      "    633     self._reset()  # type: ignore[call-arg]\n",
      "--> 634 data = self._next_data()\n",
      "    635 self._num_yielded += 1\n",
      "    636 if self._dataset_kind == _DatasetKind.Iterable and \\\n",
      "    637         self._IterableDataset_len_called is not None and \\\n",
      "    638         self._num_yielded > self._IterableDataset_len_called:\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678, in _SingleProcessDataLoaderIter._next_data(self)\n",
      "    676 def _next_data(self):\n",
      "    677     index = self._next_index()  # may raise StopIteration\n",
      "--> 678     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "    679     if self._pin_memory:\n",
      "    680         data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54, in _MapDatasetFetcher.fetch(self, possibly_batched_index)\n",
      "     52 else:\n",
      "     53     data = self.dataset[possibly_batched_index]\n",
      "---> 54 return self.collate_fn(data)\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:264, in default_collate(batch)\n",
      "    203 def default_collate(batch):\n",
      "    204     r\"\"\"\n",
      "    205         Function that takes in a batch of data and puts the elements within the batch\n",
      "    206         into a tensor with an additional outer dimension - batch size. The exact output type can be\n",
      "   (...)\n",
      "    262             >>> default_collate(batch)  # Handle `CustomType` automatically\n",
      "    263     \"\"\"\n",
      "--> 264     return collate(batch, collate_fn_map=default_collate_fn_map)\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142, in collate(batch, collate_fn_map)\n",
      "    139 transposed = list(zip(*batch))  # It may be accessed twice, so we use a list.\n",
      "    141 if isinstance(elem, tuple):\n",
      "--> 142     return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n",
      "    143 else:\n",
      "    144     try:\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142, in <listcomp>(.0)\n",
      "    139 transposed = list(zip(*batch))  # It may be accessed twice, so we use a list.\n",
      "    141 if isinstance(elem, tuple):\n",
      "--> 142     return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n",
      "    143 else:\n",
      "    144     try:\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:119, in collate(batch, collate_fn_map)\n",
      "    117 if collate_fn_map is not None:\n",
      "    118     if elem_type in collate_fn_map:\n",
      "--> 119         return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n",
      "    121     for collate_type in collate_fn_map:\n",
      "    122         if isinstance(elem, collate_type):\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:162, in collate_tensor_fn(batch, collate_fn_map)\n",
      "    160     storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n",
      "    161     out = elem.new(storage).resize_(len(batch), *list(elem.size()))\n",
      "--> 162 return torch.stack(batch, 0, out=out)\n",
      "\n",
      "RuntimeError: stack expects each tensor to be equal size, but got [3, 97, 97] at entry 0 and [3, 83, 83] at entry 1\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 4/4 with eid 9433965b-5943-36e6-86c7-c798eb50fa1c.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "RuntimeError                              Traceback (most recent call last)\n",
      "Cell In[79], line 12\n",
      "      9 print(len(train_loader), \"batches \")\n",
      "     10 print(len(val_loader), \" batches \")\n",
      "---> 12 for image,mask in train_loader:\n",
      "     13     # img_embed: (B, 256, 64, 64), gt2D: (B, 1, 256, 256), bboxes: (B, 4)\n",
      "     14     print(f\"{image.shape=}, {mask.shape=},{boxes.shape=}\")\n",
      "     15     break\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634, in _BaseDataLoaderIter.__next__(self)\n",
      "    631 if self._sampler_iter is None:\n",
      "    632     # TODO(https://github.com/pytorch/pytorch/issues/76750)\n",
      "    633     self._reset()  # type: ignore[call-arg]\n",
      "--> 634 data = self._next_data()\n",
      "    635 self._num_yielded += 1\n",
      "    636 if self._dataset_kind == _DatasetKind.Iterable and \\\n",
      "    637         self._IterableDataset_len_called is not None and \\\n",
      "    638         self._num_yielded > self._IterableDataset_len_called:\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678, in _SingleProcessDataLoaderIter._next_data(self)\n",
      "    676 def _next_data(self):\n",
      "    677     index = self._next_index()  # may raise StopIteration\n",
      "--> 678     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "    679     if self._pin_memory:\n",
      "    680         data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54, in _MapDatasetFetcher.fetch(self, possibly_batched_index)\n",
      "     52 else:\n",
      "     53     data = self.dataset[possibly_batched_index]\n",
      "---> 54 return self.collate_fn(data)\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:264, in default_collate(batch)\n",
      "    203 def default_collate(batch):\n",
      "    204     r\"\"\"\n",
      "    205         Function that takes in a batch of data and puts the elements within the batch\n",
      "    206         into a tensor with an additional outer dimension - batch size. The exact output type can be\n",
      "   (...)\n",
      "    262             >>> default_collate(batch)  # Handle `CustomType` automatically\n",
      "    263     \"\"\"\n",
      "--> 264     return collate(batch, collate_fn_map=default_collate_fn_map)\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142, in collate(batch, collate_fn_map)\n",
      "    139 transposed = list(zip(*batch))  # It may be accessed twice, so we use a list.\n",
      "    141 if isinstance(elem, tuple):\n",
      "--> 142     return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n",
      "    143 else:\n",
      "    144     try:\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142, in <listcomp>(.0)\n",
      "    139 transposed = list(zip(*batch))  # It may be accessed twice, so we use a list.\n",
      "    141 if isinstance(elem, tuple):\n",
      "--> 142     return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n",
      "    143 else:\n",
      "    144     try:\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:119, in collate(batch, collate_fn_map)\n",
      "    117 if collate_fn_map is not None:\n",
      "    118     if elem_type in collate_fn_map:\n",
      "--> 119         return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n",
      "    121     for collate_type in collate_fn_map:\n",
      "    122         if isinstance(elem, collate_type):\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:162, in collate_tensor_fn(batch, collate_fn_map)\n",
      "    160     storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n",
      "    161     out = elem.new(storage).resize_(len(batch), *list(elem.size()))\n",
      "--> 162 return torch.stack(batch, 0, out=out)\n",
      "\n",
      "RuntimeError: stack expects each tensor to be equal size, but got [0, 4] at entry 0 and [24, 4] at entry 1\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 7345 in Kaggle error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++9/10: Error examples for Cluster 7157 in Kaggle error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/3 with eid 1b359d5e-d22d-31db-919e-132677cc18d6.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "AttributeError                            Traceback (most recent call last)\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/axes/_base.py in ticklabel_format(self, axis, style, scilimits, useOffset, useLocale, useMathText)\n",
      "   3300                 if is_sci_style is not None:\n",
      "-> 3301                     axis.major.formatter.set_scientific(is_sci_style)\n",
      "   3302                 if scilimits is not None:\n",
      "\n",
      "AttributeError: 'FuncFormatter' object has no attribute 'set_scientific'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "AttributeError                            Traceback (most recent call last)\n",
      "/tmp/ipykernel_28/723682737.py in <module>\n",
      "      9 # Remove scientific notation\n",
      "     10 for ax in axs:\n",
      "---> 11     ax.ticklabel_format(style='plain', axis='x')\n",
      "     12     # Set x-axis limit\n",
      "     13     axs[0].set_xlim(-200000, 2000000)\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/axes/_base.py in ticklabel_format(self, axis, style, scilimits, useOffset, useLocale, useMathText)\n",
      "   3310         except AttributeError as err:\n",
      "   3311             raise AttributeError(\n",
      "-> 3312                 \"This method only works with the ScalarFormatter\") from err\n",
      "   3313 \n",
      "   3314     def locator_params(self, axis='both', tight=None, **kwargs):\n",
      "\n",
      "AttributeError: This method only works with the ScalarFormatter\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 2/3 with eid c83b69b7-aea1-378b-abe9-25e4dff33b8b.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "AttributeError                            Traceback (most recent call last)\n",
      "File /opt/conda/lib/python3.10/site-packages/matplotlib/axes/_base.py:3297, in _AxesBase.ticklabel_format(self, axis, style, scilimits, useOffset, useLocale, useMathText)\n",
      "   3296 if is_sci_style is not None:\n",
      "-> 3297     axis.major.formatter.set_scientific(is_sci_style)\n",
      "   3298 if scilimits is not None:\n",
      "\n",
      "AttributeError: 'FuncFormatter' object has no attribute 'set_scientific'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "AttributeError                            Traceback (most recent call last)\n",
      "Cell In[137], line 13\n",
      "     11 # Remove scientific notation\n",
      "     12 for ax in axs:\n",
      "---> 13     ax.ticklabel_format(style='plain', axis='x', useOffset=False)\n",
      "     14     # Set x-axis limit\n",
      "     15     axs[0].set_xlim(-200000, 2000000)\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/matplotlib/axes/_base.py:3307, in _AxesBase.ticklabel_format(self, axis, style, scilimits, useOffset, useLocale, useMathText)\n",
      "   3305             axis.major.formatter.set_useMathText(useMathText)\n",
      "   3306 except AttributeError as err:\n",
      "-> 3307     raise AttributeError(\n",
      "   3308         \"This method only works with the ScalarFormatter\") from err\n",
      "\n",
      "AttributeError: This method only works with the ScalarFormatter\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 3/3 with eid face70c4-87d8-3698-90c3-54fb82545405.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "AttributeError                            Traceback (most recent call last)\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/axes/_base.py in ticklabel_format(self, axis, style, scilimits, useOffset, useLocale, useMathText)\n",
      "   3300                 if is_sci_style is not None:\n",
      "-> 3301                     axis.major.formatter.set_scientific(is_sci_style)\n",
      "   3302                 if scilimits is not None:\n",
      "\n",
      "AttributeError: 'FuncFormatter' object has no attribute 'set_scientific'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "AttributeError                            Traceback (most recent call last)\n",
      "/tmp/ipykernel_28/147184186.py in <module>\n",
      "      5 sns.boxplot(margin[\"net_margin\"], ax=axs[2])\n",
      "      6 # Remove scientific notation\n",
      "----> 7 axs[0].ticklabel_format(style='plain', axis='x')\n",
      "      8 axs[1].ticklabel_format(style='plain', axis='x')\n",
      "      9 axs[2].ticklabel_format(style='plain', axis='x')\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/axes/_base.py in ticklabel_format(self, axis, style, scilimits, useOffset, useLocale, useMathText)\n",
      "   3310         except AttributeError as err:\n",
      "   3311             raise AttributeError(\n",
      "-> 3312                 \"This method only works with the ScalarFormatter\") from err\n",
      "   3313 \n",
      "   3314     def locator_params(self, axis='both', tight=None, **kwargs):\n",
      "\n",
      "AttributeError: This method only works with the ScalarFormatter\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 7157 in Kaggle error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++10/10: Error examples for Cluster 647 in Kaggle error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/5 with eid 13f6c8e9-a422-35e9-9d3c-371ea4cb3c61.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "KeyError                                  Traceback (most recent call last)\n",
      "Cell In[200], line 1\n",
      "----> 1 emotiondat.loc[[8116, 15221]]\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/pandas/core/indexing.py:1103, in _LocationIndexer.__getitem__(self, key)\n",
      "   1100 axis = self.axis or 0\n",
      "   1102 maybe_callable = com.apply_if_callable(key, self.obj)\n",
      "-> 1103 return self._getitem_axis(maybe_callable, axis=axis)\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/pandas/core/indexing.py:1332, in _LocIndexer._getitem_axis(self, key, axis)\n",
      "   1329     if hasattr(key, \"ndim\") and key.ndim > 1:\n",
      "   1330         raise ValueError(\"Cannot index with multidimensional key\")\n",
      "-> 1332     return self._getitem_iterable(key, axis=axis)\n",
      "   1334 # nested tuple slicing\n",
      "   1335 if is_nested_tuple(key, labels):\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/pandas/core/indexing.py:1272, in _LocIndexer._getitem_iterable(self, key, axis)\n",
      "   1269 self._validate_key(key, axis)\n",
      "   1271 # A collection of keys\n",
      "-> 1272 keyarr, indexer = self._get_listlike_indexer(key, axis)\n",
      "   1273 return self.obj._reindex_with_indexers(\n",
      "   1274     {axis: [keyarr, indexer]}, copy=True, allow_dups=True\n",
      "   1275 )\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/pandas/core/indexing.py:1462, in _LocIndexer._get_listlike_indexer(self, key, axis)\n",
      "   1459 ax = self.obj._get_axis(axis)\n",
      "   1460 axis_name = self.obj._get_axis_name(axis)\n",
      "-> 1462 keyarr, indexer = ax._get_indexer_strict(key, axis_name)\n",
      "   1464 return keyarr, indexer\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:5877, in Index._get_indexer_strict(self, key, axis_name)\n",
      "   5874 else:\n",
      "   5875     keyarr, indexer, new_indexer = self._reindex_non_unique(keyarr)\n",
      "-> 5877 self._raise_if_missing(keyarr, indexer, axis_name)\n",
      "   5879 keyarr = self.take(indexer)\n",
      "   5880 if isinstance(key, Index):\n",
      "   5881     # GH 42790 - Preserve name from an Index\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:5938, in Index._raise_if_missing(self, key, indexer, axis_name)\n",
      "   5936     if use_interval_msg:\n",
      "   5937         key = list(key)\n",
      "-> 5938     raise KeyError(f\"None of [{key}] are in the [{axis_name}]\")\n",
      "   5940 not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique())\n",
      "   5941 raise KeyError(f\"{not_found} not in index\")\n",
      "\n",
      "KeyError: \"None of [Index([8116, 15221], dtype='int64')] are in the [index]\"\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 2/5 with eid 71623a82-fed6-3029-a6dd-f6966e611e5b.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "KeyError                                  Traceback (most recent call last)\n",
      "/tmp/ipykernel_42/1889882571.py in ?()\n",
      "----> 1 b.set_index(['category','sub_category'], inplace = True)\n",
      "      2 b\n",
      "\n",
      "/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py in ?(self, keys, drop, append, inplace, verify_integrity)\n",
      "   5855                     if not found:\n",
      "   5856                         missing.append(col)\n",
      "   5857 \n",
      "   5858         if missing:\n",
      "-> 5859             raise KeyError(f\"None of {missing} are in the columns\")\n",
      "   5860 \n",
      "   5861         if inplace:\n",
      "   5862             frame = self\n",
      "\n",
      "KeyError: \"None of ['category', 'sub_category'] are in the columns\"\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 3/5 with eid ebf058e2-d05e-301e-9129-ed7539cbb882.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "KeyError                                  Traceback (most recent call last)\n",
      "Cell In[5], line 1\n",
      "----> 1 x = b[['First', 'Second', 'Third']]\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:3767, in DataFrame.__getitem__(self, key)\n",
      "   3765     if is_iterator(key):\n",
      "   3766         key = list(key)\n",
      "-> 3767     indexer = self.columns._get_indexer_strict(key, \"columns\")[1]\n",
      "   3769 # take() does not accept boolean indexers\n",
      "   3770 if getattr(indexer, \"dtype\", None) == bool:\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:5877, in Index._get_indexer_strict(self, key, axis_name)\n",
      "   5874 else:\n",
      "   5875     keyarr, indexer, new_indexer = self._reindex_non_unique(keyarr)\n",
      "-> 5877 self._raise_if_missing(keyarr, indexer, axis_name)\n",
      "   5879 keyarr = self.take(indexer)\n",
      "   5880 if isinstance(key, Index):\n",
      "   5881     # GH 42790 - Preserve name from an Index\n",
      "\n",
      "File /opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:5938, in Index._raise_if_missing(self, key, indexer, axis_name)\n",
      "   5936     if use_interval_msg:\n",
      "   5937         key = list(key)\n",
      "-> 5938     raise KeyError(f\"None of [{key}] are in the [{axis_name}]\")\n",
      "   5940 not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique())\n",
      "   5941 raise KeyError(f\"{not_found} not in index\")\n",
      "\n",
      "KeyError: \"None of [Index(['First', 'Second', 'Third'], dtype='object')] are in the [columns]\"\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 4/5 with eid f22e56fd-72d5-3a2c-8e56-75ac9b0a2bcc.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "KeyError                                  Traceback (most recent call last)\n",
      "/tmp/ipykernel_32/824952247.py in ?()\n",
      "      1 import matplotlib.pyplot as plt\n",
      "      2 biowaste_df = data.iloc[:, :11]\n",
      "      3 \n",
      "      4 # Transpose the DataFrame and set the index to Latitude and Longitude\n",
      "----> 5 biowaste_df.set_index(['Latitude', 'Longitude'], inplace=True)\n",
      "      6 biowaste_df = biowaste_df.T.reset_index()\n",
      "      7 \n",
      "      8 # Melt the DataFrame to have a 'Year' column and a 'Biowaste Value' column\n",
      "\n",
      "/opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py in ?(*args, **kwargs)\n",
      "    327                     msg.format(arguments=_format_argument_list(allow_args)),\n",
      "    328                     FutureWarning,\n",
      "    329                     stacklevel=find_stack_level(),\n",
      "    330                 )\n",
      "--> 331             return func(*args, **kwargs)\n",
      "\n",
      "/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py in ?(self, keys, drop, append, inplace, verify_integrity)\n",
      "   6008                     if not found:\n",
      "   6009                         missing.append(col)\n",
      "   6010 \n",
      "   6011         if missing:\n",
      "-> 6012             raise KeyError(f\"None of {missing} are in the columns\")\n",
      "   6013 \n",
      "   6014         if inplace:\n",
      "   6015             frame = self\n",
      "\n",
      "KeyError: \"None of ['Latitude', 'Longitude'] are in the columns\"\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 5/5 with eid dbb3916c-7ca1-379c-93d6-d2e4dc0bfd2b.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "KeyError                                  Traceback (most recent call last)\n",
      "/tmp/ipykernel_47/846008544.py in ?()\n",
      "----> 1 ghost_input.set_index('tim_id', inplace=True)\n",
      "\n",
      "/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py in ?(self, keys, drop, append, inplace, verify_integrity)\n",
      "   5855                     if not found:\n",
      "   5856                         missing.append(col)\n",
      "   5857 \n",
      "   5858         if missing:\n",
      "-> 5859             raise KeyError(f\"None of {missing} are in the columns\")\n",
      "   5860 \n",
      "   5861         if inplace:\n",
      "   5862             frame = self\n",
      "\n",
      "KeyError: \"None of ['tim_id'] are in the columns\"\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 647 in Kaggle error notebooks end.+++++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# randomly select min(5,cluster size) crashes from each cluster\n",
    "# Kaggle\n",
    "for j in range(len(sample_k_small_clusters)):\n",
    "    skc = sample_k_small_clusters[j]\n",
    "    print('\\033[1m'+\"+++++{}/{}: Error examples for Cluster {} in Kaggle error notebooks.+++++\\n\".format(j+1, 10, skc)+'\\033[0m')\n",
    "    df_target = df_err_grouped_k_small[df_err_grouped_k_small.pregroup_cluster==skc]\n",
    "    n = min(5, len(df_target))\n",
    "    tmp = df_target.sample(n=n, random_state=30)\n",
    "    for i in range(len(tmp)):\n",
    "        print(\"-----Error example {}/{} with eid {}.-----\\n\".format(i+1, n, tmp.iloc[i].eid))\n",
    "        util.print_traceback(tmp.iloc[i].traceback)\n",
    "        print(\"---------------------------------------------------------------------------\\n\")\n",
    "    print(\"+++++Error examples for Cluster {} in Kaggle error notebooks end.+++++\\n\".format(skc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "40f087f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m+++++1/10: Error examples for Cluster 6296 in GitHub error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/1 with eid b58c4060-d217-3b5c-bfed-62b8486c11d4.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "<ipython-input-6-fabd3c12b267> in <module>\n",
      "      3     pred_var='x',\n",
      "      4     truth_var='y',\n",
      "----> 5     plotvars=(\"precision_misspelled\", \"recall\"),\n",
      "      6 )\n",
      "\n",
      "~/opt/anaconda3/envs/ai_academy_3_7/lib/python3.7/site-packages/wvpy/util.py in threshold_plot(d, pred_var, truth_var, truth_target, threshold_range, plotvars, title)\n",
      "    365                          + str(prt_frame.columns)\n",
      "    366                          + \", \" + str(bad_plot_vars)\n",
      "--> 367                          + \" unexpected.\")\n",
      "    368 \n",
      "    369     selector = (threshold_range[0] <= prt_frame.threshold) & (\n",
      "\n",
      "ValueError: allowed plotting variables are: Index(['threshold', 'count', 'fraction', 'precision', 'true_positive_rate',\n",
      "       'false_positive_rate', 'true_negative_rate', 'false_negative_rate',\n",
      "       'enrichment', 'gain', 'lift', 'recall', 'sensitivity', 'specificity'],\n",
      "      dtype='object'), {'precision_misspelled'} unexpected.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 6296 in GitHub error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++2/10: Error examples for Cluster 251 in GitHub error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/5 with eid 23fe65a2-c2d0-3f21-8fd9-6a33cca811ac.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "RuntimeError                              Traceback (most recent call last)\n",
      "/tmp/ipykernel_3228549/3644741161.py in <module>\n",
      "     12 \n",
      "     13 # Loss and accuracy prior to training\n",
      "---> 14 vl, accuracy, _ = validate(None, valid_loader, device, model, criterion)\n",
      "     15 valid_losses.extend(vl)\n",
      "     16 accuracies.append(accuracy)\n",
      "\n",
      "/tmp/ipykernel_3228549/2487186573.py in validate(mb, loader, device, model, criterion)\n",
      "     27             X, Y = X.to(device), Y.to(device)\n",
      "     28 \n",
      "---> 29             output = model(X)\n",
      "     30 \n",
      "     31             loss = criterion(output, Y)\n",
      "\n",
      "/opt/mambaforge/envs/cs152/lib/python3.9/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n",
      "   1100         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
      "   1101                 or _global_forward_hooks or _global_forward_pre_hooks):\n",
      "-> 1102             return forward_call(*input, **kwargs)\n",
      "   1103         # Do not call functions when jit is used\n",
      "   1104         full_backward_hooks, non_full_backward_hooks = [], []\n",
      "\n",
      "/opt/mambaforge/envs/cs152/lib/python3.9/site-packages/torchvision/models/inception.py in forward(self, x)\n",
      "    198     def forward(self, x: Tensor) -> InceptionOutputs:\n",
      "    199         x = self._transform_input(x)\n",
      "--> 200         x, aux = self._forward(x)\n",
      "    201         aux_defined = self.training and self.aux_logits\n",
      "    202         if torch.jit.is_scripting():\n",
      "\n",
      "/opt/mambaforge/envs/cs152/lib/python3.9/site-packages/torchvision/models/inception.py in _forward(self, x)\n",
      "    157         x = self.Mixed_5d(x)\n",
      "    158         # N x 288 x 35 x 35\n",
      "--> 159         x = self.Mixed_6a(x)\n",
      "    160         # N x 768 x 17 x 17\n",
      "    161         x = self.Mixed_6b(x)\n",
      "\n",
      "/opt/mambaforge/envs/cs152/lib/python3.9/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n",
      "   1100         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
      "   1101                 or _global_forward_hooks or _global_forward_pre_hooks):\n",
      "-> 1102             return forward_call(*input, **kwargs)\n",
      "   1103         # Do not call functions when jit is used\n",
      "   1104         full_backward_hooks, non_full_backward_hooks = [], []\n",
      "\n",
      "/opt/mambaforge/envs/cs152/lib/python3.9/site-packages/torchvision/models/inception.py in forward(self, x)\n",
      "    280 \n",
      "    281     def forward(self, x: Tensor) -> Tensor:\n",
      "--> 282         outputs = self._forward(x)\n",
      "    283         return torch.cat(outputs, 1)\n",
      "    284 \n",
      "\n",
      "/opt/mambaforge/envs/cs152/lib/python3.9/site-packages/torchvision/models/inception.py in _forward(self, x)\n",
      "    268 \n",
      "    269     def _forward(self, x: Tensor) -> List[Tensor]:\n",
      "--> 270         branch3x3 = self.branch3x3(x)\n",
      "    271 \n",
      "    272         branch3x3dbl = self.branch3x3dbl_1(x)\n",
      "\n",
      "/opt/mambaforge/envs/cs152/lib/python3.9/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n",
      "   1100         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
      "   1101                 or _global_forward_hooks or _global_forward_pre_hooks):\n",
      "-> 1102             return forward_call(*input, **kwargs)\n",
      "   1103         # Do not call functions when jit is used\n",
      "   1104         full_backward_hooks, non_full_backward_hooks = [], []\n",
      "\n",
      "/opt/mambaforge/envs/cs152/lib/python3.9/site-packages/torchvision/models/inception.py in forward(self, x)\n",
      "    470 \n",
      "    471     def forward(self, x: Tensor) -> Tensor:\n",
      "--> 472         x = self.conv(x)\n",
      "    473         x = self.bn(x)\n",
      "    474         return F.relu(x, inplace=True)\n",
      "\n",
      "/opt/mambaforge/envs/cs152/lib/python3.9/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n",
      "   1100         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
      "   1101                 or _global_forward_hooks or _global_forward_pre_hooks):\n",
      "-> 1102             return forward_call(*input, **kwargs)\n",
      "   1103         # Do not call functions when jit is used\n",
      "   1104         full_backward_hooks, non_full_backward_hooks = [], []\n",
      "\n",
      "/opt/mambaforge/envs/cs152/lib/python3.9/site-packages/torch/nn/modules/conv.py in forward(self, input)\n",
      "    444 \n",
      "    445     def forward(self, input: Tensor) -> Tensor:\n",
      "--> 446         return self._conv_forward(input, self.weight, self.bias)\n",
      "    447 \n",
      "    448 class Conv3d(_ConvNd):\n",
      "\n",
      "/opt/mambaforge/envs/cs152/lib/python3.9/site-packages/torch/nn/modules/conv.py in _conv_forward(self, input, weight, bias)\n",
      "    440                             weight, bias, self.stride,\n",
      "    441                             _pair(0), self.dilation, self.groups)\n",
      "--> 442         return F.conv2d(input, weight, bias, self.stride,\n",
      "    443                         self.padding, self.dilation, self.groups)\n",
      "    444 \n",
      "\n",
      "RuntimeError: Calculated padded input size per channel: (1 x 1). Kernel size: (3 x 3). Kernel size can't be greater than actual input size\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 2/5 with eid 496cfb06-cb89-3756-b447-7dfcc226dbcb.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "RuntimeError                              Traceback (most recent call last)\n",
      "<ipython-input-12-1a52cecf785c> in <module>\n",
      "     29     for strides in strides_values:\n",
      "     30         print('(windows, strides):', (windows, strides))\n",
      "---> 31         nnet = run21(windows, strides)\n",
      "     32         Ytest, _, _ = nnet.use(Xtest)\n",
      "     33         #Ytest = Ttest # DEBUG\n",
      "\n",
      "<ipython-input-12-1a52cecf785c> in run21(windows, strides)\n",
      "     18         n_iterations=10,\n",
      "     19         batch_size=100,\n",
      "---> 20         learning_rate=0.001,\n",
      "     21     )\n",
      "     22     return nnet\n",
      "\n",
      "~/f/doc/school/files/y4/s2/lc445/a4/neuralnetworks_pytorch.py in train(self, Xtrain, Ttrain, Xtest, Ttest, n_iterations, batch_size, learning_rate)\n",
      "     95                 Ttrain_batch = Variable(Ttrain[start:end, ...], requires_grad=False)\n",
      "     96                 optimizer.zero_grad()\n",
      "---> 97                 Y = self.forward(Xtrain_batch)\n",
      "     98                 output = loss.forward(Y, Ttrain_batch)\n",
      "     99                 output.backward()\n",
      "\n",
      "~/f/doc/school/files/y4/s2/lc445/a4/neuralnetworks_pytorch.py in forward(self, X)\n",
      "     68 \n",
      "     69     def forward(self, X):\n",
      "---> 70         Ys = self.forward_all_outputs(X)\n",
      "     71         return Ys[-1]\n",
      "     72 \n",
      "\n",
      "~/f/doc/school/files/y4/s2/lc445/a4/neuralnetworks_pytorch.py in forward_all_outputs(self, X)\n",
      "     55         for i in range(len(self.conv_layers)):\n",
      "     56             # print('i', i, 'Input', Ys[-1].shape)\n",
      "---> 57             Ys.append(self.activation(self.conv_layers[i](Ys[-1])))\n",
      "     58 \n",
      "     59         for i in range(len(self.fc_layers)-1):\n",
      "\n",
      "/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\n",
      "    487             result = self._slow_forward(*input, **kwargs)\n",
      "    488         else:\n",
      "--> 489             result = self.forward(*input, **kwargs)\n",
      "    490         for hook in self._forward_hooks.values():\n",
      "    491             hook_result = hook(self, input, result)\n",
      "\n",
      "/usr/local/lib/python3.6/site-packages/torch/nn/modules/conv.py in forward(self, input)\n",
      "    318     def forward(self, input):\n",
      "    319         return F.conv2d(input, self.weight, self.bias, self.stride,\n",
      "--> 320                         self.padding, self.dilation, self.groups)\n",
      "    321 \n",
      "    322 \n",
      "\n",
      "RuntimeError: Calculated padded input size per channel: (7 x 7). Kernel size: (10 x 10). Kernel size can't be greater than actual input size at /Users/soumith/b101_2/2019_02_08/wheel_build_dirs/wheel_3.6/pytorch/aten/src/THNN/generic/SpatialConvolutionMM.c:50\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 3/5 with eid 44bf6365-91f1-3d4f-a7ed-03317df96563.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "RuntimeError                              Traceback (most recent call last)\n",
      "<ipython-input-15-d0379ba399d0> in <module>\n",
      "      1 netD = NetD()\n",
      "----> 2 errD_fake = netD(fake_cim, feat_sim)\n",
      "\n",
      "~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\n",
      "    539             result = self._slow_forward(*input, **kwargs)\n",
      "    540         else:\n",
      "--> 541             result = self.forward(*input, **kwargs)\n",
      "    542         for hook in self._forward_hooks.values():\n",
      "    543             hook_result = hook(self, input, result)\n",
      "\n",
      "~/AlacGAN/models/standard.py in forward(self, color, sketch_feat)\n",
      "    169         x = torch.cat([x, sketch_feat], 1)\n",
      "    170         print(x.shape)\n",
      "--> 171         x = self.feed2(x)\n",
      "    172         prnit(x.shape)\n",
      "    173 \n",
      "\n",
      "~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\n",
      "    539             result = self._slow_forward(*input, **kwargs)\n",
      "    540         else:\n",
      "--> 541             result = self.forward(*input, **kwargs)\n",
      "    542         for hook in self._forward_hooks.values():\n",
      "    543             hook_result = hook(self, input, result)\n",
      "\n",
      "~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py in forward(self, input)\n",
      "     90     def forward(self, input):\n",
      "     91         for module in self._modules.values():\n",
      "---> 92             input = module(input)\n",
      "     93         return input\n",
      "     94 \n",
      "\n",
      "~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\n",
      "    539             result = self._slow_forward(*input, **kwargs)\n",
      "    540         else:\n",
      "--> 541             result = self.forward(*input, **kwargs)\n",
      "    542         for hook in self._forward_hooks.values():\n",
      "    543             hook_result = hook(self, input, result)\n",
      "\n",
      "~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py in forward(self, input)\n",
      "    343 \n",
      "    344     def forward(self, input):\n",
      "--> 345         return self.conv2d_forward(input, self.weight)\n",
      "    346 \n",
      "    347 class Conv3d(_ConvNd):\n",
      "\n",
      "~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py in conv2d_forward(self, input, weight)\n",
      "    340                             _pair(0), self.dilation, self.groups)\n",
      "    341         return F.conv2d(input, weight, self.bias, self.stride,\n",
      "--> 342                         self.padding, self.dilation, self.groups)\n",
      "    343 \n",
      "    344     def forward(self, input):\n",
      "\n",
      "RuntimeError: Calculated padded input size per channel: (2 x 2). Kernel size: (4 x 4). Kernel size can't be greater than actual input size\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 4/5 with eid 77336d97-be53-3271-9312-ce3c26ffbc25.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "RuntimeError                              Traceback (most recent call last)\n",
      "/tmp/ipykernel_470512/1569587606.py in <module>\n",
      "     18 \n",
      "     19         # forward + backward + optimize\n",
      "---> 20         outputs = net(inputs)\n",
      "     21         labels = labels.type(torch.long)\n",
      "     22 #         print(inputs, labels, outputs)\n",
      "\n",
      "~/anaconda3/envs/radioml/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n",
      "   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
      "   1050                 or _global_forward_hooks or _global_forward_pre_hooks):\n",
      "-> 1051             return forward_call(*input, **kwargs)\n",
      "   1052         # Do not call functions when jit is used\n",
      "   1053         full_backward_hooks, non_full_backward_hooks = [], []\n",
      "\n",
      "/tmp/ipykernel_470512/3869714053.py in forward(self, x)\n",
      "     53         x = self.conv0(x)\n",
      "     54         x = self.bn0(x)\n",
      "---> 55         x = self.conv1(x)\n",
      "     56         x = self.bn1(x)\n",
      "     57         x = self.conv2(x)\n",
      "\n",
      "~/anaconda3/envs/radioml/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n",
      "   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
      "   1050                 or _global_forward_hooks or _global_forward_pre_hooks):\n",
      "-> 1051             return forward_call(*input, **kwargs)\n",
      "   1052         # Do not call functions when jit is used\n",
      "   1053         full_backward_hooks, non_full_backward_hooks = [], []\n",
      "\n",
      "~/anaconda3/envs/radioml/lib/python3.8/site-packages/torch/nn/modules/conv.py in forward(self, input)\n",
      "    441 \n",
      "    442     def forward(self, input: Tensor) -> Tensor:\n",
      "--> 443         return self._conv_forward(input, self.weight, self.bias)\n",
      "    444 \n",
      "    445 class Conv3d(_ConvNd):\n",
      "\n",
      "~/anaconda3/envs/radioml/lib/python3.8/site-packages/torch/nn/modules/conv.py in _conv_forward(self, input, weight, bias)\n",
      "    437                             weight, bias, self.stride,\n",
      "    438                             _pair(0), self.dilation, self.groups)\n",
      "--> 439         return F.conv2d(input, weight, bias, self.stride,\n",
      "    440                         self.padding, self.dilation, self.groups)\n",
      "    441 \n",
      "\n",
      "RuntimeError: Calculated padded input size per channel: (1026 x 1). Kernel size: (3 x 2). Kernel size can't be greater than actual input size\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 5/5 with eid bfa9c742-c060-3895-9a71-d719647e4ccc.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "RuntimeError                              Traceback (most recent call last)\n",
      "Input In [18], in <cell line: 1>()\n",
      "      2 end = time.time()\n",
      "      3 print(f\"==================== Starting at epoch {epoch} ====================\", flush=True)\n",
      "----> 5 train_loss, train_acc = train_epoch(model, epoch, criterion, optimizer, train_loader, device)\n",
      "      6 print('Training loss: {:.4f} Acc: {:.4f}'.format(train_loss, train_acc), flush=True)\n",
      "      8 rand_img = train_data[randint(0, len(train_data))].to(device)\n",
      "\n",
      "Input In [14], in train_epoch(model, epoch, criterion, optimizer, dataloader, device)\n",
      "     15 optimizer.zero_grad()\n",
      "     17 # Forward Pass\n",
      "---> 18 outputs, mu, logvar = model(inputs)\n",
      "     20 # Compute Loss\n",
      "     21 loss, bce, kld = criterion(outputs, inputs, mu, logvar)\n",
      "\n",
      "File ~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110, in Module._call_impl(self, *input, **kwargs)\n",
      "   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n",
      "   1107 # this function, and just call forward.\n",
      "   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
      "   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n",
      "-> 1110     return forward_call(*input, **kwargs)\n",
      "   1111 # Do not call functions when jit is used\n",
      "   1112 full_backward_hooks, non_full_backward_hooks = [], []\n",
      "\n",
      "Input In [8], in VAE.forward(self, x)\n",
      "     54 def forward(self, x):\n",
      "---> 55     z, mu, logvar = self.encode(x)\n",
      "     56     z = self.decode(z)\n",
      "     57     return z, mu, logvar\n",
      "\n",
      "Input In [8], in VAE.encode(self, x)\n",
      "     44 def encode(self, x):\n",
      "---> 45     h = self.encoder(x)\n",
      "     46     z, mu, logvar = self.bottleneck(h)\n",
      "     47     return z, mu, logvar\n",
      "\n",
      "File ~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110, in Module._call_impl(self, *input, **kwargs)\n",
      "   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n",
      "   1107 # this function, and just call forward.\n",
      "   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
      "   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n",
      "-> 1110     return forward_call(*input, **kwargs)\n",
      "   1111 # Do not call functions when jit is used\n",
      "   1112 full_backward_hooks, non_full_backward_hooks = [], []\n",
      "\n",
      "File ~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:141, in Sequential.forward(self, input)\n",
      "    139 def forward(self, input):\n",
      "    140     for module in self:\n",
      "--> 141         input = module(input)\n",
      "    142     return input\n",
      "\n",
      "File ~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110, in Module._call_impl(self, *input, **kwargs)\n",
      "   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n",
      "   1107 # this function, and just call forward.\n",
      "   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
      "   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n",
      "-> 1110     return forward_call(*input, **kwargs)\n",
      "   1111 # Do not call functions when jit is used\n",
      "   1112 full_backward_hooks, non_full_backward_hooks = [], []\n",
      "\n",
      "File ~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:447, in Conv2d.forward(self, input)\n",
      "    446 def forward(self, input: Tensor) -> Tensor:\n",
      "--> 447     return self._conv_forward(input, self.weight, self.bias)\n",
      "\n",
      "File ~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443, in Conv2d._conv_forward(self, input, weight, bias)\n",
      "    439 if self.padding_mode != 'zeros':\n",
      "    440     return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n",
      "    441                     weight, bias, self.stride,\n",
      "    442                     _pair(0), self.dilation, self.groups)\n",
      "--> 443 return F.conv2d(input, weight, bias, self.stride,\n",
      "    444                 self.padding, self.dilation, self.groups)\n",
      "\n",
      "RuntimeError: Calculated padded input size per channel: (3 x 30). Kernel size: (4 x 4). Kernel size can't be greater than actual input size\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 251 in GitHub error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++3/10: Error examples for Cluster 3329 in GitHub error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/1 with eid 621575ed-6db3-3247-a29f-cda8fc5c42b0.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "com_error                                 Traceback (most recent call last)\n",
      "<ipython-input-32-96722835b0dc> in <module>\n",
      "----> 1 vis_net.Paths.AddPath(origin_lot, destination_lot, [str(node) for node in node_paths[0]])\n",
      "\n",
      "~\\AppData\\Local\\Temp\\gen_py\\3.7\\6331B57B-DCD4-4E00-9BFA-D97922D2B1ECx0x9x0\\IPathContainer.py in AddPath(self, OriginParkingLot, DestinationParkingLot, NodeList)\n",
      "     35                 'Add a path between two parking lots containing the given nodes.'\n",
      "     36 \t\tret = self._oleobj_.InvokeTypes(1610940418, LCID, 1, (9, 0), ((12, 1), (12, 1), (12, 1)),OriginParkingLot\n",
      "---> 37 \t\t\t, DestinationParkingLot, NodeList)\n",
      "     38                 if ret is not None:\n",
      "     39                         ret = Dispatch(ret, 'AddPath', '{99A6D616-DB7C-4E9F-A07A-B7D2A452A9FA}')\n",
      "\n",
      "com_error: (-2147352567, 'Exception occurred.', (0, 'VISSIM.Vissim.900', 'AddPath failed: adjacent node pair in node list with no open edge between nodes.', None, 0, -2147352567), None)\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 3329 in GitHub error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++4/10: Error examples for Cluster 5518 in GitHub error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/5 with eid 01fdd12b-aaed-3b9b-822d-5f9dacd80c49.-----\n",
      "\n",
      "\n",
      "ValueErrorTraceback (most recent call last)\n",
      "<ipython-input-97-35f05ccdaf93> in <module>()\n",
      "      1 stats = pd.DataFrame()\n",
      "----> 2 stats.append( df[df['due_dt'] == 0.10].sample(int(0.10*df[df['due_dt'] == 0.10].size)))\n",
      "      3 \n",
      "\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\pandas\\core\\generic.pyc in sample(self, n, frac, replace, weights, random_state, axis)\n",
      "   2898                              \"provide positive value.\")\n",
      "   2899 \n",
      "-> 2900         locs = rs.choice(axis_length, size=n, replace=replace, p=weights)\n",
      "   2901         return self.take(locs, axis=axis, is_copy=False)\n",
      "   2902 \n",
      "\n",
      "mtrand.pyx in mtrand.RandomState.choice()\n",
      "\n",
      "ValueError: a must be greater than 0\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 2/5 with eid 2615d0ac-ca7d-3305-a83f-1d93e97ed3f1.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "<ipython-input-16-21409002644d> in <module>\n",
      "----> 1 week_ger = cleanup(week_df, pumps, 'city', 0.01)\n",
      "      2 features_ger = create_geojson_features(week_ger)\n",
      "      3 map_germany = make_map(features_ger)\n",
      "      4 map_germany.save(outfile= 'Germany.html')\n",
      "\n",
      "<ipython-input-11-eb6f271098a8> in cleanup(time, pumps, city, fraction)\n",
      "      3     pumps['openingtimes_json'] = pumps[pumps['openingtimes_json'] == '']\n",
      "      4     pumps = pumps[(pumps[\"city\"] == city)]\n",
      "----> 5     pumps_crop = pumps.sample(int(len(pumps)* fraction))\n",
      "      6     pumps_crop.reset_index(inplace = True)\n",
      "      7 \n",
      "\n",
      "~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py in sample(self, n, frac, replace, weights, random_state, axis)\n",
      "   4863                              \"provide positive value.\")\n",
      "   4864 \n",
      "-> 4865         locs = rs.choice(axis_length, size=n, replace=replace, p=weights)\n",
      "   4866         return self.take(locs, axis=axis, is_copy=False)\n",
      "   4867 \n",
      "\n",
      "mtrand.pyx in mtrand.RandomState.choice()\n",
      "\n",
      "ValueError: a must be greater than 0\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 3/5 with eid 375c6226-6fd4-3c47-b978-b3e45f0c498a.-----\n",
      "\n",
      "\n",
      "ValueErrorTraceback (most recent call last)\n",
      "<ipython-input-110-a932879bb757> in <module>()\n",
      "      1 stats = pd.DataFrame()\n",
      "      2 for k in [x / 100 for x in range(5,45,5)]:\n",
      "----> 3     stats.append( df[df['due_dt'] == k].sample(int(k*df[df['due_dt'] == k].size)))\n",
      "\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\pandas\\core\\generic.pyc in sample(self, n, frac, replace, weights, random_state, axis)\n",
      "   2898                              \"provide positive value.\")\n",
      "   2899 \n",
      "-> 2900         locs = rs.choice(axis_length, size=n, replace=replace, p=weights)\n",
      "   2901         return self.take(locs, axis=axis, is_copy=False)\n",
      "   2902 \n",
      "\n",
      "mtrand.pyx in mtrand.RandomState.choice()\n",
      "\n",
      "ValueError: a must be greater than 0\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 4/5 with eid eb9282f4-c939-3234-a5b5-361f9a4ffbdb.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "<ipython-input-57-cb0733844779> in <module>()\n",
      "      1 d = moviegoers.merge(zipcodes, how = 'inner', left_on = 'zip_code', right_on = 'Zipcode')\n",
      "----> 2 d.sample(1)\n",
      "      3 d['State'].value_counts()\n",
      "\n",
      "~/miniconda3/lib/python3.6/site-packages/pandas/core/generic.py in sample(self, n, frac, replace, weights, random_state, axis)\n",
      "   2898                              \"provide positive value.\")\n",
      "   2899 \n",
      "-> 2900         locs = rs.choice(axis_length, size=n, replace=replace, p=weights)\n",
      "   2901         return self.take(locs, axis=axis, is_copy=False)\n",
      "   2902 \n",
      "\n",
      "mtrand.pyx in mtrand.RandomState.choice (numpy/random/mtrand/mtrand.c:17104)()\n",
      "\n",
      "ValueError: a must be greater than 0\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 5/5 with eid 2b50ea49-8e57-39ae-817a-c2fd0b37779e.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "<ipython-input-5-7902bb229da0> in <module>\n",
      "      3 \n",
      "      4 cf = cf[(cf['product'] == 'hills-prescription-diet-cd-multicare') | (cf['product'] == 'fancy-feast-poultry-beef-classic-pate')]\n",
      "----> 5 cf.sample(5)\n",
      "\n",
      "~/anaconda3/envs/insight/lib/python3.6/site-packages/pandas/core/generic.py in sample(self, n, frac, replace, weights, random_state, axis)\n",
      "   4863                              \"provide positive value.\")\n",
      "   4864 \n",
      "-> 4865         locs = rs.choice(axis_length, size=n, replace=replace, p=weights)\n",
      "   4866         return self.take(locs, axis=axis, is_copy=False)\n",
      "   4867 \n",
      "\n",
      "mtrand.pyx in mtrand.RandomState.choice()\n",
      "\n",
      "ValueError: a must be greater than 0\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 5518 in GitHub error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++5/10: Error examples for Cluster 4531 in GitHub error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/3 with eid 1216a36b-37ca-33e3-844a-3266b41625f2.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "OSError                                   Traceback (most recent call last)\n",
      "<ipython-input-15-1cbfbd529636> in <module>()\n",
      "      9 \n",
      "     10 output = 'custom_output.mp4'\n",
      "---> 11 clip1 = VideoFileClip(\"project_video.mp4\")\n",
      "     12 white_clip = clip1.fl_image(pipeline_final)\n",
      "     13 get_ipython().magic('time white_clip.write_videofile(output, audio=False)')\n",
      "\n",
      "C:\\Users\\Kiko-PC\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\moviepy\\video\\io\\VideoFileClip.py in __init__(self, filename, has_mask, audio, audio_buffersize, target_resolution, resize_algorithm, audio_fps, audio_nbytes, verbose, fps_source)\n",
      "     79                                          target_resolution=target_resolution,\n",
      "     80                                          resize_algo=resize_algorithm,\n",
      "---> 81                                          fps_source=fps_source)\n",
      "     82 \n",
      "     83         # Make some of the reader's attributes accessible from the clip\n",
      "\n",
      "C:\\Users\\Kiko-PC\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\moviepy\\video\\io\\ffmpeg_reader.py in __init__(self, filename, print_infos, bufsize, pix_fmt, check_duration, target_resolution, resize_algo, fps_source)\n",
      "     30         self.filename = filename\n",
      "     31         infos = ffmpeg_parse_infos(filename, print_infos, check_duration,\n",
      "---> 32                                    fps_source)\n",
      "     33         self.fps = infos['video_fps']\n",
      "     34         self.size = infos['video_size']\n",
      "\n",
      "C:\\Users\\Kiko-PC\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\moviepy\\video\\io\\ffmpeg_reader.py in ffmpeg_parse_infos(filename, print_infos, check_duration, fps_source)\n",
      "    270         raise IOError((\"MoviePy error: the file %s could not be found !\\n\"\n",
      "    271                       \"Please check that you entered the correct \"\n",
      "--> 272                       \"path.\")%filename)\n",
      "    273 \n",
      "    274     result = dict()\n",
      "\n",
      "OSError: MoviePy error: the file project_video.mp4 could not be found !\n",
      "Please check that you entered the correct path.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 2/3 with eid 985b91b8-2285-3fba-b0f3-609a8c886c55.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "<ipython-input-5-57aa7745a0ed> in <module>\n",
      "     21 \n",
      "     22 # Load saved model\n",
      "---> 23 model = HER.load('her_sac_highway', env=env)\n",
      "     24 \n",
      "     25 obs = env.reset()\n",
      "\n",
      "~\\Anaconda3\\lib\\site-packages\\stable_baselines\\her\\her.py in load(cls, load_path, env, custom_objects, **kwargs)\n",
      "    146     @classmethod\n",
      "    147     def load(cls, load_path, env=None, custom_objects=None, **kwargs):\n",
      "--> 148         data, _ = cls._load_from_file(load_path, custom_objects=custom_objects)\n",
      "    149 \n",
      "    150         if 'policy_kwargs' in kwargs and kwargs['policy_kwargs'] != data['policy_kwargs']:\n",
      "\n",
      "~\\Anaconda3\\lib\\site-packages\\stable_baselines\\common\\base_class.py in _load_from_file(load_path, load_data, custom_objects)\n",
      "    649                     load_path += \".zip\"\n",
      "    650                 else:\n",
      "--> 651                     raise ValueError(\"Error: the file {} could not be found\".format(load_path))\n",
      "    652 \n",
      "    653         # Open the zip archive and load data.\n",
      "\n",
      "ValueError: Error: the file her_sac_highway could not be found\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 3/3 with eid 62455b5d-b86f-3c88-88a2-40282544969c.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "OSError                                   Traceback (most recent call last)\n",
      "<ipython-input-28-9a299c3111be> in <module>()\n",
      "     27     output_video.write_videofile(outputVideo, audio=False)\n",
      "     28 \n",
      "---> 29 processVideo('./videos/project_video.mp4', './video_output/project_video.mp4', threshhold=2)\n",
      "\n",
      "<ipython-input-28-9a299c3111be> in processVideo(inputVideo, outputVideo, frames_to_remember, threshhold)\n",
      "     23         return draw_labeled_bboxes(np.copy(img), labels)\n",
      "     24 \n",
      "---> 25     myclip = VideoFileClip(inputVideo)\n",
      "     26     output_video = myclip.fl_image(pipeline)\n",
      "     27     output_video.write_videofile(outputVideo, audio=False)\n",
      "\n",
      "/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/moviepy/video/io/VideoFileClip.py in __init__(self, filename, has_mask, audio, audio_buffersize, target_resolution, resize_algorithm, audio_fps, audio_nbytes, verbose, fps_source)\n",
      "     79                                          target_resolution=target_resolution,\n",
      "     80                                          resize_algo=resize_algorithm,\n",
      "---> 81                                          fps_source=fps_source)\n",
      "     82 \n",
      "     83         # Make some of the reader's attributes accessible from the clip\n",
      "\n",
      "/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/moviepy/video/io/ffmpeg_reader.py in __init__(self, filename, print_infos, bufsize, pix_fmt, check_duration, target_resolution, resize_algo, fps_source)\n",
      "     30         self.filename = filename\n",
      "     31         infos = ffmpeg_parse_infos(filename, print_infos, check_duration,\n",
      "---> 32                                    fps_source)\n",
      "     33         self.fps = infos['video_fps']\n",
      "     34         self.size = infos['video_size']\n",
      "\n",
      "/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/moviepy/video/io/ffmpeg_reader.py in ffmpeg_parse_infos(filename, print_infos, check_duration, fps_source)\n",
      "    270         raise IOError((\"MoviePy error: the file %s could not be found !\\n\"\n",
      "    271                       \"Please check that you entered the correct \"\n",
      "--> 272                       \"path.\")%filename)\n",
      "    273 \n",
      "    274     result = dict()\n",
      "\n",
      "OSError: MoviePy error: the file ./videos/project_video.mp4 could not be found !\n",
      "Please check that you entered the correct path.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 4531 in GitHub error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++6/10: Error examples for Cluster 1008 in GitHub error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/1 with eid c96a2a87-93ff-3e6b-92e0-fae1312b89ff.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "AssertionError                            Traceback (most recent call last)\n",
      "<ipython-input-34-9621c70b3bb8> in <module>\n",
      "     15 coupled_encoding_dist = nsd.MultivariateCoupledNormal(loc=encoding_loc,\n",
      "     16                                                       scale=encoding_scale,\n",
      "---> 17                                                       kappa=0.1\n",
      "     18                                                       )\n",
      "     19 \n",
      "\n",
      "~/anaconda3/envs/nsc/lib/python3.7/site-packages/nsc/distributions/multivariate_coupled_normal.py in __init__(self, loc, scale, kappa, alpha, validate_args)\n",
      "     37             self._scale = np.diag(self._scale)\n",
      "     38         # Ensure that scale is indeed positive definite\n",
      "---> 39         assert self.is_positive_definite(self._scale), \"scale must be positive definite, but not necessarily symmetric.\"\n",
      "     40 \n",
      "     41     # Credit: https://stackoverflow.com/questions/16266720/find-out-if-matrix-is-positive-definite-with-numpy\n",
      "\n",
      "AssertionError: scale must be positive definite, but not necessarily symmetric.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 1008 in GitHub error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++7/10: Error examples for Cluster 5456 in GitHub error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/1 with eid 10a3376a-36a9-3357-a434-2b2cdd62a7b1.-----\n",
      "\n",
      "--------------------------------------------\n",
      "MySQLInterfaceErrorTraceback (most recent call last)\n",
      "~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/mysql/connector/connection_cext.py in cmd_query(self, query, raw, buffered, raw_as_string)\n",
      "    376                                raw=raw, buffered=buffered,\n",
      "--> 377                                raw_as_string=raw_as_string)\n",
      "    378         except MySQLInterfaceError as exc:\n",
      "\n",
      "MySQLInterfaceError: Unknown column 'users.userID' in 'field list'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "ProgrammingErrorTraceback (most recent call last)\n",
      "~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/sqlalchemy/engine/base.py in _execute_context(self, dialect, constructor, statement, parameters, *args)\n",
      "   1192                         parameters,\n",
      "-> 1193                         context)\n",
      "   1194         except BaseException as e:\n",
      "\n",
      "~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/sqlalchemy/engine/default.py in do_execute(self, cursor, statement, parameters, context)\n",
      "    506     def do_execute(self, cursor, statement, parameters, context=None):\n",
      "--> 507         cursor.execute(statement, parameters)\n",
      "    508 \n",
      "\n",
      "~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/mysql/connector/cursor_cext.py in execute(self, operation, params, multi)\n",
      "    263                                          buffered=self._buffered,\n",
      "--> 264                                          raw_as_string=self._raw_as_string)\n",
      "    265         except MySQLInterfaceError as exc:\n",
      "\n",
      "~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/mysql/connector/connection_cext.py in cmd_query(self, query, raw, buffered, raw_as_string)\n",
      "    379             raise errors.get_mysql_exception(exc.errno, msg=exc.msg,\n",
      "--> 380                                              sqlstate=exc.sqlstate)\n",
      "    381         except AttributeError:\n",
      "\n",
      "ProgrammingError: 1054 (42S22): Unknown column 'users.userID' in 'field list'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "ProgrammingErrorTraceback (most recent call last)\n",
      "<ipython-input-9-bec0e627f117> in <module>()\n",
      "----> 1 cursor.next()\n",
      "\n",
      "~/Dropbox/PainNarrativesLab/TwitterDataAnalysis/DataTools/Cursors.py in next(self)\n",
      "    186     def next( self ):\n",
      "    187         self.callCount += 1\n",
      "--> 188         return next( self.my_iter )\n",
      "    189 \n",
      "    190     def _create_iterator( self ):\n",
      "\n",
      "~/Dropbox/PainNarrativesLab/TwitterDataAnalysis/DataTools/Cursors.py in _create_iterator(self)\n",
      "    196                 q = self.qry.filter( self.pk_attr > self.firstId )\n",
      "    197             rec = None\n",
      "--> 198             for rec in q.order_by( self.pk_attr ).limit( self.limit ):\n",
      "    199                 yield rec\n",
      "    200             if rec is None:\n",
      "\n",
      "~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/sqlalchemy/orm/query.py in __iter__(self)\n",
      "   2923         if self._autoflush and not self._populate_existing:\n",
      "   2924             self.session._autoflush()\n",
      "-> 2925         return self._execute_and_instances(context)\n",
      "   2926 \n",
      "   2927     def __str__(self):\n",
      "\n",
      "~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/sqlalchemy/orm/query.py in _execute_and_instances(self, querycontext)\n",
      "   2946             close_with_result=True)\n",
      "   2947 \n",
      "-> 2948         result = conn.execute(querycontext.statement, self._params)\n",
      "   2949         return loading.instances(querycontext.query, result, querycontext)\n",
      "   2950 \n",
      "\n",
      "~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/sqlalchemy/engine/base.py in execute(self, object, *multiparams, **params)\n",
      "    946             raise exc.ObjectNotExecutableError(object)\n",
      "    947         else:\n",
      "--> 948             return meth(self, multiparams, params)\n",
      "    949 \n",
      "    950     def _execute_function(self, func, multiparams, params):\n",
      "\n",
      "~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/sqlalchemy/sql/elements.py in _execute_on_connection(self, connection, multiparams, params)\n",
      "    267     def _execute_on_connection(self, connection, multiparams, params):\n",
      "    268         if self.supports_execution:\n",
      "--> 269             return connection._execute_clauseelement(self, multiparams, params)\n",
      "    270         else:\n",
      "    271             raise exc.ObjectNotExecutableError(self)\n",
      "\n",
      "~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/sqlalchemy/engine/base.py in _execute_clauseelement(self, elem, multiparams, params)\n",
      "   1058             compiled_sql,\n",
      "   1059             distilled_params,\n",
      "-> 1060             compiled_sql, distilled_params\n",
      "   1061         )\n",
      "   1062         if self._has_events or self.engine._has_events:\n",
      "\n",
      "~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/sqlalchemy/engine/base.py in _execute_context(self, dialect, constructor, statement, parameters, *args)\n",
      "   1198                 parameters,\n",
      "   1199                 cursor,\n",
      "-> 1200                 context)\n",
      "   1201 \n",
      "   1202         if self._has_events or self.engine._has_events:\n",
      "\n",
      "~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/sqlalchemy/engine/base.py in _handle_dbapi_exception(self, e, statement, parameters, cursor, context)\n",
      "   1411                 util.raise_from_cause(\n",
      "   1412                     sqlalchemy_exception,\n",
      "-> 1413                     exc_info\n",
      "   1414                 )\n",
      "   1415             else:\n",
      "\n",
      "~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/sqlalchemy/util/compat.py in raise_from_cause(exception, exc_info)\n",
      "    201     exc_type, exc_value, exc_tb = exc_info\n",
      "    202     cause = exc_value if exc_value is not exception else None\n",
      "--> 203     reraise(type(exception), exception, tb=exc_tb, cause=cause)\n",
      "    204 \n",
      "    205 if py3k:\n",
      "\n",
      "~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/sqlalchemy/util/compat.py in reraise(tp, value, tb, cause)\n",
      "    184             value.__cause__ = cause\n",
      "    185         if value.__traceback__ is not tb:\n",
      "--> 186             raise value.with_traceback(tb)\n",
      "    187         raise value\n",
      "    188 \n",
      "\n",
      "~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/sqlalchemy/engine/base.py in _execute_context(self, dialect, constructor, statement, parameters, *args)\n",
      "   1191                         statement,\n",
      "   1192                         parameters,\n",
      "-> 1193                         context)\n",
      "   1194         except BaseException as e:\n",
      "   1195             self._handle_dbapi_exception(\n",
      "\n",
      "~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/sqlalchemy/engine/default.py in do_execute(self, cursor, statement, parameters, context)\n",
      "    505 \n",
      "    506     def do_execute(self, cursor, statement, parameters, context=None):\n",
      "--> 507         cursor.execute(statement, parameters)\n",
      "    508 \n",
      "    509     def do_execute_no_params(self, cursor, statement, context=None):\n",
      "\n",
      "~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/mysql/connector/cursor_cext.py in execute(self, operation, params, multi)\n",
      "    262             result = self._cnx.cmd_query(stmt, raw=self._raw,\n",
      "    263                                          buffered=self._buffered,\n",
      "--> 264                                          raw_as_string=self._raw_as_string)\n",
      "    265         except MySQLInterfaceError as exc:\n",
      "    266             raise errors.get_mysql_exception(msg=exc.msg, errno=exc.errno,\n",
      "\n",
      "~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/mysql/connector/connection_cext.py in cmd_query(self, query, raw, buffered, raw_as_string)\n",
      "    378         except MySQLInterfaceError as exc:\n",
      "    379             raise errors.get_mysql_exception(exc.errno, msg=exc.msg,\n",
      "--> 380                                              sqlstate=exc.sqlstate)\n",
      "    381         except AttributeError:\n",
      "    382             if self._unix_socket:\n",
      "\n",
      "ProgrammingError: (mysql.connector.errors.ProgrammingError) 1054 (42S22): Unknown column 'users.userID' in 'field list' [SQL: 'SELECT users.`userID` AS `users_userID`, users.screen_name AS users_screen_name, users.id_str AS users_id_str, users.name AS users_name, users.description AS users_description, users.lang AS users_lang, users.utc_offset AS users_utc_offset, users.verified AS users_verified, users.followers_count AS users_followers_count, users.friends_count AS users_friends_count, users.url AS users_url, users.time_zone AS users_time_zone, users.created_at AS users_created_at, users.entities AS users_entities, users.favourites_count AS users_favourites_count, users.statuses_count AS users_statuses_count, users.id AS users_id, users.location AS users_location, users.is_translation_enabled AS users_is_translation_enabled \\nFROM users \\nWHERE users.`userID` > %(userID_1)s ORDER BY users.`userID` \\n LIMIT %(param_1)s'] [parameters: {'userID_1': 0, 'param_1': 4}] (Background on this error at: http://sqlalche.me/e/f405)\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 5456 in GitHub error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++8/10: Error examples for Cluster 7037 in GitHub error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/5 with eid d593deb0-2c9d-3e18-ba54-0b545d9025b9.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "<ipython-input-13-f2b8d3e25236> in <module>\n",
      "----> 1 compare = np.concatenate((img, light_removing(img)),axis=1)\n",
      "      2 cv2.imshow('img',compare)\n",
      "      3 cv2.waitKey(0)\n",
      "      4 cv2.destroyWindow('img')\n",
      "\n",
      "<__array_function__ internals> in concatenate(*args, **kwargs)\n",
      "\n",
      "ValueError: all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 2 dimension(s)\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 2/5 with eid 9bb93824-45aa-3420-bb7d-e7ad8b917f71.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "<ipython-input-65-130f6f3b68c6> in <module>()\n",
      "      1 # Обучаем классификатор\n",
      "      2 dt = DecisionTreeCustom(max_depth=5, min_samples_leaf=5)\n",
      "----> 3 dt.fit(X_train, y_train)\n",
      "\n",
      "<ipython-input-59-0ce5722558d4> in fit(self, X, y)\n",
      "     46 # обучение дерева\n",
      "     47     def fit(self, X, y):\n",
      "---> 48         XY_stacked = np.hstack((X, y))\n",
      "     49         self.tree = self.tree(XY_stacked)\n",
      "     50 \n",
      "\n",
      "<__array_function__ internals> in hstack(*args, **kwargs)\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/shape_base.py in hstack(tup)\n",
      "    343         return _nx.concatenate(arrs, 0)\n",
      "    344     else:\n",
      "--> 345         return _nx.concatenate(arrs, 1)\n",
      "    346 \n",
      "    347 \n",
      "\n",
      "<__array_function__ internals> in concatenate(*args, **kwargs)\n",
      "\n",
      "ValueError: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 3/5 with eid ef16d03e-e219-3ec6-a3e8-46a37b11d501.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "KeyError                                  Traceback (most recent call last)\n",
      "/usr/local/lib/python3.6/dist-packages/bayes_opt/target_space.py in probe(self, params)\n",
      "    190         try:\n",
      "--> 191             target = self._cache[_hashable(x)]\n",
      "    192         except KeyError:\n",
      "\n",
      "KeyError: (11.236203565420874, 105.5292880115007)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "ValueError                                Traceback (most recent call last)\n",
      "<ipython-input-42-77396c3fce50> in <module>()\n",
      "     23 \n",
      "     24   print('Optimization at injector well:', i+1)\n",
      "---> 25   optimizer.maximize(init_points=5, n_iter=20)\n",
      "     26 \n",
      "     27   x_reloc = optimizer.max['params']['x']\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/bayes_opt/bayesian_optimization.py in maximize(self, init_points, n_iter, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\n",
      "    183                 iteration += 1\n",
      "    184 \n",
      "--> 185             self.probe(x_probe, lazy=False)\n",
      "    186 \n",
      "    187             if self._bounds_transformer:\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/bayes_opt/bayesian_optimization.py in probe(self, params, lazy)\n",
      "    114             self._queue.add(params)\n",
      "    115         else:\n",
      "--> 116             self._space.probe(params)\n",
      "    117             self.dispatch(Events.OPTIMIZATION_STEP)\n",
      "    118 \n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/bayes_opt/target_space.py in probe(self, params)\n",
      "    193             params = dict(zip(self._keys, x))\n",
      "    194             target = self.target_func(**params)\n",
      "--> 195             self.register(x, target)\n",
      "    196         return target\n",
      "    197 \n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/bayes_opt/target_space.py in register(self, params, target)\n",
      "    165 \n",
      "    166         self._params = np.concatenate([self._params, x.reshape(1, -1)])\n",
      "--> 167         self._target = np.concatenate([self._target, [target]])\n",
      "    168 \n",
      "    169     def probe(self, params):\n",
      "\n",
      "<__array_function__ internals> in concatenate(*args, **kwargs)\n",
      "\n",
      "ValueError: all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 4/5 with eid a1314d64-32ca-3b26-a4ed-f4c574f01d4d.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "/var/folders/jj/5w7vmf8n4vb1jdw_pp5rln4w0000gn/T/ipykernel_5551/1705301518.py in <module>\n",
      "      4 dim = np.array([[]])\n",
      "      5 for (a, b) in lulu:\n",
      "----> 6     np.append(dim, (b-a), axis=1)\n",
      "      7 dim\n",
      "\n",
      "<__array_function__ internals> in append(*args, **kwargs)\n",
      "\n",
      "~/opt/anaconda3/envs/sdia-python/lib/python3.8/site-packages/numpy/lib/function_base.py in append(arr, values, axis)\n",
      "   4743         values = ravel(values)\n",
      "   4744         axis = arr.ndim-1\n",
      "-> 4745     return concatenate((arr, values), axis=axis)\n",
      "   4746 \n",
      "   4747 \n",
      "\n",
      "<__array_function__ internals> in concatenate(*args, **kwargs)\n",
      "\n",
      "ValueError: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 0 dimension(s)\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 5/5 with eid a010c419-88ee-3748-9883-3ef60629294b.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "<ipython-input-116-4ae1392ff801> in <module>\n",
      "----> 1 np.hstack((a,b))\n",
      "\n",
      "<__array_function__ internals> in hstack(*args, **kwargs)\n",
      "\n",
      "~/anaconda3/lib/python3.6/site-packages/numpy/core/shape_base.py in hstack(tup)\n",
      "    342         return _nx.concatenate(arrs, 0)\n",
      "    343     else:\n",
      "--> 344         return _nx.concatenate(arrs, 1)\n",
      "    345 \n",
      "    346 \n",
      "\n",
      "<__array_function__ internals> in concatenate(*args, **kwargs)\n",
      "\n",
      "ValueError: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 7037 in GitHub error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++9/10: Error examples for Cluster 3224 in GitHub error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/2 with eid b8d16d51-b8af-371e-a3fd-1e1a8c456b1c.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "AssertionError                            Traceback (most recent call last)\n",
      "<ipython-input-64-e400f13f596c> in <module>()\n",
      "      1 from keras.models import model_from_json\n",
      "----> 2 model = model_from_json(open('./tmpdata/convnet_architecture.json').read())\n",
      "\n",
      "/Users/thibaut/anaconda/envs/nilmtk-env/lib/python2.7/site-packages/keras/models.pyc in model_from_json(json_string, custom_objects)\n",
      "     28     and returns a model instance.\n",
      "     29     '''\n",
      "---> 30     import json\n",
      "     31     from keras.utils.layer_utils import layer_from_config\n",
      "     32     config = json.loads(json_string)\n",
      "\n",
      "/Users/thibaut/anaconda/envs/nilmtk-env/lib/python2.7/site-packages/keras/utils/layer_utils.pyc in layer_from_config(config, custom_objects)\n",
      "     33         layer_class = get_from_module(class_name, globals(), 'layer',\n",
      "     34                                       instantiate=False)\n",
      "---> 35     return layer_class.from_config(config['config'])\n",
      "     36 \n",
      "     37 \n",
      "\n",
      "/Users/thibaut/anaconda/envs/nilmtk-env/lib/python2.7/site-packages/keras/engine/topology.pyc in from_config(cls, config, custom_objects)\n",
      "   2165             node_index = self.input_layers_node_indices[i]\n",
      "   2166             node_key = layer.name + '_ib-' + str(node_index)\n",
      "-> 2167             new_node_index = node_conversion_map[node_key]\n",
      "   2168             tensor_index = self.input_layers_tensor_indices[i]\n",
      "   2169             model_inputs.append([layer.name, new_node_index, tensor_index])\n",
      "\n",
      "/Users/thibaut/anaconda/envs/nilmtk-env/lib/python2.7/site-packages/keras/utils/layer_utils.pyc in layer_from_config(config, custom_objects)\n",
      "     33         layer_class = get_from_module(class_name, globals(), 'layer',\n",
      "     34                                       instantiate=False)\n",
      "---> 35     return layer_class.from_config(config['config'])\n",
      "     36 \n",
      "     37 \n",
      "\n",
      "/Users/thibaut/anaconda/envs/nilmtk-env/lib/python2.7/site-packages/keras/engine/topology.pyc in from_config(cls, config)\n",
      "    896                 output of get_config.\n",
      "    897         '''\n",
      "--> 898         return cls(**config)\n",
      "    899 \n",
      "    900     def count_params(self):\n",
      "\n",
      "/Users/thibaut/anaconda/envs/nilmtk-env/lib/python2.7/site-packages/keras/layers/convolutional.pyc in __init__(self, nb_filter, filter_length, init, activation, weights, border_mode, subsample_length, W_regularizer, b_regularizer, activity_regularizer, W_constraint, b_constraint, input_dim, input_length, **kwargs)\n",
      "    113         self.input_dim = input_dim\n",
      "    114         self.input_length = input_length\n",
      "--> 115         if self.input_dim:\n",
      "    116             kwargs['input_shape'] = (self.input_length, self.input_dim)\n",
      "    117         super(Convolution1D, self).__init__(**kwargs)\n",
      "\n",
      "/Users/thibaut/anaconda/envs/nilmtk-env/lib/python2.7/site-packages/keras/engine/topology.pyc in __init__(self, **kwargs)\n",
      "    300                           'create_input_layer'}\n",
      "    301         for kwarg in kwargs.keys():\n",
      "--> 302             assert kwarg in allowed_kwargs, 'Keyword argument not understood: ' + kwarg\n",
      "    303 \n",
      "    304         name = kwargs.get('name')\n",
      "\n",
      "AssertionError: Keyword argument not understood: bias\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 2/2 with eid f9312b0a-8a83-3ef6-b24f-a1b359064737.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "TypeError                                 Traceback (most recent call last)\n",
      "<ipython-input-39-f4259acbc99d> in <module>()\n",
      "      1 ''' test CPPN '''\n",
      "----> 2 g = model_cppn_generator(levels=4, nodes=64, stddev=1)\n",
      "      3 \n",
      "      4 XL = 20\n",
      "      5 coords_test = create_coordinates(nx=NX*XL, ny=NY*XL, scale=6, nbatch=1)\n",
      "\n",
      "<ipython-input-36-32ae28ee8afe> in model_cppn_generator(name, levels, nodes, stddev)\n",
      "     64   z_in = layers.Input(shape=(NZ,))\n",
      "     65   coord_in = layers.Input(shape=(None, 3))\n",
      "---> 66   h = layers.Lambda(repeat_vector, ouptut_shape=(None, NZ))([z_in, coord_in])\n",
      "     67   h = layers.Concatenate() ([h, coord_in])\n",
      "     68   h = layers.Dense(nodes, activation='softplus', **inits) (h)\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py in __init__(self, function, output_shape, mask, arguments, **kwargs)\n",
      "    670   def __init__(self, function, output_shape=None, mask=None, arguments=None,\n",
      "    671                **kwargs):\n",
      "--> 672     super(Lambda, self).__init__(**kwargs)\n",
      "    673     self.function = function\n",
      "    674     self.arguments = arguments if arguments else {}\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpointable/base.py in _method_wrapper(self, *args, **kwargs)\n",
      "    440     self._setattr_tracking = False  # pylint: disable=protected-access\n",
      "    441     try:\n",
      "--> 442       method(self, *args, **kwargs)\n",
      "    443     finally:\n",
      "    444       self._setattr_tracking = previous_value  # pylint: disable=protected-access\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __init__(self, trainable, name, dtype, **kwargs)\n",
      "    120     for kwarg in kwargs:\n",
      "    121       if kwarg not in allowed_kwargs:\n",
      "--> 122         raise TypeError('Keyword argument not understood:', kwarg)\n",
      "    123 \n",
      "    124     # Mutable properties\n",
      "\n",
      "TypeError: ('Keyword argument not understood:', 'ouptut_shape')\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 3224 in GitHub error notebooks end.+++++\n",
      "\n",
      "\u001b[1m+++++10/10: Error examples for Cluster 4853 in GitHub error notebooks.+++++\n",
      "\u001b[0m\n",
      "-----Error example 1/5 with eid 7bbb1500-f94d-30d2-88ac-601b584ac64f.-----\n",
      "\n",
      "\n",
      "TypeErrorTraceback (most recent call last)\n",
      "<ipython-input-80-5d9f4f664923> in <module>()\n",
      "----> 1 np.polyfit(df['avgmean'], df['avgmax'], 1)\n",
      "\n",
      "C:\\Python27\\lib\\site-packages\\numpy\\lib\\polynomial.pyc in polyfit(x, y, deg, rcond, full, w, cov)\n",
      "    539     \"\"\"\n",
      "    540     order = int(deg) + 1\n",
      "--> 541     x = NX.asarray(x) + 0.0\n",
      "    542     y = NX.asarray(y) + 0.0\n",
      "    543 \n",
      "\n",
      "TypeError: cannot concatenate 'str' and 'float' objects\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 2/5 with eid f62896b4-2a2d-3baa-8bd2-207023fa5ce7.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "TypeError                                 Traceback (most recent call last)\n",
      "<ipython-input-4-ab4e4114e876> in <module>()\n",
      "      7 series = read_csv('./datasets/japan/non_seasonal_split/train.csv', parse_dates=[\"datetime\"], index_col=\"datetime\", squeeze=True)\n",
      "      8 # fit model\n",
      "----> 9 model = ARIMA(series, order=(5,1,0))\n",
      "     10 model_fit = model.fit(disp=0)\n",
      "     11 print(model_fit.summary())\n",
      "\n",
      "/home/infernov/tensorflow/local/lib/python2.7/site-packages/statsmodels/tsa/arima_model.pyc in __new__(cls, endog, order, exog, dates, freq, missing)\n",
      "    994         else:\n",
      "    995             mod = super(ARIMA, cls).__new__(cls)\n",
      "--> 996             mod.__init__(endog, order, exog, dates, freq, missing)\n",
      "    997             return mod\n",
      "    998 \n",
      "\n",
      "/home/infernov/tensorflow/local/lib/python2.7/site-packages/statsmodels/tsa/arima_model.pyc in __init__(self, endog, order, exog, dates, freq, missing)\n",
      "   1022             self.exog = self.exog[d:]\n",
      "   1023         if d == 1:\n",
      "-> 1024             self.data.ynames = 'D.' + self.endog_names\n",
      "   1025         else:\n",
      "   1026             self.data.ynames = 'D{0:d}.'.format(d) + self.endog_names\n",
      "\n",
      "TypeError: cannot concatenate 'str' and 'list' objects\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 3/5 with eid 61701ed6-8b06-3c45-bb9d-cf6f34764bf1.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "TypeError                                 Traceback (most recent call last)\n",
      "<ipython-input-6-c9796d1dc9d8> in <module>()\n",
      "      2 \n",
      "      3 # try and add 1 to the string!\n",
      "----> 4 anumber = astring + 1\n",
      "\n",
      "TypeError: cannot concatenate 'str' and 'int' objects\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 4/5 with eid b3f77cab-a8e0-32fa-85ca-60d989094d21.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "TypeError                                 Traceback (most recent call last)\n",
      "<ipython-input-60-107e7297d09f> in <module>()\n",
      "----> 1 say_hello_to(10)\n",
      "\n",
      "<ipython-input-58-6c2b731d9caf> in say_hello_to(name)\n",
      "      2 def say_hello_to(name):\n",
      "      3     \"\"\"Return a greeting to `name`\"\"\"\n",
      "----> 4     return 'Hello ' + name\n",
      "\n",
      "TypeError: cannot concatenate 'str' and 'int' objects\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "-----Error example 5/5 with eid ac285d14-d057-365c-84f1-fbee51197dca.-----\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "TypeError                                 Traceback (most recent call last)\n",
      "<ipython-input-2-8d3ec455c17a> in <module>()\n",
      "      8 \n",
      "      9 KI = pd.read_csv(Kipath)\n",
      "---> 10 print(stats.norm.fit(KI))\n",
      "     11 plt.hist(np.array(KI),100)\n",
      "     12 plt.show()\n",
      "\n",
      "c:\\python27\\lib\\site-packages\\scipy\\stats\\_continuous_distns.pyc in fit(self, data, **kwds)\n",
      "    188 \n",
      "    189         if floc is None:\n",
      "--> 190             loc = data.mean()\n",
      "    191         else:\n",
      "    192             loc = floc\n",
      "\n",
      "c:\\python27\\lib\\site-packages\\numpy\\core\\_methods.pyc in _mean(a, axis, dtype, out, keepdims)\n",
      "     68             is_float16_result = True\n",
      "     69 \n",
      "---> 70     ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "     71     if isinstance(ret, mu.ndarray):\n",
      "     72         ret = um.true_divide(\n",
      "\n",
      "TypeError: cannot concatenate 'str' and 'float' objects\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "+++++Error examples for Cluster 4853 in GitHub error notebooks end.+++++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GitHub\n",
    "for j in range(len(sample_g_small_clusters)):\n",
    "    sgc = sample_g_small_clusters[j]\n",
    "    print('\\033[1m'+\"+++++{}/{}: Error examples for Cluster {} in GitHub error notebooks.+++++\\n\".format(j+1, 10, sgc)+'\\033[0m')\n",
    "    df_target = df_err_grouped_g_small[df_err_grouped_g_small.pregroup_cluster==sgc]\n",
    "    n = min(5, len(df_target))\n",
    "    tmp = df_target.sample(n=n, random_state=30)\n",
    "    for i in range(len(tmp)):\n",
    "        print(\"-----Error example {}/{} with eid {}.-----\\n\".format(i+1, n, tmp.iloc[i].eid))\n",
    "        util.print_traceback(tmp.iloc[i].traceback)\n",
    "        print(\"---------------------------------------------------------------------------\\n\")\n",
    "    print(\"+++++Error examples for Cluster {} in GitHub error notebooks end.+++++\\n\".format(sgc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72061266",
   "metadata": {},
   "source": [
    "Potentially interesting clusters:\n",
    "\n",
    "Kaggle:\n",
    "\n",
    "        ○ Cluster 7348 (size 4): \"ValueError: Cannot assign value to variable ' embedding_2/embeddings:0': Shape mismatch.The variable shape (514157, 300), and the assigned value shape (514157,) are incompatible.\" shape mismatch during model construction\n",
    "        ○ Cluster 7274 (size 2): \"InternalError: RET_CHECK failure (tensorflow/core/tpu/graph_rewrite/distributed_tpu_rewrite_pass.cc:2008) arg_shape.handle_type != DT_INVALID  input edge: [id=881 sequential_keras_layer_2728:0 -> cluster_train_function:63] [Op:__inference_train_function_3219]\" due to (probably) improper training dataset size\n",
    "        ○ Cluster 7345 (size 4): \"RuntimeError: stack expects each tensor to be equal size, but got [19] at entry 0 and [21] at entry 1\"\n",
    "\t\n",
    "GitHub:\n",
    "\n",
    "        ○ Cluster 251 (size 13): \"RuntimeError: Calculated padded input size per channel: (1 x 1). Kernel size: (3 x 3). Kernel size can't be greater than actual input size\" (torch)\n",
    "        ○ Cluster 7037 (size 25): \"ValueError: all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 2 dimension(s)\" (numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6fd456da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yirwa29\\AppData\\Local\\anaconda3\\envs\\chatgpt\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# check if any of them are included in the samples\n",
    "df_mlerr_labels = pd.read_excel(config.path_default.joinpath('Manual_labeing/df_err_processed_pregroup_sampled4_resampled7.xlsx'),\n",
    "                                sheet_name = \"Del-All\",\n",
    "                                keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "06f9000b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7348 cluster size\n",
      "    Kaggle: 4\n",
      "    GitHub: 1\n",
      "    If being sampled: False\n",
      "7274 cluster size\n",
      "    Kaggle: 2\n",
      "    GitHub: 0\n",
      "    If being sampled: False\n",
      "7345 cluster size\n",
      "    Kaggle: 4\n",
      "    GitHub: 12\n",
      "    If being sampled: False\n",
      "251 cluster size\n",
      "    Kaggle: 0\n",
      "    GitHub: 13\n",
      "    If being sampled: False\n",
      "7037 cluster size\n",
      "    Kaggle: 1\n",
      "    GitHub: 25\n",
      "    If being sampled: False\n"
     ]
    }
   ],
   "source": [
    "for i in [7348,7274,7345,251,7037]:\n",
    "    print(\"{} cluster size\".format(i))\n",
    "    print(\"    Kaggle: {}\".format(sum(df_err_grouped_k.pregroup_cluster==i)))\n",
    "    print(\"    GitHub: {}\".format(sum(df_err_grouped_g.pregroup_cluster==i)))\n",
    "    print(\"    If being sampled: {}\".format(sum(df_mlerr_labels.pregroup_cluster==i)>0))\n",
    "#     print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
