{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47f41d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(cluster_util)\n",
    "# import cluster_util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4566c1f6",
   "metadata": {},
   "source": [
    "# Notebook error analysis \n",
    "\n",
    "## Refining error types - Evaluate cluster results\n",
    "\n",
    "\n",
    "clusters and LDA \n",
    "\n",
    "LDA, Latent Dirichlet Allocation\n",
    "\n",
    "    LDA works off the premise that documents with the same topic will have a lot of words in common.\n",
    "    LDA is a bag of words model meaning that it only considers individual tokens and not their relationships in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2bdfbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yirwa29\\AppData\\Local\\anaconda3\\envs\\chatgpt\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path_default=WindowsPath('C:/Users/yirwa29/Downloads/Dataset-Nb')\n"
     ]
    }
   ],
   "source": [
    "# original datasets\n",
    "\n",
    "# import pandas as pd \n",
    "# import cluster_util, config\n",
    "\n",
    "# df_mlerr_g_mlbugs_unique = pd.read_excel(config.path_github_error_analysis.joinpath(\"df_mlerr_g_mlbugs_filtered_dedup.xlsx\"))\n",
    "# df_mlerr_k_mlbugs_unique = pd.read_excel(config.path_kaggle_error_analysis.joinpath(\"df_mlerr_k_mlbugs_filtered_dedup.xlsx\"))\n",
    "\n",
    "# df_mlerr_mlbugs_unique = pd.concat([df_mlerr_g_mlbugs_unique, df_mlerr_k_mlbugs_unique], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b896ba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustered results\n",
    "\n",
    "import pandas as pd \n",
    "import cluster_util, config\n",
    "\n",
    "df_mlerr_clustered = pd.read_excel(config.path_default.joinpath(\"df_mlerr_mlbugs_filtered_dedup_clustered.xlsx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89233f35",
   "metadata": {},
   "source": [
    "hierarchical clustering tree -> labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37ce3d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(config.path_default.joinpath(\"aggroot_transformer.pkl\"), \"rb\") as f:\n",
    "    root_sentence = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c53d5589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(config.path_default.joinpath(\"aggroot_wordemb.pkl\"), \"rb\") as f:\n",
    "    root_word = pickle.load(f)\n",
    "# root_word = cluster_util.aggcluster_to_tree(model_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "20789b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for labeling data with predefined num of clusters\n",
    "from bigtree import levelorder_iter,levelordergroup_iter\n",
    "import numpy as np\n",
    "\n",
    "def hc_tree_labeling(root_node, n_clusters):\n",
    "    res_labels = np.zeros(len(list(root_node.leaves)))\n",
    "    for group in levelordergroup_iter(root_node):\n",
    "    #     print(len(list(group)))\n",
    "        if len(list(group)) >= n_clusters:\n",
    "            id_clusters = 1\n",
    "            for node_sib in group:\n",
    "                if id_clusters < n_clusters:\n",
    "                    for leaf in node_sib.leaves:\n",
    "                        res_labels[leaf.node_name-1] = id_clusters\n",
    "                    id_clusters += 1\n",
    "                else:\n",
    "                    for leaf in node_sib.leaves:\n",
    "                        res_labels[leaf.node_name-1] = id_clusters\n",
    "            break\n",
    "    return res_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8a20a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mlerr_clustered.loc[:,\"cluster_hc_transformers\"] = hc_tree_labeling(root_sentence, n_clusters=60)\n",
    "df_mlerr_clustered.loc[:,\"cluster_hc_wordemb\"] = hc_tree_labeling(root_word, n_clusters=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e27cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # explore hc tree based on node sizes\n",
    "\n",
    "# from bigtree import levelorder_iter\n",
    "\n",
    "# res_nodes = []\n",
    "\n",
    "# for node in levelorder_iter(root):\n",
    "#     if len(list(node.leaves)) in range(20,60):\n",
    "#         res_nodes.append(node)\n",
    "\n",
    "        \n",
    "# import random\n",
    "# #take the last one for example\n",
    "# res_names = [leave_node.node_name-1 for leave_node in res_nodes[random.randint(0,len(res_nodes))].leaves]\n",
    "# # see what are they\n",
    "# for i in df_mlerr_mlbugs_unique.iloc[res_names].evalue:\n",
    "#     print(\"==>\",i)\n",
    "    \n",
    "# df_mlerr_mlbugs_unique.iloc[res_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5592d017",
   "metadata": {},
   "source": [
    "kmeans, dbscan, optics with document-level(sentence transformer) and word-level(finetuned subword-wordemb) embeddings\n",
    "\n",
    "LDA60+75\n",
    "\n",
    "H-clustering60+75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eb8915c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fname', 'eid', 'ename', 'evalue', 'traceback', 'ename_mapped',\n",
       "       'imports', 'lib_alias', 'exp_mllib', 'exp_mllib_extracted',\n",
       "       'python_version', 'evalue_tokenized', 'hash_evalue_tokenized',\n",
       "       'cluster_km_transformers', 'cluster_dbscan_transformers',\n",
       "       'cluster_optics_transformers', 'cluster_km_glove',\n",
       "       'cluster_dbscan_glove', 'cluster_optics_glove', 'cluster_lda60',\n",
       "       'cluster_lda75', 'cluster_km_wordemb', 'cluster_dbscan_wordemb',\n",
       "       'cluster_optics_wordemb', 'cluster_hc_wordemb',\n",
       "       'cluster_hc_transformers'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mlerr_clustered.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9a691d",
   "metadata": {},
   "source": [
    "evaluate if the clusterings correlate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5cc612e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20138148164723754"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "\n",
    "adjusted_rand_score(df_mlerr_clustered.cluster_km_transformers, df_mlerr_clustered.cluster_km_wordemb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "da7a51d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023854138369017626"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_rand_score(df_mlerr_clustered.cluster_dbscan_transformers, df_mlerr_clustered.cluster_dbscan_wordemb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3317aeaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30616859771333393"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_rand_score(df_mlerr_clustered.cluster_optics_transformers, df_mlerr_clustered.cluster_optics_wordemb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05aa5893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04210359137590884"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_rand_score(df_mlerr_clustered.cluster_hc_transformers, df_mlerr_clustered.cluster_hc_wordemb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6d468db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0604557538916081"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_rand_score(df_mlerr_clustered.cluster_hc_transformers, df_mlerr_mlbugs_unique.cluster_lda60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3d53bef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41332208941227294"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_rand_score(df_mlerr_clustered.cluster_dbscan_wordemb, df_mlerr_mlbugs_unique.cluster_lda60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4098832a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21927502871512328"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_rand_score(df_mlerr_clustered.cluster_km_wordemb, df_mlerr_mlbugs_unique.cluster_lda60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448e050e",
   "metadata": {},
   "source": [
    "We implemented and tried quite a lot of alternative embeddings and clustering algorithms. We focus on:\n",
    "\n",
    "    embeddings:\n",
    "        document level embedding -> sentence transformer\n",
    "        word level embedding -> finetuned word embedding with fasttext (support subword)\n",
    "    clustering:\n",
    "        kmeans\n",
    "        (maybe shold exclude, too many noise samples)dbscan/optics\n",
    "        hc\n",
    "        lda\n",
    "Evaluate clusters:\n",
    "\n",
    "    take a random subset (say 200 errors), take each pair, manually label -> if they should be in the same cluster or not\n",
    "    check clustering results based on the labeling\n",
    "    then choose a reasonable embedding+clustering technique\n",
    "ML bug categorization:\n",
    "\n",
    "    with the chosen embedding+clustering results, summarize each clusters and assign them to existing or new categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "310c063f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105378903"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random sample pairs\n",
    "\n",
    "import random\n",
    "\n",
    "# all pairs\n",
    "id_pairs = []\n",
    "for i in df_mlerr_clustered.index:\n",
    "    for j in range(i+1, len(df_mlerr_clustered.index)):\n",
    "        id_pairs.append((i, j))\n",
    "print(len(id_pairs))\n",
    "\n",
    "# random sample\n",
    "n = 200\n",
    "sampled_pair_ids = random.sample(id_pairs, n)\n",
    "\n",
    "# get dataframes\n",
    "sampled_pair_id_left = [p[0] for p in sampled_pair_ids]\n",
    "sampled_pair_id_right = [p[1] for p in sampled_pair_ids]\n",
    "\n",
    "df_mlerr_sample_left = df_mlerr_clustered.loc[sampled_pair_id_left,:]\n",
    "df_mlerr_sample_right = df_mlerr_clustered.loc[sampled_pair_id_right,:]\n",
    "\n",
    "# save the sampled dataframes\n",
    "df_mlerr_sample_left.to_excel(config.path_default.joinpath(\"df_mlerr_clustered_sample_left.xlsx\"),\n",
    "                              index=False, engine='xlsxwriter')\n",
    "df_mlerr_sample_right.to_excel(config.path_default.joinpath(\"df_mlerr_clustered_sample_right.xlsx\"),\n",
    "                               index=False, engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8b43084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather results from clustering algorithms\n",
    "\n",
    "check_columns = ['cluster_km_transformers', 'cluster_dbscan_transformers',\n",
    "                 'cluster_optics_transformers', 'cluster_km_glove',\n",
    "                 'cluster_dbscan_glove', 'cluster_optics_glove', 'cluster_lda60',\n",
    "                 'cluster_lda75', 'cluster_km_wordemb', 'cluster_dbscan_wordemb',\n",
    "                 'cluster_optics_wordemb', 'cluster_hc_wordemb','cluster_hc_transformers']\n",
    "res = {} # same:1, not same:0\n",
    "for cn in check_columns:\n",
    "    res[cn] = np.full(n, -1)\n",
    "    for i in range(n):\n",
    "        res[cn][i] = df_mlerr_sample_left.iloc[i][cn]==df_mlerr_sample_right.iloc[i][cn]\n",
    "        \n",
    "df_res_mlerr_sample = pd.DataFrame.from_dict(res)\n",
    "df_res_mlerr_sample.loc[:, \"fname_left\"] = df_mlerr_sample_left.fname.reset_index(drop=True)\n",
    "df_res_mlerr_sample.loc[:, \"eid_left\"] = df_mlerr_sample_left.eid.reset_index(drop=True)\n",
    "df_res_mlerr_sample.loc[:, \"fname_right\"] = df_mlerr_sample_right.fname.reset_index(drop=True)\n",
    "df_res_mlerr_sample.loc[:, \"eid_right\"] = df_mlerr_sample_right.eid.reset_index(drop=True)\n",
    "df_res_mlerr_sample.to_excel(config.path_default.joinpath(\"df_mlerr_clustered_sample_res_algrithms.xlsx\"),\n",
    "                             index=False, engine='xlsxwriter')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "630cc48a",
   "metadata": {},
   "source": [
    "### Compare with manual results\n",
    "\n",
    "Potential clusters:\n",
    "\n",
    "    Tensor shape mismatch\n",
    "    Wrong query for data\n",
    "    Wrongly use APIs (no attribute, wrong inputs, wrong operations-bool+[])\n",
    "    Logic error (undefined)\n",
    "    External errors (no file found, )\n",
    "    \n",
    "manual labeling [file](https://liuonline-my.sharepoint.com/:x:/r/personal/yirwa29_liu_se/_layouts/15/Doc.aspx?sourcedoc=%7BA108BDF6-8E26-4B44-BF4F-68E4FBF689C5%7D&file=df_mlerr_clustered_sample_res_manual.xlsx&action=default&mobileredirect=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939b6611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled clustered results\n",
    "\n",
    "import pandas as pd \n",
    "import cluster_util, config\n",
    "\n",
    "df_res_mlerr_sample = pd.read_excel(config.path_default.joinpath(\"df_mlerr_clustered_sample_res_algrithms.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504c259d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
