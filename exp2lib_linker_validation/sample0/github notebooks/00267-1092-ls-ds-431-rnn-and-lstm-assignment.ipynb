{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
                "<br></br>\n",
                "<br></br>\n",
                "\n",
                "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
                "\n",
                "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
                "\n",
                "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
                "\n",
                "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
                "\n",
                "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
                "\n",
                "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
                "\n",
                "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
                "\n",
                "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO - Words, words, mere words, no matter from the heart."
            ]
        },
        {
            "cell_type": "markdown",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "### Important Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "from __future__ import print_function\n",
                "from tensorflow.keras.preprocessing import sequence\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Dense, Embedding\n",
                "from tensorflow.keras.layers import LSTM\n",
                "from tensorflow.keras.datasets import imdb\n",
                "from tensorflow.keras.callbacks import LambdaCallback\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Dense, LSTM\n",
                "from tensorflow.keras.optimizers import RMSprop\n",
                "\n",
                "import requests\n",
                "import numpy as np\n",
                "import random\n",
                "import sys\n",
                "import os\n",
                "\n",
                "max_features = 20000\n",
                "maxlen = 80\n",
                "batch_size = 32"
            ]
        },
        {
            "cell_type": "markdown",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Creating streamlined function for text processing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setting up function for streamlined processing\n",
                "def process_text(text_file):\n",
                "    \"\"\"Opens and processes text to be ready for model fitting\"\"\"\n",
                "\n",
                "    # Import and encoding\n",
                "    response = requests.get(text_file)\n",
                "    train_text = response.text\n",
                "    response.encoding = 'utf-8'\n",
                "    # re.sub('[^A-Za-z0-9]+', '', train_text)\n",
                "    len(train_text) # For debugging\n",
                "    print(train_text[:200]) # For debugging\n",
                "\n",
                "    # Encoding data as chars\n",
                "    chars = list(set(train_text))\n",
                "    char_int = {c:i for i, c in enumerate(chars)}\n",
                "    int_char = {i:c for i, c in enumerate(chars)}\n",
                "\n",
                "    # Create the Sequence Data\n",
                "    maxlen = 40\n",
                "    step = 5\n",
                "    encoded = [char_int[c] for c in train_text]\n",
                "    sequences = [] # Each element is 40 characters long\n",
                "    next_chars = [] # One element for each sequence\n",
                "    for i in range(0, len(encoded) - maxlen, step):\n",
                "        sequences.append(encoded[i : i + maxlen])\n",
                "        next_chars.append(encoded[i + maxlen])\n",
                "    # print('sequences:', len(sequences)) # For debugging\n",
                "\n",
                "    # Specify x & y\n",
                "    x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
                "    y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
                "    for i, sequence in enumerate(sequences):\n",
                "        for t, char in enumerate(sequence):\n",
                "            x[i, t, char] = 1\n",
                "        y[i, next_chars[i]] = 1\n",
                "    # print(x.shape, y.shape) # For debugging\n",
                "    return x, y, train_text\n",
                "\n",
                "def on_epoch_end(epoch, _):\n",
                "    \"\"\" Prints generated text one char at a time for each epoch on trained data\"\"\"\n",
                "    \n",
                "    print()\n",
                "    print('----- Generating text after Epoch: %d' % epoch)\n",
                "\n",
                "    start_index = random.randint(0, len(train_text) - maxlen - 1)\n",
                "\n",
                "    for diversity in [0.2, 0.5, 0.7, 1.0, 1.2]:\n",
                "        print('----- diversity:', diversity)\n",
                "\n",
                "        generated = ''\n",
                "        sentence = train_text[start_index: start_index + maxlen]\n",
                "        generated += sentence\n",
                "\n",
                "        print('----- Generating with seed: \"' + sentence + '\"')\n",
                "        sys.stdout.write(generated)\n",
                "\n",
                "        for i in range(400):\n",
                "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
                "            for t, char in enumerate(sentence):\n",
                "                x_pred[0, t, char_int[char]] = 1\n",
                "\n",
                "            preds = model.predict(x_pred, verbose=0)[0]\n",
                "            next_index = sample(preds, diversity)\n",
                "            next_char = int_char[next_index]\n",
                "            # print(\"next_char: \" + next_char)\n",
                "\n",
                "            sentence = sentence[1:] + next_char\n",
                "            # print(\"sentence[1:]: \" + sentence[1:])\n",
                "\n",
                "            sys.stdout.write(next_char)\n",
                "            # sys.stdout.flush() # This is bugging, not adding any spaces\n",
                "        print()\n",
                "\n",
                "def fit_model(x, y, num=5):\n",
                "    \"\"\"Instantiates and fits the model\"\"\"\n",
                "    model = Sequential()\n",
                "    model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
                "    model.add(Dense(len(chars), activation='softmax'))\n",
                "    model.compile(loss='categorical_crossentropy', optimizer='adam')  \n",
                "    print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
                "    model.fit(x, y, batch_size=128, epochs=num, callbacks=[print_callback])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "plays_sonnets = 'https://raw.githubusercontent.com/lechemrc/DS-Unit-4-Sprint-3-Deep-Learning/master/module1-rnn-and-lstm/shakespeare_full.txt'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "﻿THE SONNETS\n\n                    1\n\nFrom fairest creatures we desire increase,\nThat thereby beauty’s rose might never die,\nBut as the riper should by time decease,\nHis tender heir might bear his memo\n"
                },
                {
                    "ename": "ValueError",
                    "evalue": "too many values to unpack (expected 2)",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
                        "\u001b[1;32m<ipython-input-8-5ba72b262d5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplays_sonnets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# fit_model(x, y) # This is not working, really not sure why...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
                    ]
                }
            ],
            "source": [
                "x, y = process_text(plays_sonnets)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# fit_model(x, y) # This is not working, really not sure why..."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 143,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\n----- Generating text after Epoch: 5\n----- diversity: 0.2\n----- Generating with seed: \"           1192\n  Holding their course \"\n           1192\n  Holding their course"
                },
                {
                    "ename": "KeyError",
                    "evalue": "'\\r'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
                        "\u001b[1;32m<ipython-input-143-7076eca98568>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[1;32m<ipython-input-140-e383ade11faf>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(epoch, _)\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mx_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m                 \u001b[0mx_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_int\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;31mKeyError\u001b[0m: '\\r'"
                    ]
                }
            ],
            "source": [
                "on_epoch_end(5, _)"
            ]
        },
        {
            "cell_type": "markdown",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generating text from all works"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "5549280\n﻿THE SONNETS\n\n                    1\n\nFrom fairest creatures we desire increase,\nThat thereby beauty’s rose might never die,\nBut as the riper should by time decease,\nHis tender heir might bear his memo\nsequences: 1109848\n"
                },
                {
                    "data": {
                        "text/plain": "((1109848, 40, 102), (1109848, 102))"
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Import and encoding\n",
                "response = requests.get(plays_sonnets)\n",
                "train_text = response.text\n",
                "response.encoding = 'utf-8'\n",
                "print(len(train_text)) # For debugging\n",
                "print(train_text[:200]) # For debugging\n",
                "\n",
                "# Encoding data as chars\n",
                "chars = list(set(train_text))\n",
                "char_int = {c:i for i, c in enumerate(chars)}\n",
                "int_char = {i:c for i, c in enumerate(chars)}\n",
                "\n",
                "# Create the Sequence Data\n",
                "maxlen = 40\n",
                "step = 5\n",
                "encoded = [char_int[c] for c in train_text]\n",
                "sequences = [] # Each element is 40 characters long\n",
                "next_chars = [] # One element for each sequence\n",
                "for i in range(0, len(encoded) - maxlen, step):\n",
                "    sequences.append(encoded[i : i + maxlen])\n",
                "    next_chars.append(encoded[i + maxlen])\n",
                "print('sequences:', len(sequences)) # For debugging\n",
                "\n",
                "# Specify x & y\n",
                "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
                "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
                "for i, sequence in enumerate(sequences):\n",
                "    for t, char in enumerate(sequence):\n",
                "        x[i, t, char] = 1\n",
                "    y[i, next_chars[i]] = 1\n",
                "\n",
                "x.shape, y.shape "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "def on_epoch_end(epoch, _):\n",
                "    \"\"\" Prints generated text one char at a time for each epoch on trained data\"\"\"\n",
                "    \n",
                "    print()\n",
                "    print('----- Generating text after Epoch: %d' % epoch)\n",
                "\n",
                "    start_index = random.randint(0, len(train_text) - maxlen - 1)\n",
                "\n",
                "    for diversity in [0.2, 0.5, 0.7, 1.0, 1.2]:\n",
                "        print('----- diversity:', diversity)\n",
                "\n",
                "        generated = ''\n",
                "        sentence = train_text[start_index: start_index + maxlen]\n",
                "        generated += sentence\n",
                "\n",
                "        print('----- Generating with seed: \"' + sentence + '\"')\n",
                "        sys.stdout.write(generated)\n",
                "\n",
                "        for i in range(400):\n",
                "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
                "            for t, char in enumerate(sentence):\n",
                "                x_pred[0, t, char_int[char]] = 1\n",
                "\n",
                "            preds = model.predict(x_pred, verbose=0)[0]\n",
                "            next_index = sample(preds, diversity)\n",
                "            next_char = int_char[next_index]\n",
                "            # print(\"next_char: \" + next_char)\n",
                "\n",
                "            sentence = sentence[1:] + next_char\n",
                "            # print(\"sentence[1:]: \" + sentence[1:])\n",
                "\n",
                "            sys.stdout.write(next_char)\n",
                "            # sys.stdout.flush() # This is bugging, not adding any spaces\n",
                "        print()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train on 1109848 samples\nEpoch 1/10\n 915456/1109848 [=======================>......] - ETA: 1:12 - loss: 2.2946"
                }
            ],
            "source": [
                "model = Sequential()\n",
                "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
                "model.add(Dense(len(chars), activation='softmax'))\n",
                "model.compile(loss='categorical_crossentropy', optimizer='adam')  \n",
                "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
                "model.fit(x, y, batch_size=128, epochs=10, callbacks=[print_callback])"
            ]
        },
        {
            "cell_type": "markdown",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Generating text from Sonnets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 129,
            "metadata": {},
            "outputs": [],
            "source": [
                "sonnets = 'https://raw.githubusercontent.com/lechemrc/DS-Unit-4-Sprint-3-Deep-Learning/master/module1-rnn-and-lstm/shakespeare_sonnets.txt'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 130,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "5777367\n﻿THE SONNETS\n\n                    1\n\nFrom fairest creatures we desire increase,\nThat thereby beauty’s rose might never die,\nBut as the riper should by time decease,\nHis tender heir might bear his memo\nsequences: 19663\n"
                },
                {
                    "data": {
                        "text/plain": "((19663, 40, 73), (19663, 73))"
                    },
                    "execution_count": 130,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Import and encoding\n",
                "response = requests.get(sonnets)\n",
                "train_text = response.text\n",
                "response.encoding = 'utf-8'\n",
                "print(len(text)) # For debugging\n",
                "print(train_text[:200]) # For debugging\n",
                "\n",
                "# Encoding data as chars\n",
                "chars = list(set(train_text))\n",
                "char_int = {c:i for i, c in enumerate(chars)}\n",
                "int_char = {i:c for i, c in enumerate(chars)}\n",
                "\n",
                "# Create the Sequence Data\n",
                "maxlen = 40\n",
                "step = 5\n",
                "encoded = [char_int[c] for c in train_text]\n",
                "sequences = [] # Each element is 40 characters long\n",
                "next_chars = [] # One element for each sequence\n",
                "for i in range(0, len(encoded) - maxlen, step):\n",
                "    sequences.append(encoded[i : i + maxlen])\n",
                "    next_chars.append(encoded[i + maxlen])\n",
                "print('sequences:', len(sequences)) # For debugging\n",
                "\n",
                "# Specify x & y\n",
                "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
                "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
                "for i, sequence in enumerate(sequences):\n",
                "    for t, char in enumerate(sequence):\n",
                "        x[i, t, char] = 1\n",
                "    y[i, next_chars[i]] = 1\n",
                "\n",
                "x.shape, y.shape "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 132,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train on 19663 samples\n19584/19663 [============================>.] - ETA: 0s - loss: 3.1593\n----- Generating text after Epoch: 0\n----- diversity: 0.2\n----- Generating with seed: \"dulness,\nAnd give it way. I know thou c\"\ndulness,\nAnd give it way. I know thou c"
                },
                {
                    "ename": "KeyError",
                    "evalue": "'\\r'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
                        "\u001b[1;32m<ipython-input-132-1007581d624b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint_callback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLambdaCallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprint_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[1;32m~\\.virtualenvs\\DS-Unit-4-Sprint-2-Neural-Networks-AITeJwQM\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
                        "\u001b[1;32m~\\.virtualenvs\\DS-Unit-4-Sprint-2-Neural-Networks-AITeJwQM\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\Lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\.virtualenvs\\DS-Unit-4-Sprint-2-Neural-Networks-AITeJwQM\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\.virtualenvs\\DS-Unit-4-Sprint-2-Neural-Networks-AITeJwQM\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m<ipython-input-117-a6aa07f1f736>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(epoch, _)\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mx_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m                 \u001b[0mx_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_int\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;31mKeyError\u001b[0m: '\\r'"
                    ]
                }
            ],
            "source": [
                "model = Sequential()\n",
                "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
                "model.add(Dense(len(chars), activation='softmax'))\n",
                "model.compile(loss='categorical_crossentropy', optimizer='adam')  \n",
                "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
                "model.fit(x, y, batch_size=128, epochs=1, callbacks=[print_callback])"
            ]
        },
        {
            "cell_type": "markdown",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Generating text from plays"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plays = 'https://raw.githubusercontent.com/lechemrc/DS-Unit-4-Sprint-3-Deep-Learning/master/module1-rnn-and-lstm/shakespeare_plays.txt'\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "uT3UV3gap9H6"
            },
            "source": [
                "## Stretch goals:\n",
                "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
                "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
                "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
                "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
                "- Run on bigger, better data\n",
                "\n",
                "## Resources:\n",
                "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
                "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
                "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
                "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
                "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import and encoding\n",
                "response = requests.get(full_text)\n",
                "train_text = response.text\n",
                "response.encoding = 'utf-8'\n",
                "# len(text) # For debugging\n",
                "\n",
                "# Encoding data as chars\n",
                "chars = list(set(train_text))\n",
                "char_int = {c:i for i, c in enumerate(chars)}\n",
                "int_char = {i:c for i, c in enumerate(chars)}\n",
                "\n",
                "# Create the Sequence Data\n",
                "maxlen = 40\n",
                "step = 5\n",
                "encoded = [char_int[c] for c in train_text]\n",
                "sequences = [] # Each element is 40 characters long\n",
                "next_chars = [] # One element for each sequence\n",
                "for i in range(0, len(encoded) - maxlen, step):\n",
                "    sequences.append(encoded[i : i + maxlen])\n",
                "    next_chars.append(encoded[i + maxlen])\n",
                "# print('sequences:', len(sequences)) # For debugging\n",
                "\n",
                "# Specify x & y\n",
                "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
                "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
                "for i, sequence in enumerate(sequences):\n",
                "    for t, char in enumerate(sequence):\n",
                "        x[i, t, char] = 1\n",
                "    y[i, next_chars[i]] = 1\n",
                "print(x.shape, y.shape) # For debugging\n",
                "y"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python (DL)",
            "language": "python",
            "name": "python-deeplearning"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.4-final"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}