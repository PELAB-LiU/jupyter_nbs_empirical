{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":653195,"sourceType":"datasetVersion","datasetId":325566}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport os\nimport sys\nimport IPython.display as ipd\nfrom IPython.display import Audio\n# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\nimport librosa\nimport librosa.display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\n# to play the audio files\nfrom IPython.display import Audio\n\nimport keras\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n\nfrom keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.layers import Input, Flatten, Dropout, Activation\nfrom keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping,ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import SGD\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nimport tensorflow as tf \nprint (\"Done\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-31T10:36:42.151404Z","iopub.execute_input":"2023-12-31T10:36:42.151777Z","iopub.status.idle":"2023-12-31T10:36:55.462418Z","shell.execute_reply.started":"2023-12-31T10:36:42.151749Z","shell.execute_reply":"2023-12-31T10:36:55.461470Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"Done\n","output_type":"stream"}]},{"cell_type":"code","source":"import librosa\nimport soundfile\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:36:55.465504Z","iopub.execute_input":"2023-12-31T10:36:55.466320Z","iopub.status.idle":"2023-12-31T10:36:55.482814Z","shell.execute_reply.started":"2023-12-31T10:36:55.466290Z","shell.execute_reply":"2023-12-31T10:36:55.481639Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"Crema = \"/kaggle/input/cremad/AudioWAV/\"","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:36:55.484464Z","iopub.execute_input":"2023-12-31T10:36:55.484880Z","iopub.status.idle":"2023-12-31T10:36:55.502630Z","shell.execute_reply.started":"2023-12-31T10:36:55.484832Z","shell.execute_reply":"2023-12-31T10:36:55.501732Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"crema_directory_list = os.listdir(Crema)\n\nfile_emotion = []\nfile_path = []\n\nfor file in crema_directory_list:\n    # storing file paths\n    file_path.append(Crema + file)\n    # storing file emotions\n    part=file.split('_')\n    if part[2] == 'SAD':\n        file_emotion.append('sad')\n    elif part[2] == 'ANG':\n        file_emotion.append('angry')\n    elif part[2] == 'DIS':\n        file_emotion.append('disgust')\n    elif part[2] == 'FEA':\n        file_emotion.append('fear')\n    elif part[2] == 'HAP':\n        file_emotion.append('happy')\n    elif part[2] == 'NEU':\n        file_emotion.append('neutral')\n    else:\n        file_emotion.append('Unknown')\n        \ncrema_df = pd.concat([\n    pd.DataFrame(file_path, columns=['path']),\n    pd.DataFrame(file_emotion, columns=['emotion'])\n], axis=1)\n\ncrema_df.head()\nprint(crema_df.emotion.value_counts())","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:36:55.505343Z","iopub.execute_input":"2023-12-31T10:36:55.505709Z","iopub.status.idle":"2023-12-31T10:36:56.090806Z","shell.execute_reply.started":"2023-12-31T10:36:55.505675Z","shell.execute_reply":"2023-12-31T10:36:56.089869Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"emotion\ndisgust    1271\nhappy      1271\nsad        1271\nfear       1271\nangry      1271\nneutral    1087\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"crema_df['emotion'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:36:56.092057Z","iopub.execute_input":"2023-12-31T10:36:56.092450Z","iopub.status.idle":"2023-12-31T10:36:56.102227Z","shell.execute_reply.started":"2023-12-31T10:36:56.092422Z","shell.execute_reply":"2023-12-31T10:36:56.101313Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"emotion\ndisgust    1271\nhappy      1271\nsad        1271\nfear       1271\nangry      1271\nneutral    1087\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df=crema_df","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:36:56.103615Z","iopub.execute_input":"2023-12-31T10:36:56.103963Z","iopub.status.idle":"2023-12-31T10:36:56.110756Z","shell.execute_reply.started":"2023-12-31T10:36:56.103932Z","shell.execute_reply":"2023-12-31T10:36:56.109860Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import librosa\nimport matplotlib.pyplot as plt\n\nplt.style.use('ggplot')","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:36:56.111921Z","iopub.execute_input":"2023-12-31T10:36:56.112254Z","iopub.status.idle":"2023-12-31T10:36:56.120514Z","shell.execute_reply.started":"2023-12-31T10:36:56.112229Z","shell.execute_reply":"2023-12-31T10:36:56.119648Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:36:56.121613Z","iopub.execute_input":"2023-12-31T10:36:56.121902Z","iopub.status.idle":"2023-12-31T10:36:56.138600Z","shell.execute_reply.started":"2023-12-31T10:36:56.121878Z","shell.execute_reply":"2023-12-31T10:36:56.137569Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                                path  emotion\n0  /kaggle/input/cremad/AudioWAV/1028_TSI_DIS_XX.wav  disgust\n1  /kaggle/input/cremad/AudioWAV/1075_IEO_HAP_LO.wav    happy\n2  /kaggle/input/cremad/AudioWAV/1084_ITS_HAP_XX.wav    happy\n3  /kaggle/input/cremad/AudioWAV/1067_IWW_DIS_XX.wav  disgust\n4  /kaggle/input/cremad/AudioWAV/1066_TIE_DIS_XX.wav  disgust","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>emotion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/cremad/AudioWAV/1028_TSI_DIS_XX.wav</td>\n      <td>disgust</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/cremad/AudioWAV/1075_IEO_HAP_LO.wav</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/cremad/AudioWAV/1084_ITS_HAP_XX.wav</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/cremad/AudioWAV/1067_IWW_DIS_XX.wav</td>\n      <td>disgust</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/cremad/AudioWAV/1066_TIE_DIS_XX.wav</td>\n      <td>disgust</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['emotion'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:36:56.139918Z","iopub.execute_input":"2023-12-31T10:36:56.140555Z","iopub.status.idle":"2023-12-31T10:36:56.148865Z","shell.execute_reply.started":"2023-12-31T10:36:56.140520Z","shell.execute_reply":"2023-12-31T10:36:56.147922Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"emotion\ndisgust    1271\nhappy      1271\nsad        1271\nfear       1271\nangry      1271\nneutral    1087\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"from pydub import AudioSegment, effects","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:36:56.151841Z","iopub.execute_input":"2023-12-31T10:36:56.152104Z","iopub.status.idle":"2023-12-31T10:36:56.167818Z","shell.execute_reply.started":"2023-12-31T10:36:56.152081Z","shell.execute_reply":"2023-12-31T10:36:56.167156Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def preprocess_audio(path):\n    _, sr = librosa.load(path)\n    raw_audio = AudioSegment.from_file(path)\n    \n    samples = np.array(raw_audio.get_array_of_samples(), dtype='float32')\n    trimmed, _ = librosa.effects.trim(samples, top_db=25)\n    padded = np.pad(trimmed, (0, 180000-len(trimmed)), 'constant')\n    return padded, sr","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:36:56.168826Z","iopub.execute_input":"2023-12-31T10:36:56.169084Z","iopub.status.idle":"2023-12-31T10:36:56.174563Z","shell.execute_reply.started":"2023-12-31T10:36:56.169060Z","shell.execute_reply":"2023-12-31T10:36:56.173732Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"emotion_dic = {\n    'neutral' : 0,\n    'happy'   : 1,\n    'sad'     : 2, \n    'angry'   : 3, \n    'fear'    : 4, \n    'disgust' : 5\n}\n\ndef encode(label):\n    return emotion_dic.get(label)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:36:56.175673Z","iopub.execute_input":"2023-12-31T10:36:56.175944Z","iopub.status.idle":"2023-12-31T10:36:56.183434Z","shell.execute_reply.started":"2023-12-31T10:36:56.175920Z","shell.execute_reply":"2023-12-31T10:36:56.182637Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"zcr_list = []\nrms_list = []\nmfccs_list = []\nemotion_list = []\n\nFRAME_LENGTH = 2048\nHOP_LENGTH = 512\n\nfor row in df.itertuples(index=False):\n    try: \n        y, sr = preprocess_audio(row.path)\n\n        zcr = librosa.feature.zero_crossing_rate(y, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)\n        rms = librosa.feature.rms(y=y, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)\n        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, hop_length=HOP_LENGTH)\n\n        zcr_list.append(zcr)\n        rms_list.append(rms)\n        mfccs_list.append(mfccs)\n\n        emotion_list.append(encode(row.emotion))\n    except:\n        print(f\"Failed for path: {row.path}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:36:56.184448Z","iopub.execute_input":"2023-12-31T10:36:56.184801Z","iopub.status.idle":"2023-12-31T10:51:08.976164Z","shell.execute_reply.started":"2023-12-31T10:36:56.184773Z","shell.execute_reply":"2023-12-31T10:51:08.974687Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"X = np.concatenate((\n    np.swapaxes(zcr_list, 1, 2), \n    np.swapaxes(rms_list, 1, 2), \n    np.swapaxes(mfccs_list, 1, 2)), \n    axis=2\n)\nX = X.astype('float32')\n\ny = np.asarray(emotion_list)\ny = np.expand_dims(y, axis=1).astype('int8')","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:51:08.983568Z","iopub.execute_input":"2023-12-31T10:51:08.988508Z","iopub.status.idle":"2023-12-31T10:51:09.277668Z","shell.execute_reply.started":"2023-12-31T10:51:08.988452Z","shell.execute_reply":"2023-12-31T10:51:09.276845Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"X_train, X_to_split, y_train, y_to_split = train_test_split(X, y, test_size=0.12, random_state=1)\nX_val, X_test, y_val, y_test = train_test_split(X_to_split, y_to_split, test_size=0.3, random_state=1)\n\ny_train_class = to_categorical(y_train, 6)\ny_val_class = to_categorical(y_val, 6)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:51:09.278747Z","iopub.execute_input":"2023-12-31T10:51:09.279043Z","iopub.status.idle":"2023-12-31T10:51:09.342240Z","shell.execute_reply.started":"2023-12-31T10:51:09.279017Z","shell.execute_reply":"2023-12-31T10:51:09.341438Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras import layers, optimizers, callbacks\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:51:09.343729Z","iopub.execute_input":"2023-12-31T10:51:09.344103Z","iopub.status.idle":"2023-12-31T10:51:09.351132Z","shell.execute_reply.started":"2023-12-31T10:51:09.344067Z","shell.execute_reply":"2023-12-31T10:51:09.350154Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"MODEL = Sequential()\nMODEL.add(layers.LSTM(512, return_sequences=True, input_shape=(X.shape[1:3])))\nMODEL.add(layers.LSTM(512))\nMODEL.add(layers.Dense(6, activation='softmax'))\n\nprint(MODEL.summary())","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:13:31.063135Z","iopub.execute_input":"2024-01-01T17:13:31.063576Z","iopub.status.idle":"2024-01-01T17:13:31.494159Z","shell.execute_reply.started":"2024-01-01T17:13:31.063533Z","shell.execute_reply":"2024-01-01T17:13:31.492633Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m MODEL \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m()\n\u001b[1;32m      2\u001b[0m MODEL\u001b[38;5;241m.\u001b[39madd(layers\u001b[38;5;241m.\u001b[39mLSTM(\u001b[38;5;241m512\u001b[39m, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, input_shape\u001b[38;5;241m=\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m3\u001b[39m])))\n\u001b[1;32m      3\u001b[0m MODEL\u001b[38;5;241m.\u001b[39madd(layers\u001b[38;5;241m.\u001b[39mLSTM(\u001b[38;5;241m512\u001b[39m))\n","\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"],"ename":"NameError","evalue":"name 'Sequential' is not defined","output_type":"error"}]},{"cell_type":"code","source":"rlrop = callbacks.ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.01, patience=100)\nMODEL.compile(loss='categorical_crossentropy', optimizer='RMSProp', metrics=['categorical_accuracy'])\nmodel = MODEL.fit(X_train, y_train_class, epochs=100, batch_size=32, validation_data=(X_val, y_val_class))","metadata":{"execution":{"iopub.status.busy":"2023-12-31T11:31:42.691151Z","iopub.execute_input":"2023-12-31T11:31:42.692096Z","iopub.status.idle":"2023-12-31T12:00:37.366276Z","shell.execute_reply.started":"2023-12-31T11:31:42.692058Z","shell.execute_reply":"2023-12-31T12:00:37.365404Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Epoch 1/100\n205/205 [==============================] - 27s 91ms/step - loss: 1.5947 - categorical_accuracy: 0.3303 - val_loss: 1.5719 - val_categorical_accuracy: 0.3056\nEpoch 2/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.5268 - categorical_accuracy: 0.3581 - val_loss: 1.7142 - val_categorical_accuracy: 0.3216\nEpoch 3/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.5131 - categorical_accuracy: 0.3691 - val_loss: 1.5788 - val_categorical_accuracy: 0.3376\nEpoch 4/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.5045 - categorical_accuracy: 0.3728 - val_loss: 1.5767 - val_categorical_accuracy: 0.2832\nEpoch 5/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.5023 - categorical_accuracy: 0.3781 - val_loss: 1.5682 - val_categorical_accuracy: 0.3248\nEpoch 6/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.5024 - categorical_accuracy: 0.3792 - val_loss: 1.5716 - val_categorical_accuracy: 0.3200\nEpoch 7/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4943 - categorical_accuracy: 0.3876 - val_loss: 1.5672 - val_categorical_accuracy: 0.3408\nEpoch 8/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4926 - categorical_accuracy: 0.3812 - val_loss: 1.5588 - val_categorical_accuracy: 0.3456\nEpoch 9/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4877 - categorical_accuracy: 0.3865 - val_loss: 1.5714 - val_categorical_accuracy: 0.3376\nEpoch 10/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4848 - categorical_accuracy: 0.3864 - val_loss: 1.5669 - val_categorical_accuracy: 0.3408\nEpoch 11/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4862 - categorical_accuracy: 0.3855 - val_loss: 1.5551 - val_categorical_accuracy: 0.3424\nEpoch 12/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4889 - categorical_accuracy: 0.3858 - val_loss: 1.5423 - val_categorical_accuracy: 0.3248\nEpoch 13/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4825 - categorical_accuracy: 0.3858 - val_loss: 1.5273 - val_categorical_accuracy: 0.3424\nEpoch 14/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4749 - categorical_accuracy: 0.3859 - val_loss: 1.5683 - val_categorical_accuracy: 0.3424\nEpoch 15/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4763 - categorical_accuracy: 0.3923 - val_loss: 1.5260 - val_categorical_accuracy: 0.3568\nEpoch 16/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4677 - categorical_accuracy: 0.3934 - val_loss: 1.5187 - val_categorical_accuracy: 0.3552\nEpoch 17/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4700 - categorical_accuracy: 0.3879 - val_loss: 1.5422 - val_categorical_accuracy: 0.3424\nEpoch 18/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4651 - categorical_accuracy: 0.3917 - val_loss: 1.5339 - val_categorical_accuracy: 0.3440\nEpoch 19/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4569 - categorical_accuracy: 0.3945 - val_loss: 1.5406 - val_categorical_accuracy: 0.3504\nEpoch 20/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4626 - categorical_accuracy: 0.3943 - val_loss: 1.5301 - val_categorical_accuracy: 0.3616\nEpoch 21/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4540 - categorical_accuracy: 0.3995 - val_loss: 1.5158 - val_categorical_accuracy: 0.3456\nEpoch 22/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4531 - categorical_accuracy: 0.4009 - val_loss: 1.4869 - val_categorical_accuracy: 0.3584\nEpoch 23/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4536 - categorical_accuracy: 0.4052 - val_loss: 1.5725 - val_categorical_accuracy: 0.3488\nEpoch 24/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4465 - categorical_accuracy: 0.3991 - val_loss: 1.5000 - val_categorical_accuracy: 0.3792\nEpoch 25/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4548 - categorical_accuracy: 0.4012 - val_loss: 1.5001 - val_categorical_accuracy: 0.3616\nEpoch 26/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4574 - categorical_accuracy: 0.3914 - val_loss: 1.4928 - val_categorical_accuracy: 0.3632\nEpoch 27/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4488 - categorical_accuracy: 0.4006 - val_loss: 1.4954 - val_categorical_accuracy: 0.3456\nEpoch 28/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4491 - categorical_accuracy: 0.3968 - val_loss: 1.5460 - val_categorical_accuracy: 0.3616\nEpoch 29/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4438 - categorical_accuracy: 0.3937 - val_loss: 1.4843 - val_categorical_accuracy: 0.3392\nEpoch 30/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4478 - categorical_accuracy: 0.3992 - val_loss: 1.4980 - val_categorical_accuracy: 0.3472\nEpoch 31/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4413 - categorical_accuracy: 0.3974 - val_loss: 1.5599 - val_categorical_accuracy: 0.3808\nEpoch 32/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4264 - categorical_accuracy: 0.4059 - val_loss: 1.4863 - val_categorical_accuracy: 0.3760\nEpoch 33/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4307 - categorical_accuracy: 0.4091 - val_loss: 1.5150 - val_categorical_accuracy: 0.3664\nEpoch 34/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4259 - categorical_accuracy: 0.4006 - val_loss: 1.4847 - val_categorical_accuracy: 0.3872\nEpoch 35/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4305 - categorical_accuracy: 0.4055 - val_loss: 1.5268 - val_categorical_accuracy: 0.3808\nEpoch 36/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4308 - categorical_accuracy: 0.3977 - val_loss: 1.5372 - val_categorical_accuracy: 0.3456\nEpoch 37/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4323 - categorical_accuracy: 0.4094 - val_loss: 1.5050 - val_categorical_accuracy: 0.3648\nEpoch 38/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4245 - categorical_accuracy: 0.4113 - val_loss: 1.4666 - val_categorical_accuracy: 0.3760\nEpoch 39/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4211 - categorical_accuracy: 0.4169 - val_loss: 1.4688 - val_categorical_accuracy: 0.3824\nEpoch 40/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4169 - categorical_accuracy: 0.4230 - val_loss: 1.4745 - val_categorical_accuracy: 0.3632\nEpoch 41/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4135 - categorical_accuracy: 0.4244 - val_loss: 1.5238 - val_categorical_accuracy: 0.3696\nEpoch 42/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4121 - categorical_accuracy: 0.4249 - val_loss: 1.5272 - val_categorical_accuracy: 0.3616\nEpoch 43/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4150 - categorical_accuracy: 0.4189 - val_loss: 1.4507 - val_categorical_accuracy: 0.3984\nEpoch 44/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.4089 - categorical_accuracy: 0.4239 - val_loss: 1.4324 - val_categorical_accuracy: 0.4192\nEpoch 45/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.3966 - categorical_accuracy: 0.4223 - val_loss: 1.4528 - val_categorical_accuracy: 0.3856\nEpoch 46/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.3974 - categorical_accuracy: 0.4308 - val_loss: 1.4613 - val_categorical_accuracy: 0.3824\nEpoch 47/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.3927 - categorical_accuracy: 0.4311 - val_loss: 1.4438 - val_categorical_accuracy: 0.3856\nEpoch 48/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.3832 - categorical_accuracy: 0.4359 - val_loss: 1.4547 - val_categorical_accuracy: 0.4080\nEpoch 49/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.3903 - categorical_accuracy: 0.4278 - val_loss: 1.4621 - val_categorical_accuracy: 0.3856\nEpoch 50/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.3809 - categorical_accuracy: 0.4337 - val_loss: 1.4604 - val_categorical_accuracy: 0.4016\nEpoch 51/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.3721 - categorical_accuracy: 0.4456 - val_loss: 1.5059 - val_categorical_accuracy: 0.3904\nEpoch 52/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.3759 - categorical_accuracy: 0.4392 - val_loss: 1.4523 - val_categorical_accuracy: 0.4096\nEpoch 53/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.3665 - categorical_accuracy: 0.4472 - val_loss: 1.4322 - val_categorical_accuracy: 0.4096\nEpoch 54/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.3591 - categorical_accuracy: 0.4534 - val_loss: 1.4164 - val_categorical_accuracy: 0.4304\nEpoch 55/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.3494 - categorical_accuracy: 0.4606 - val_loss: 1.3988 - val_categorical_accuracy: 0.4448\nEpoch 56/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.3476 - categorical_accuracy: 0.4583 - val_loss: 1.3928 - val_categorical_accuracy: 0.4288\nEpoch 57/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.3466 - categorical_accuracy: 0.4626 - val_loss: 1.4457 - val_categorical_accuracy: 0.4080\nEpoch 58/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.3417 - categorical_accuracy: 0.4653 - val_loss: 1.3933 - val_categorical_accuracy: 0.4432\nEpoch 59/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.3280 - categorical_accuracy: 0.4736 - val_loss: 1.4175 - val_categorical_accuracy: 0.4416\nEpoch 60/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.3119 - categorical_accuracy: 0.4823 - val_loss: 1.4413 - val_categorical_accuracy: 0.4096\nEpoch 61/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.3113 - categorical_accuracy: 0.4795 - val_loss: 1.3928 - val_categorical_accuracy: 0.4464\nEpoch 62/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.2923 - categorical_accuracy: 0.4896 - val_loss: 1.3615 - val_categorical_accuracy: 0.4512\nEpoch 63/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.2998 - categorical_accuracy: 0.4928 - val_loss: 1.3788 - val_categorical_accuracy: 0.4672\nEpoch 64/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.2967 - categorical_accuracy: 0.4954 - val_loss: 1.3892 - val_categorical_accuracy: 0.4288\nEpoch 65/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.2836 - categorical_accuracy: 0.4937 - val_loss: 1.3284 - val_categorical_accuracy: 0.4848\nEpoch 66/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.2638 - categorical_accuracy: 0.5050 - val_loss: 1.4795 - val_categorical_accuracy: 0.3952\nEpoch 67/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.2586 - categorical_accuracy: 0.5095 - val_loss: 1.4267 - val_categorical_accuracy: 0.4112\nEpoch 68/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.2536 - categorical_accuracy: 0.5156 - val_loss: 1.2979 - val_categorical_accuracy: 0.4896\nEpoch 69/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.2471 - categorical_accuracy: 0.5092 - val_loss: 1.3058 - val_categorical_accuracy: 0.4816\nEpoch 70/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.2419 - categorical_accuracy: 0.5148 - val_loss: 1.3280 - val_categorical_accuracy: 0.4704\nEpoch 71/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.2410 - categorical_accuracy: 0.5174 - val_loss: 1.3173 - val_categorical_accuracy: 0.4688\nEpoch 72/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.2264 - categorical_accuracy: 0.5226 - val_loss: 1.2990 - val_categorical_accuracy: 0.4656\nEpoch 73/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.2145 - categorical_accuracy: 0.5293 - val_loss: 1.2610 - val_categorical_accuracy: 0.5008\nEpoch 74/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.2120 - categorical_accuracy: 0.5237 - val_loss: 1.3461 - val_categorical_accuracy: 0.4480\nEpoch 75/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.2047 - categorical_accuracy: 0.5263 - val_loss: 1.3200 - val_categorical_accuracy: 0.4704\nEpoch 76/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.1975 - categorical_accuracy: 0.5295 - val_loss: 1.2803 - val_categorical_accuracy: 0.4896\nEpoch 77/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.1830 - categorical_accuracy: 0.5449 - val_loss: 1.2782 - val_categorical_accuracy: 0.4720\nEpoch 78/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.1862 - categorical_accuracy: 0.5415 - val_loss: 1.3127 - val_categorical_accuracy: 0.4864\nEpoch 79/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.1802 - categorical_accuracy: 0.5458 - val_loss: 1.2675 - val_categorical_accuracy: 0.5008\nEpoch 80/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.1641 - categorical_accuracy: 0.5502 - val_loss: 1.2588 - val_categorical_accuracy: 0.4896\nEpoch 81/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.1595 - categorical_accuracy: 0.5570 - val_loss: 1.2627 - val_categorical_accuracy: 0.4912\nEpoch 82/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.1493 - categorical_accuracy: 0.5567 - val_loss: 1.2978 - val_categorical_accuracy: 0.4784\nEpoch 83/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.1349 - categorical_accuracy: 0.5580 - val_loss: 1.2521 - val_categorical_accuracy: 0.5136\nEpoch 84/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.1383 - categorical_accuracy: 0.5664 - val_loss: 1.2252 - val_categorical_accuracy: 0.5056\nEpoch 85/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.1311 - categorical_accuracy: 0.5654 - val_loss: 1.3638 - val_categorical_accuracy: 0.4912\nEpoch 86/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.1130 - categorical_accuracy: 0.5744 - val_loss: 1.2718 - val_categorical_accuracy: 0.4928\nEpoch 87/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.0968 - categorical_accuracy: 0.5834 - val_loss: 1.3192 - val_categorical_accuracy: 0.4752\nEpoch 88/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.0941 - categorical_accuracy: 0.5765 - val_loss: 1.2069 - val_categorical_accuracy: 0.5040\nEpoch 89/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.0840 - categorical_accuracy: 0.5866 - val_loss: 1.2467 - val_categorical_accuracy: 0.5136\nEpoch 90/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.0756 - categorical_accuracy: 0.5942 - val_loss: 1.2432 - val_categorical_accuracy: 0.5184\nEpoch 91/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.0757 - categorical_accuracy: 0.5886 - val_loss: 1.1864 - val_categorical_accuracy: 0.5216\nEpoch 92/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.0560 - categorical_accuracy: 0.5971 - val_loss: 1.2059 - val_categorical_accuracy: 0.5296\nEpoch 93/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.0526 - categorical_accuracy: 0.6014 - val_loss: 1.2526 - val_categorical_accuracy: 0.5168\nEpoch 94/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.0386 - categorical_accuracy: 0.6042 - val_loss: 1.1854 - val_categorical_accuracy: 0.5264\nEpoch 95/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.0318 - categorical_accuracy: 0.6043 - val_loss: 1.1945 - val_categorical_accuracy: 0.5440\nEpoch 96/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.0278 - categorical_accuracy: 0.6055 - val_loss: 1.2013 - val_categorical_accuracy: 0.5456\nEpoch 97/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.0202 - categorical_accuracy: 0.6141 - val_loss: 1.2438 - val_categorical_accuracy: 0.5168\nEpoch 98/100\n205/205 [==============================] - 17s 84ms/step - loss: 1.0080 - categorical_accuracy: 0.6213 - val_loss: 1.1683 - val_categorical_accuracy: 0.5504\nEpoch 99/100\n205/205 [==============================] - 17s 84ms/step - loss: 0.9946 - categorical_accuracy: 0.6202 - val_loss: 1.3294 - val_categorical_accuracy: 0.5200\nEpoch 100/100\n205/205 [==============================] - 17s 84ms/step - loss: 0.9767 - categorical_accuracy: 0.6297 - val_loss: 1.1778 - val_categorical_accuracy: 0.5504\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}