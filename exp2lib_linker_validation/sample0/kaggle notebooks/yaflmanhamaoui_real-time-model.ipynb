{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import cv2\nimport mediapipe as mp\nimport numpy as np\nfrom keras.models import load_model\n\n# Load the ASL model\nloaded_model = load_model('ASL2.h5')\n\n# Define labels dictionary (replace with your actual label mapping)\nlabels_dict = {\n    0: 'A',\n    1: 'B',\n    2: 'C',\n    3: 'D',\n    4: 'Del',\n    5: 'E',\n    6: 'F',\n    7: 'G',\n    8: 'H',\n    9: 'I',\n    10: 'J',\n    11: 'K',\n    12: 'L',\n    13: 'M',\n    14: 'N',\n    15: 'Nothing',\n    16: 'O',\n    17: 'P',\n    18: 'Q',\n    19: 'R',\n    20: 'S',\n    21: 'Space',\n    22: 'T',\n    23: 'U',\n    24: 'V',\n    25: 'W',\n    26: 'X',\n    27: 'Y',\n    28: 'Z'\n}\n\n\n\n# Open a connection to the webcam (0 represents the default camera)\ncap = cv2.VideoCapture(0)\n\nmp_hands = mp.solutions.hands\nmp_drawing = mp.solutions.drawing_utils\nmp_drawing_styles = mp.solutions.drawing_styles\n\nhands = mp_hands.Hands(static_image_mode=False, min_detection_confidence=0.3)\n\nwhile True:\n    ret, frame = cap.read()\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    results = hands.process(frame_rgb)\n\n    if results.multi_hand_landmarks:\n        hand_landmarks = results.multi_hand_landmarks[0]  # Assuming only one hand is detected\n\n        # Extract coordinates of bounding box around hand\n        x_min = int(min(hand_landmarks.landmark[i].x * frame.shape[1] for i in range(21)))\n        y_min = int(min(hand_landmarks.landmark[i].y * frame.shape[0] for i in range(21)))\n        x_max = int(max(hand_landmarks.landmark[i].x * frame.shape[1] for i in range(21)))\n        y_max = int(max(hand_landmarks.landmark[i].y * frame.shape[0] for i in range(21)))\n\n        # Crop the hand region\n        hand_roi = frame[y_min:y_max, x_min:x_max]\n        hand_roi_resized = cv2.resize(hand_roi, (32, 32))  # Resize to match input size of the model\n\n        # Preprocess the image and make a prediction\n        hand_input = np.expand_dims(hand_roi_resized, axis=0) / 255.0  # Normalize to [0, 1]\n        prediction = loaded_model.predict(hand_input)\n\n        predicted_character = labels_dict[np.argmax(prediction)]\n\n        cv2.putText(frame, predicted_character, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 3)\n\n    cv2.imshow('ASL Recognition', frame)\n\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-01T01:20:36.528384Z","iopub.execute_input":"2023-09-01T01:20:36.528840Z","iopub.status.idle":"2023-09-01T01:20:37.252830Z","shell.execute_reply.started":"2023-09-01T01:20:36.528804Z","shell.execute_reply":"2023-09-01T01:20:37.251248Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mediapipe'"],"ename":"ModuleNotFoundError","evalue":"No module named 'mediapipe'","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}