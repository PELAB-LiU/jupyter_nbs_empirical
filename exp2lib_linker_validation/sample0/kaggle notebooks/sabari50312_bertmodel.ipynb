{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport emoji\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-26T10:24:39.872462Z","iopub.execute_input":"2023-02-26T10:24:39.872976Z","iopub.status.idle":"2023-02-26T10:24:39.891330Z","shell.execute_reply.started":"2023-02-26T10:24:39.872928Z","shell.execute_reply":"2023-02-26T10:24:39.888321Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"/kaggle/input/twitter-data/neg_tweets.csv\n/kaggle/input/twitter-data/pos_tweets.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"#Import dataset and view sample\npos=pd.read_csv('/kaggle/input/twitter-data/pos_tweets.csv')\nneg=pd.read_csv('/kaggle/input/twitter-data/neg_tweets.csv')\nprint(neg.head(),'\\n\\n# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\\n\\n',pos.head())","metadata":{"execution":{"iopub.status.busy":"2023-02-26T10:24:39.893401Z","iopub.execute_input":"2023-02-26T10:24:39.894234Z","iopub.status.idle":"2023-02-26T10:24:39.929014Z","shell.execute_reply.started":"2023-02-26T10:24:39.894199Z","shell.execute_reply":"2023-02-26T10:24:39.926874Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"                                    user_description\n0  Food addict, sweet tooth, house music and tequ...\n1  Bollywood and Celebrities Brands Breaking News...\n2          ‏‏‏حساب شخصي. مدمن كرة قدم. CFC #Ittihad#\n3                                   IG: _RichyRozay_\n4                                    fuck em we ball \n\n# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n\n                                     user_description\n0                                            alright\n1  ADHD & Diverse-Ability Transition Coach/Traine...\n2  queer trans bot | she/her | property of @Corde...\n3                 People tend to say I'm stone cold.\n4  Hi, I'm Kris. I'm an okay cross/cosplayer. he/...\n","output_type":"stream"}]},{"cell_type":"code","source":"#Define helper functions and preprocessing function\ndef load_dict_smileys():\n    \n    return {\n        \":‑)\":\"smiley\",\n        \":-]\":\"smiley\",\n        \":-3\":\"smiley\",\n        \":->\":\"smiley\",\n        \"8-)\":\"smiley\",\n        \":-}\":\"smiley\",\n        \":)\":\"smiley\",\n        \":]\":\"smiley\",\n        \":3\":\"smiley\",\n        \":>\":\"smiley\",\n        \"8)\":\"smiley\",\n        \":}\":\"smiley\",\n        \":o)\":\"smiley\",\n        \":c)\":\"smiley\",\n        \":^)\":\"smiley\",\n        \"=]\":\"smiley\",\n        \"=)\":\"smiley\",\n        \":-))\":\"smiley\",\n        \":‑D\":\"smiley\",\n        \"8‑D\":\"smiley\",\n        \"x‑D\":\"smiley\",\n        \"X‑D\":\"smiley\",\n        \":D\":\"smiley\",\n        \"8D\":\"smiley\",\n        \"xD\":\"smiley\",\n        \"XD\":\"smiley\",\n        \":‑(\":\"sad\",\n        \":‑c\":\"sad\",\n        \":‑<\":\"sad\",\n        \":‑[\":\"sad\",\n        \":(\":\"sad\",\n        \":c\":\"sad\",\n        \":<\":\"sad\",\n        \":[\":\"sad\",\n        \":-||\":\"sad\",\n        \">:[\":\"sad\",\n        \":{\":\"sad\",\n        \":@\":\"sad\",\n        \">:(\":\"sad\",\n        \":'‑(\":\"sad\",\n        \":'(\":\"sad\",\n        \":‑P\":\"playful\",\n        \"X‑P\":\"playful\",\n        \"x‑p\":\"playful\",\n        \":‑p\":\"playful\",\n        \":‑Þ\":\"playful\",\n        \":‑þ\":\"playful\",\n        \":‑b\":\"playful\",\n        \":P\":\"playful\",\n        \"XP\":\"playful\",\n        \"xp\":\"playful\",\n        \":p\":\"playful\",\n        \":Þ\":\"playful\",\n        \":þ\":\"playful\",\n        \":b\":\"playful\",\n        \"<3\":\"love\"\n        }\n\n# self defined contractions\ndef load_dict_contractions():\n    \n    return {\n        \"ain't\":\"is not\",\n        \"amn't\":\"am not\",\n        \"aren't\":\"are not\",\n        \"can't\":\"cannot\",\n        \"'cause\":\"because\",\n        \"couldn't\":\"could not\",\n        \"couldn't've\":\"could not have\",\n        \"could've\":\"could have\",\n        \"daren't\":\"dare not\",\n        \"daresn't\":\"dare not\",\n        \"dasn't\":\"dare not\",\n        \"didn't\":\"did not\",\n        \"doesn't\":\"does not\",\n        \"don't\":\"do not\",\n        \"e'er\":\"ever\",\n        \"em\":\"them\",\n        \"everyone's\":\"everyone is\",\n        \"finna\":\"fixing to\",\n        \"gimme\":\"give me\",\n        \"gonna\":\"going to\",\n        \"gon't\":\"go not\",\n        \"gotta\":\"got to\",\n        \"hadn't\":\"had not\",\n        \"hasn't\":\"has not\",\n        \"haven't\":\"have not\",\n        \"he'd\":\"he would\",\n        \"he'll\":\"he will\",\n        \"he's\":\"he is\",\n        \"he've\":\"he have\",\n        \"how'd\":\"how would\",\n        \"how'll\":\"how will\",\n        \"how're\":\"how are\",\n        \"how's\":\"how is\",\n        \"i'd\":\"i would\",\n        \"i'll\":\"i will\",\n        \"i'm\":\"i am\",\n        \"i'm'a\":\"i am about to\",\n        \"i'm'o\":\"i am going to\",\n        \"isn't\":\"is not\",\n        \"it'd\":\"it would\",\n        \"it'll\":\"it will\",\n        \"it's\":\"it is\",\n        \"i've\":\"i have\",\n        \"kinda\":\"kind of\",\n        \"let's\":\"let us\",\n        \"mayn't\":\"may not\",\n        \"may've\":\"may have\",\n        \"mightn't\":\"might not\",\n        \"might've\":\"might have\",\n        \"mustn't\":\"must not\",\n        \"mustn't've\":\"must not have\",\n        \"must've\":\"must have\",\n        \"needn't\":\"need not\",\n        \"ne'er\":\"never\",\n        \"o'\":\"of\",\n        \"o'er\":\"over\",\n        \"ol'\":\"old\",\n        \"oughtn't\":\"ought not\",\n        \"shalln't\":\"shall not\",\n        \"shan't\":\"shall not\",\n        \"she'd\":\"she would\",\n        \"she'll\":\"she will\",\n        \"she's\":\"she is\",\n        \"shouldn't\":\"should not\",\n        \"shouldn't've\":\"should not have\",\n        \"should've\":\"should have\",\n        \"somebody's\":\"somebody is\",\n        \"someone's\":\"someone is\",\n        \"something's\":\"something is\",\n        \"that'd\":\"that would\",\n        \"that'll\":\"that will\",\n        \"that're\":\"that are\",\n        \"that's\":\"that is\",\n        \"there'd\":\"there would\",\n        \"there'll\":\"there will\",\n        \"there're\":\"there are\",\n        \"there's\":\"there is\",\n        \"these're\":\"these are\",\n        \"they'd\":\"they would\",\n        \"they'll\":\"they will\",\n        \"they're\":\"they are\",\n        \"they've\":\"they have\",\n        \"this's\":\"this is\",\n        \"those're\":\"those are\",\n        \"'tis\":\"it is\",\n        \"'twas\":\"it was\",\n        \"wanna\":\"want to\",\n        \"wasn't\":\"was not\",\n        \"we'd\":\"we would\",\n        \"we'd've\":\"we would have\",\n        \"we'll\":\"we will\",\n        \"we're\":\"we are\",\n        \"weren't\":\"were not\",\n        \"we've\":\"we have\",\n        \"what'd\":\"what did\",\n        \"what'll\":\"what will\",\n        \"what're\":\"what are\",\n        \"what's\":\"what is\",\n        \"what've\":\"what have\",\n        \"when's\":\"when is\",\n        \"where'd\":\"where did\",\n        \"where're\":\"where are\",\n        \"where's\":\"where is\",\n        \"where've\":\"where have\",\n        \"which's\":\"which is\",\n        \"who'd\":\"who would\",\n        \"who'd've\":\"who would have\",\n        \"who'll\":\"who will\",\n        \"who're\":\"who are\",\n        \"who's\":\"who is\",\n        \"who've\":\"who have\",\n        \"why'd\":\"why did\",\n        \"why're\":\"why are\",\n        \"why's\":\"why is\",\n        \"won't\":\"will not\",\n        \"wouldn't\":\"would not\",\n        \"would've\":\"would have\",\n        \"y'all\":\"you all\",\n        \"you'd\":\"you would\",\n        \"you'll\":\"you will\",\n        \"you're\":\"you are\",\n        \"you've\":\"you have\",\n        \"whatcha\":\"what are you\",\n        \"luv\":\"love\",\n        \"sux\":\"sucks\"\n        }\n\ndef reduce_lengthening(text):\n    pattern = re.compile(r\"(.)\\1{2,}\")\n    return pattern.sub(r\"\\1\\1\", text)\n\ndef clean_str(string):\n\n    string = re.sub(r'http\\S+', '', string, flags=re.MULTILINE)\n    string = re.sub(r'www.\\S+', '', string, flags=re.MULTILINE)\n#    string = re.sub(r'@\\S+', '', string, flags=re.MULTILINE)\n    string = re.sub(r\"http\\S+\", \"\", string, flags=re.MULTILINE)\n#    print(string)\n    string = string.replace('\\n', ' ').replace('\\t', ' ')\n    string = string.lower()\n    string = reduce_lengthening(string)\n    \n    #CONTRACTIONS source: https://en.wikipedia.org/wiki/Contraction_%28grammar%29\n    CONTRACTIONS = load_dict_contractions()\n    string = string.replace(\"’\",\"'\").replace(\"“\",\"'\").replace(\"”\",\"'\").replace(\"’\",\"'\").replace(\"‘\",\"'\")\n    words = string.split()\n    reformed = [CONTRACTIONS[word] if word in CONTRACTIONS else word for word in words]\n    string = \" \".join(reformed)\n\n    #Deal with emoticons source: https://en.wikipedia.org/wiki/List_of_emoticons\n    SMILEY = load_dict_smileys()  \n    words = string.split()\n    reformed = [SMILEY[word] if word in SMILEY else word for word in words]\n    string = \" \".join(reformed)\n    \n    #Deal with emojis\n    string = emoji.demojize(string)\n\n    string = ' '.join(string.split())\n\n    string = re.sub('[^A-Za-z0-9.?;!]+', ' ', string).lstrip().lower()\n    string = string.replace(\";\",\" ; \").replace(\".\",\" . \").replace(\"?\",\" ? \").replace(\"!\",\" ! \")\n\n    string = ' '.join(string.split())    \n\n#    load()\n#    segs = segment(string)\n#    print(segs)\n#    string = ' '.join(segs)\n#    print(string)\n\n    return string.strip().lower()","metadata":{"execution":{"iopub.status.busy":"2023-02-26T10:24:39.931170Z","iopub.execute_input":"2023-02-26T10:24:39.931560Z","iopub.status.idle":"2023-02-26T10:24:39.963506Z","shell.execute_reply.started":"2023-02-26T10:24:39.931531Z","shell.execute_reply":"2023-02-26T10:24:39.960713Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#Apply text cleaning to both datasets\nneg['user_description']=neg['user_description'].apply(lambda x: clean_str(x))\npos['user_description']=pos['user_description'].apply(lambda x: clean_str(x))\n\n#Create label column (0 for neg and 1 for pos)\nneg['label']=0\npos['label']=1\n\n#Combine the 2 datasets\ndf=[neg,pos]\ndf=pd.concat(df)\ndf\n\n#Split it into train and test with shuffling\ntest,train=train_test_split(df,test_size=0.25, random_state=1234, shuffle=True)\n\n#View the results\nprint(train,test,sep='\\n')","metadata":{"execution":{"iopub.status.busy":"2023-02-26T10:24:39.966966Z","iopub.execute_input":"2023-02-26T10:24:39.967524Z","iopub.status.idle":"2023-02-26T10:24:40.644704Z","shell.execute_reply.started":"2023-02-26T10:24:39.967474Z","shell.execute_reply":"2023-02-26T10:24:40.643285Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"                                       user_description  label\n3317            want to try something new in your bed ?      0\n402   books wrestling music ganja and animals . get ...      1\n3274  jon snow . lord of the north . swordsman leade...      0\n4401  born feb2 1992 seahawks fan run what you broun...      1\n545                                               ! ! !      0\n...                                                 ...    ...\n3318  writer journalist mental health campaigner . c...      1\n3185  nintendo switch going to be lit spare tony shh...      0\n2761  in the quiet words of the virgin mary come aga...      1\n217   follow my site csa survivor supports end to dv...      1\n4048                rainbow flag love wins rainbow flag      0\n\n[2650 rows x 2 columns]\n                                       user_description  label\n5279  queer trans bot she her property of cordelia e...      1\n2255  23 programmer internetislife i am just trying ...      0\n5038  we are all going to be one big happy dysfuncti...      1\n1659                                                 aa      0\n1804  hello pals i am ad i love doofluffle smiling f...      1\n...                                                 ...    ...\n664   random thoughts my life other stuff . . writer...      0\n2833  18 m libertarian republican sci fi fan all tho...      1\n2514  28 years old ex casino hotel manager turned po...      1\n1318                 sc yallknowlondon ig thereallondon      0\n4208  22 blogger my ultimateidol danniiminogue follo...      1\n\n[7949 rows x 2 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load pre-trained model and tokenizer\nmodel_name = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertForSequenceClassification.from_pretrained(model_name)\n# Tokenize input text\ntext = \"Hello, how are you doing today?\"\ninputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n\n# Run model inference\noutputs = model(**inputs)\n\n# Get predicted class\n_, predicted_class = torch.max(outputs.logits, dim=1)\nprint(predicted_class)\n\n\n\n\n#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n#model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2023-02-26T10:29:10.117950Z","iopub.execute_input":"2023-02-26T10:29:10.118396Z","iopub.status.idle":"2023-02-26T10:30:30.253705Z","shell.execute_reply.started":"2023-02-26T10:29:10.118357Z","shell.execute_reply":"2023-02-26T10:30:30.252248Z"},"trusted":true},"execution_count":20,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_28/1451838718.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load pre-trained model and tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Tokenize input text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1787\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_file_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull_file_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m             raise EnvironmentError(\n\u001b[0;32m-> 1789\u001b[0;31m                 \u001b[0;34mf\"Can't load tokenizer for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1790\u001b[0m                 \u001b[0;34m\"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0;34mf\"Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a directory \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'bert-base-uncased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bert-base-uncased' is the correct path to a directory containing all relevant files for a BertTokenizer tokenizer."],"ename":"OSError","evalue":"Can't load tokenizer for 'bert-base-uncased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bert-base-uncased' is the correct path to a directory containing all relevant files for a BertTokenizer tokenizer.","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}