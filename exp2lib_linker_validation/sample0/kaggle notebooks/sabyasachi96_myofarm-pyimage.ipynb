{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6157465,"sourceType":"datasetVersion","datasetId":3448145}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# All the imports\n!pip install patchify\n!pip install GPUtil\n!pip install torchsummary\n\nfrom GPUtil import showUtilization as gpu_usage\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n# import seaborn as sns\nimport os\nimport cv2\n# import random\nimport glob\nimport PIL\nfrom PIL import Image\nfrom tqdm import tqdm\nimport imghdr\nfrom patchify import patchify \n\n\nimport time\nimport torch\nimport torchvision\nimport torch.optim as optim\nimport albumentations as A\nimport torch.nn as nn\nimport torchvision.transforms.functional as TF\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn import ConvTranspose2d\nfrom torch.nn import Conv2d\nfrom torch.nn import MaxPool2d\nfrom torch.nn import Module\nfrom torch.nn import ModuleList\nfrom torch.nn import ReLU\nfrom torch.nn import Dropout\nfrom torchsummary import summary\n\nfrom torch.nn import BatchNorm2d \n\nfrom torchvision.transforms import CenterCrop\nfrom torch.nn import functional as F\nfrom torch.nn.functional import normalize","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-15T12:24:03.751566Z","iopub.execute_input":"2023-12-15T12:24:03.752327Z","iopub.status.idle":"2023-12-15T12:24:47.701249Z","shell.execute_reply.started":"2023-12-15T12:24:03.752296Z","shell.execute_reply":"2023-12-15T12:24:47.700161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- source_tutorial: https://pyimagesearch.com/2021/11/08/u-net-training-image-segmentation-models-in-pytorch/\n- unet: https://github.com/milesial/Pytorch-UNet\n- Probablistic_unet: https://github.com/stefanknegt/Probabilistic-Unet-Pytorch/blob/master/probabilistic_unet.py\n- utility script: https://github.com/CaptainDredge/Image-segmentation-utilities\n- GPU_utility: https://github.com/anderskm/gputil\n- torch_em: https://github.com/computational-cell-analytics/dl-for-micro/blob/main/2_cell_segmentation/torchem-train-cell-membrane-segmentation.ipynb\n- torch_em: https://github.com/constantinpape/torch-em/blob/main/torch_em/model/unet.py\n- digital_sreeni: https://github.com/bnsreenu/python_for_microscopists/blob/master/206_sem_segm_large_images_using_unet_with_custom_patch_inference.py\n- data_Augmentatin: https://albumentations.ai/docs/getting_started/mask_augmentation/\n- learning rate https://www.kaggle.com/code/isbhargav/guide-to-pytorch-learning-rate-scheduling\n\n","metadata":{}},{"cell_type":"markdown","source":"### loss functionds\nThe validation loss function is just a metric and actually not needed for training. It's there because it make sense to compare the metrics which your network is actually optimzing on. So you can add any other loss function as metric during compilation and you'll see it during training.\n- https://dev.to/_aadidev/3-common-loss-functions-for-image-segmentation-545o\n- https://www.kaggle.com/code/yassinealouini/all-the-segmentation-metrics/notebook\n- https://github.com/JunMa11/SegLoss \n- https://pyimagesearch.com/2019/10/14/why-is-my-validation-loss-lower-than-my-training-loss/\n- important for iunet loss and everytjing https://discuss.pytorch.org/t/unet-implementation/426/12","metadata":{}},{"cell_type":"markdown","source":"name coding: \nbSz2_1CH_ip128_vls20_ADM_EnDc32-1024_drp_ep10_aug4\n\n- bsz: batch size\n- lR: learning rate 1e-num\n- SDG: optimiser\n- ADM: Adam optimiser\n- clcLR: scheduler \n- vls: validation split\n- EnDc: encodert decoder channels\n- pt: patience for earkly stopping\n- i/p : input size \n- ep: epochs\n- wd: weight decay(l2 reg)\n- drp: drop out added\n- 1CH: input one channel instead of RGB\n- aug: num of augmentation","metadata":{}},{"cell_type":"markdown","source":"## Setting up configuration for running in notebooks\n\n- The pin memory is set to True to the DataLoader which will automatically put the fetched data Tensors in pinned memory, enabling faster data transfer to CUDA-enabled GPU's. For every epoch the data is transferred from CPU to GPU, with augmentations done in the CPU, and trainings done in the GPU.\n- Check the output directory for saving the images, model path, plot path and test path (BASE_OUTPUT, MODEL_PATH, PLOT_PATH, TEST_PATH)\n- Note keep the bacthsize small (2,5) untill you start using pacthes of the image otherwise the GPU will run out of memory","metadata":{}},{"cell_type":"code","source":"plotName = 'bSz2_1CH_ip128_vls20_ADM_EnDc32-1024_drp_ep10_aug4'\n# define the test split\n# TEST_SPLIT = 0.3\nVALIDATION_SPLIT = 20\n\n# determine the device to be used for training and evaluation\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# determine if we will be pinning memory during data loading\n# PIN_MEMORY = True if DEVICE == \"cuda\" else False\n\n# define the number of channels in the input, number of classes,\n# and number of levels in the U-Net model\nNUM_CHANNELS = 1\nNUM_CLASSES = 1\nNUM_LEVELS = 3\nNUM_WORKERS = 2 if torch.cuda.is_available() else 1 \n\n# initialize learning rate, number of epochs to train for, and the\n# batch size ( in general small batch size has 256 samples, here due to 2 batch ss we have )\nINIT_LR = 1e-3\nNUM_EPOCHS = 20\nBATCH_SIZE = 2\nPATCH_SIZE = 128\nWEIGHT_DECAY = 1e-6\n\n\n# define the input image dimensions\n# INPUT_IMAGE_WIDTH = 2048\n# INPUT_IMAGE_HEIGHT = 1536\n\nINPUT_IMAGE_WIDTH = PATCH_SIZE\nINPUT_IMAGE_HEIGHT = PATCH_SIZE\n# num_augmentations = 2\nNUM_AUGMENTATION = 4\n\n#learning rate scheduler \nPatience = 4\nMAX_lr = 1e-1\nBASE_lr = 1e-6\nSTEP_SIZE = 50\n\n# define thresholds for early stopping, for accuracy calculation and predcitions \nEARLY_STOP_THRES = 3\nbest_accuracy = -1\nbest_epoch = -1\nTHRESHOLD = 0.5\n\nBASE_OUTPUT = \"/kaggle/working/\"\n# define the path to the output serialized model, model training\n# plot, and testing image paths\nMODEL_PATH = os.path.join(BASE_OUTPUT, \"unet_spheroid.pth\")\n# PLOT_PATH = os.path.sep.join([BASE_OUTPUT, \"plot.png\"])\n# TEST_PATHS = os.path.sep.join([BASE_OUTPUT, \"test_paths.txt\"])\n\n# define the path to the base output directory\ngpu_usage() ","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:25:57.187708Z","iopub.execute_input":"2023-12-15T12:25:57.188360Z","iopub.status.idle":"2023-12-15T12:25:57.208845Z","shell.execute_reply.started":"2023-12-15T12:25:57.188324Z","shell.execute_reply":"2023-12-15T12:25:57.208010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CREATING THE CUSTOM DATASET","metadata":{}},{"cell_type":"code","source":"# Define paths to image and mask folders\nimgs_path = '/kaggle/input/myofarm/images/'   #'/Users/sabyasachi/Documents/internship_data/images/'\nfile_list = glob.glob(imgs_path + \"*\")\nprint(file_list)\ninput_folder = file_list[0] #+ \"/Images_input\"\nprint(input_folder, len(input_folder))\nmask_folder = file_list[1]# + \"/Images_target\"\nprint(mask_folder)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:25:58.804476Z","iopub.execute_input":"2023-12-15T12:25:58.804820Z","iopub.status.idle":"2023-12-15T12:25:58.823498Z","shell.execute_reply.started":"2023-12-15T12:25:58.804794Z","shell.execute_reply":"2023-12-15T12:25:58.822517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def readData(imgPath, labelPath, convertType):\n    \"\"\"Reads and creates a list of target\"\"\" \n    image = []\n    label = []\n    imageList = []\n    labelList = []\n  \n    for i, image_name in enumerate(sorted(os.listdir(imgPath))):\n        if ((image_name.split('.')[1] == 'tif') or (image_name.split('.')[1] == 'tiff')):\n            \n            img_Path = os.path.join(imgPath, image_name)\n            img = Image.open(img_Path).convert(convertType)\n            # normalise by 255.0 -> convert to array -> append to list\n            image.append(np.array(img, dtype = np.float32)/255.0)\n            imageList.append(np.array(img_Path))\n            \n            label_name = '.'.join(image_name.split('.')[:-1]) +  '_bn.tif'\n            label_Path = os.path.join(labelPath, label_name)\n            img = Image.open(label_Path).convert(convertType)\n            # convert to array -> binarise -> append to list\n            label.append(np.where(np.array(img) >= 1, 1.0, 0.0)) \n            labelList.append(np.array(label_Path))\n            \n            \n    return image, imageList, label, labelList\n            \ninput_folder = '/kaggle/input/myofarm/images/Images_input'\nlabel_folder = '/kaggle/input/myofarm/images/Images_target'\nimage, imageList, label, labelList = readData(input_folder, label_folder, convertType = 'L')   \n\nimage[0].shape, imageList[0], label[0].shape, labelList[0], image[0].max(), label[0].max()","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:25:59.396919Z","iopub.execute_input":"2023-12-15T12:25:59.397285Z","iopub.status.idle":"2023-12-15T12:26:05.883955Z","shell.execute_reply.started":"2023-12-15T12:25:59.397248Z","shell.execute_reply":"2023-12-15T12:26:05.883000Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plotSanityCheckImages(img1, img2):\n    \n    print(f'Images:{img1.shape} and mask:{img2.shape} shape\\n')\n    \n    figure, ax = plt.subplots(nrows=1, ncols=3, figsize=(10,10))\n    ax[0].imshow(img1)\n    ax[1].imshow(img2, cmap = 'gray')\n    ax[2].imshow(img2 * img1, cmap = 'gray')\n\n    ax[0].set_title(\"Image with the original channel\")\n    ax[1].set_title(\"Original Mask\")\n    ax[2].set_title(\"combined and made gray\")\n\n    figure.tight_layout()\n    figure.show()\n\nplotSanityCheckImages(image[0], label[0])","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:26:05.885966Z","iopub.execute_input":"2023-12-15T12:26:05.886608Z","iopub.status.idle":"2023-12-15T12:26:07.448744Z","shell.execute_reply.started":"2023-12-15T12:26:05.886571Z","shell.execute_reply":"2023-12-15T12:26:07.447808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_plot(origImage, origMask, predMask):\n#     print('prepare_plot predMask min and max:', predMask.min(), predMask.max())\n    # initialize our figure\n    figure, ax = plt.subplots(nrows=1, ncols=3, figsize=(10,10))\n    ax[0].imshow(origImage)\n    ax[1].imshow(origImage * origMask)\n    ax[2].imshow(origImage * predMask)\n\n    ax[0].set_title(\"Image\")\n    ax[1].set_title(\"Original Mask\")\n    ax[2].set_title(\"Predicted Mask\")\n\n    figure.tight_layout()\n    figure.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:26:07.449814Z","iopub.execute_input":"2023-12-15T12:26:07.450084Z","iopub.status.idle":"2023-12-15T12:26:07.456177Z","shell.execute_reply.started":"2023-12-15T12:26:07.450060Z","shell.execute_reply":"2023-12-15T12:26:07.455458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read about differet kinds of augmentations\n\ndef data_transform(image, mask):\n    \n    transform = A.Compose([\n        A.HorizontalFlip(p = 0.5),\n        A.VerticalFlip(p = 0.5),\n        A.RandomBrightnessContrast(p=0.5),\n        A.ElasticTransform(p=0.5),\n        A.GridDistortion(p = 0.5),\n    ])\n    \n    # convert the images into array \n    # create a list\n    image = np.float32(image)/255.0\n    images_list = [np.array(image)]\n    masks_list = [np.array(mask)]\n    \n#     for i in tqdm(range(4), desc=\"Augmenting Images\"):\n    for i in range(NUM_AUGMENTATION):\n        augmentations = transform(image = np.array(image), mask = np.array(mask))\n        augmented_img = augmentations[\"image\"]\n        augmented_mask = augmentations[\"mask\"]\n        images_list.append(augmented_img)\n        masks_list.append(augmented_mask)\n        \n    return images_list, masks_list\n    \nprint('To check the augmnetion patterns')\n\n# img_Path = '/kaggle/input/myofarm/images/Images_input/20220912_JK_isRASb1_mc_p22_from_2.5_mio_220024_5uM_CHIR_dish2.tif'\n# mask_Path = '/kaggle/input/myofarm/images/Images_target/20220912_JK_isRASb1_mc_p22_from_2.5_mio_220024_5uM_CHIR_dish2_bn.tif'\n# # img_PIL = Image.open(img_Path).convert(\"RGB\")\n# mask_PIL = Image.open(mask_Path).convert(\"L\")\n    \n# images_list, masks_list = data_transform(img_PIL, mask_PIL)\n\n# for i, (img, mask) in enumerate(zip(images_list, masks_list)):\n#         prepare_plot(img, mask, img*(mask.reshape(1536,2048,1)>0))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:26:07.458534Z","iopub.execute_input":"2023-12-15T12:26:07.458830Z","iopub.status.idle":"2023-12-15T12:26:07.470317Z","shell.execute_reply.started":"2023-12-15T12:26:07.458806Z","shell.execute_reply":"2023-12-15T12:26:07.469429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def createPATCHES(imgList, maskList, PATCH_SIZE):\n    images = []\n    masks = []\n    for i, (image, mask) in enumerate(zip(imgList, maskList)):      \n        patch_images = patchify(image, (PATCH_SIZE,PATCH_SIZE), step = PATCH_SIZE)\n        patch_masks = patchify(mask, (PATCH_SIZE,PATCH_SIZE), step = PATCH_SIZE)\n        \n        for i in range(patch_images.shape[0]):\n            for j in range(patch_images.shape[1]):\n                single_patch_img = patch_images[i,j,:,:]\n                images.append(single_patch_img)\n                \n                single_patch_img = patch_masks[i,j,:,:]\n                masks.append(single_patch_img)\n                \n    images = np.array(images)\n    images = torch.reshape(torch.tensor(images), [-1,1,PATCH_SIZE,PATCH_SIZE])       \n    masks = np.array(masks)\n    masks = torch.reshape(torch.tensor(masks), [-1,1,PATCH_SIZE,PATCH_SIZE])\n        \n    return images, masks    \n        \n\n    \n# image_patch, mask_patch = createPATCHES(images_list, masks_list,PATCH_SIZE )\n    ","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:26:07.471359Z","iopub.execute_input":"2023-12-15T12:26:07.471649Z","iopub.status.idle":"2023-12-15T12:26:07.480320Z","shell.execute_reply.started":"2023-12-15T12:26:07.471625Z","shell.execute_reply":"2023-12-15T12:26:07.479421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Suppose NUM_AUGMENTATION is set to 15, and you have 10 original images in your dataset. So, the total number of items in the augmented dataset would be 10 * 15 = 150.\n\nWhen idx is 25:\noriginal_idx = 25 // 15 = 1\naugmentation_idx = 25 % 15 = 10\nThis means the current index corresponds to the 10th augmentation of the 2nd original image.\nWhen idx is 43:\noriginal_idx = 43 // 15 = 2\naugmentation_idx = 43 % 15 = 13\nThis means the current index corresponds to the 13th augmentation of the 3rd original image.\n\n\nhe createPATCHES function likely processes multiple images and masks at once. By passing [augmented_img] and [augmented_mask] (which are lists), you're providing the function with the necessary format for handling multiple images and masks, even though in this case, you're passing only one of each.","metadata":{}},{"cell_type":"code","source":"class SpheroidDataset(Dataset):\n    \n    def __init__(self, imagePaths, maskPaths, transforms, num_augmentations = NUM_AUGMENTATION):\n        self.imagePaths = imagePaths\n        self.maskPaths = maskPaths\n        self.transforms = transforms\n        self.num_augmentations = NUM_AUGMENTATION\n\n#         Get list of image and target file paths\n        self.imageFiles = sorted(os.listdir(imagePaths))\n        self.maskFiles = sorted(os.listdir(maskPaths))\n        \n\n    def __len__(self):\n        # Calculate the new length of the dataset\n        original_length = len(self.imageFiles)\n        augmented_length = original_length * self.num_augmentations\n        return augmented_length\n    \n    \n    def __getitem__(self, idx):\n        # getitem iterates through the files and gets the index as well\n        \n        # Calculate the original index and augmentation index\n        original_idx = idx // self.num_augmentations\n        augmentation_idx = idx % self.num_augmentations\n\n        # grab the image name from the current index  \n        # Read the image perform augmentation \n        imagePath = self.imageFiles[original_idx]\n#         print('here verify the image name', imagePath)\n        if imagePath == '.DS_Store':\n            idx += 1\n            imagePath = self.imageFiles[original_idx]\n        img_Path = os.path.join(self.imagePaths, imagePath)\n        img_PIL = Image.open(img_Path).convert(\"L\")\n        \n        maskPath = '.'.join(imagePath.split('.')[:-1]) + '_bn.tif'\n        mask_Path = os.path.join(self.maskPaths,maskPath)\n#         mask_PIL = Image.open(mask_Path).convert(\"L\")\n        mask = np.array(Image.open(mask_Path).convert(\"L\"), dtype=np.float32)\n#         mask[mask > 0.1] = 1.0\n        mask = np.where(mask>=1, 1.0, 0.0)\n        \n        images_list, masks_list = data_transform(img_PIL, mask)\n         # Retrieve the augmented image and mask for the current augmentation index\n        augmented_img = images_list[augmentation_idx]\n        augmented_mask = masks_list[augmentation_idx]\n        \n#         image_patch, mask_patch = createPATCHES(images_list, masks_list,PATCH_SIZE )\n        image_patch, mask_patch = createPATCHES([augmented_img], [augmented_mask],PATCH_SIZE )\n        \n               \n        return image_patch, mask_patch, img_Path, mask_Path, imagePath, maskPath \n\ngpu_usage() \n\ndataset = SpheroidDataset(input_folder, mask_folder, transforms = None)\n\n\nfor images, masks, img_Path, mask_Path, imagePath, maskPath in dataset:    \n    print(images.shape) #print path and others also for checking\n    print(torch.min(images), torch.mean(images), torch.max(images))\n    print(masks.shape)\n    print('\\n')\n    break\n    \nlen(dataset)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:26:07.481432Z","iopub.execute_input":"2023-12-15T12:26:07.481765Z","iopub.status.idle":"2023-12-15T12:26:20.956030Z","shell.execute_reply.started":"2023-12-15T12:26:07.481729Z","shell.execute_reply":"2023-12-15T12:26:20.955088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check Point Early stopping ","metadata":{}},{"cell_type":"code","source":"# this is for the full model saving \ndef save_checkpoint(state, filename):\n    print(\"=> Saving complete model \")\n    torch.save(state, filename)\n    \n# this is just to save the model parameters not very comprehensive\ndef checkpoint(model, filename):\n    print(\"=> Saving all model parameters \")\n    torch.save(model.state_dict(), filename)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:26:20.957078Z","iopub.execute_input":"2023-12-15T12:26:20.957382Z","iopub.status.idle":"2023-12-15T12:26:20.962524Z","shell.execute_reply.started":"2023-12-15T12:26:20.957356Z","shell.execute_reply":"2023-12-15T12:26:20.961589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LOSS Functions and Accuracy metric\n","metadata":{}},{"cell_type":"code","source":"class DiceLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceLoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n#         inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        intersection = (inputs * targets).sum()                            \n        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n        \n        return 1 - dice","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:26:20.963978Z","iopub.execute_input":"2023-12-15T12:26:20.964280Z","iopub.status.idle":"2023-12-15T12:26:20.977669Z","shell.execute_reply.started":"2023-12-15T12:26:20.964247Z","shell.execute_reply":"2023-12-15T12:26:20.976810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#PyTorch\nclass DiceBCELoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceBCELoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n#         inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        intersection = (inputs * targets).sum()                            \n        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n        Dice_BCE = BCE + dice_loss\n        \n        return Dice_BCE","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:26:20.978834Z","iopub.execute_input":"2023-12-15T12:26:20.979210Z","iopub.status.idle":"2023-12-15T12:26:20.988706Z","shell.execute_reply.started":"2023-12-15T12:26:20.979174Z","shell.execute_reply":"2023-12-15T12:26:20.987918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#PyTorch\nALPHA = 0.8\nGAMMA = 2\n\nclass FocalLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(FocalLoss, self).__init__()\n\n    def forward(self, inputs, targets, alpha=ALPHA, gamma=GAMMA, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        #first compute binary cross-entropy \n        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n        BCE_EXP = torch.exp(-BCE)\n        focal_loss = alpha * (1-BCE_EXP)**gamma * BCE\n                       \n        return focal_loss","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:26:20.991782Z","iopub.execute_input":"2023-12-15T12:26:20.992098Z","iopub.status.idle":"2023-12-15T12:26:20.999167Z","shell.execute_reply.started":"2023-12-15T12:26:20.992073Z","shell.execute_reply":"2023-12-15T12:26:20.998280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy(preds, targets, threshold=0.5):\n    # Apply threshold to predicted masks\n    preds = (preds > threshold).float()\n    \n    # Flatten the tensors\n    preds = preds.view(-1)\n    targets = targets.view(-1)\n    \n    # Calculate pixel-wise accuracy\n    correct = (preds == targets).float()\n    acc = correct.sum() / targets.numel()\n    \n    return acc","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:26:21.000177Z","iopub.execute_input":"2023-12-15T12:26:21.000468Z","iopub.status.idle":"2023-12-15T12:26:21.011991Z","shell.execute_reply.started":"2023-12-15T12:26:21.000444Z","shell.execute_reply":"2023-12-15T12:26:21.011107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building UNET\n\n- Overall, our U-Net model will consist of an Encoder class and a Decoder class. The encoder will gradually reduce the spatial dimension to compress information. Furthermore, it will increase the number of channels, that is, the number of feature maps at each stage, enabling our model to capture different details or features in our image. On the other hand, the decoder will take the final encoder representation and gradually increase the spatial dimension and reduce the number of channels to finally output a segmentation mask of the same spatial dimension as the input image.\n\n- Next, we define a Block module as the building unit of our encoder and decoder architecture. It is worth noting that all models or model sub-parts that we define are required to inherit from the PyTorch Module class, which is the parent class in PyTorch for all neural network modules.","metadata":{}},{"cell_type":"markdown","source":"### Creating the model \n- here mutiple kind of experimenst can be done taking inspiratin from cellpose","metadata":{}},{"cell_type":"code","source":"class Block(Module):\n    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n    \"\"\" Note make sure to change the i/p channel to later on\"\"\"\n    \n    def __init__(self, inChannels, outChannels):#, mid_channels=None):\n        super().__init__()\n        # store the convolution and RELU layers\n        self.conv1 = nn.Conv2d(inChannels, outChannels,\n                            kernel_size=3, stride=1, padding=1, bias=True)\n        self.BN1 = nn.BatchNorm2d(outChannels)\n        self.relu1 = nn.ReLU(inplace = True)\n        self.dropout = nn.Dropout(0.25)\n            \n        self.conv2 = Conv2d(outChannels, outChannels, \n                            kernel_size=3, stride=1, padding=1, bias=True)\n        self.BN2 = BatchNorm2d(outChannels)\n        self.relu2 = ReLU(inplace = True)\n        \n    def forward(self, x):\n        # apply CONV => [BN] => RELU => CONV block to the inputs and return it\n        outputConv1 = self.dropout(self.relu1(self.BN1(self.conv1(x))))\n        outputConv2 = self.relu2(self.BN2(self.conv2(outputConv1)))\n        #self.conv2(self.relu(self.BN(self.conv1(x))))\n        \n        return outputConv2\n    \n# Block()","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:26:21.013040Z","iopub.execute_input":"2023-12-15T12:26:21.013348Z","iopub.status.idle":"2023-12-15T12:26:21.021613Z","shell.execute_reply.started":"2023-12-15T12:26:21.013324Z","shell.execute_reply":"2023-12-15T12:26:21.020760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(Module):\n    \"\"\" Note make sure to change the i/p channel to later on ( #(1, 32, 64, 128, 256, 512))\"\"\"\n    \n    def __init__(self, channels=(1, 32, 64, 128, 256, 512)): \n        super().__init__()\n        # using the block code to get the decoder conv blocks and maxpooling layer\n        self.encBlocks = ModuleList([Block(channels[i], channels[i + 1]) \n                                      for i in range(len(channels) - 1)])\n        self.pool = MaxPool2d(kernel_size=2, stride=2)\n\n        \n    def forward(self, x):\n        # this takes an input image x\n        # initialize an empty list to store the intermediate outputs\n        # Note that this will enable us to later pass these outputs\n        # to that decoder where they can be processed with the decoder feature maps.\n        blockOutputs = []\n        \n        # loop through the encoder blocks \n        for block in self.encBlocks:\n            # pass the inputs through the current encoder block, store\n            # the outputs in blockOutputs list, and then apply maxpooling on the output\n            x = block(x)\n#             print(\"Encoder -forward input shape:\", x.shape)\n            blockOutputs.append(x)\n            x = self.pool(x)\n#             print(\"Encoder -forward output shape:\", x.shape)\n        # return the list containing the intermediate outputs\n        return blockOutputs\n    \n    \n\n# to check    \n# Encoder()","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:26:21.022653Z","iopub.execute_input":"2023-12-15T12:26:21.022980Z","iopub.status.idle":"2023-12-15T12:26:21.034955Z","shell.execute_reply.started":"2023-12-15T12:26:21.022948Z","shell.execute_reply":"2023-12-15T12:26:21.034157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Decoder(Module):\n    def __init__(self, channels=(512, 256, 128, 64, 32)):\n        super().__init__()\n        # initialize the number of channels, upsampler blocks, and decoder blocks\n        self.channels = channels\n        \n        # note the ConvTranspose2d,unlike our block earlier in encoding, \n        # is part of the nn torch module imported earlier\n        self.upconvs = ModuleList([ConvTranspose2d(channels[i], channels[i + 1], kernel_size=2, stride=2)\n                                   for i in range(len(channels) - 1)])\n\n        # using the block code to get the decoder conv blocks \n        self.dec_blocks = ModuleList([Block(channels[i], channels[i + 1])\n                                       for i in range(len(channels) - 1)])\n    \n        # forward for the decoder section \n        # which takes as input our feature map x and \n        # the list of intermediate outputs from the encoder (i.e., encFeatures).\n    def forward(self, x, encFeatures):\n        # loop through the number of channels\n        for i in range(len(self.channels) - 1):\n            # pass the inputs through the upsampler blocks\n            x = self.upconvs[i](x)\n            # crop the current features from the encoder blocks,\n            # concatenate them with the current upsampled features,\n            # and pass the concatenated output through the current\n            # decoder block\n            # encFeat = self.crop(encFeatures[i], x)\n            x = torch.cat([x, encFeatures[i]], dim=1)\n            x = self.dec_blocks[i](x)\n#             print(x.shape)\n        # return the final decoder output\n        return x   \n\n# to chevk    \n# Decoder()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:26:21.038331Z","iopub.execute_input":"2023-12-15T12:26:21.038595Z","iopub.status.idle":"2023-12-15T12:26:21.048114Z","shell.execute_reply.started":"2023-12-15T12:26:21.038571Z","shell.execute_reply":"2023-12-15T12:26:21.047277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- nbClasses: This defines the number of segmentation classes where we have to classify each pixel. This usually corresponds to the number of channels in our output segmentation map, where we have one channel for each class. Since we are working with two classes (i.e., binary classification), we keep a single channel and use thresholding for classification, as we will discuss later.\n\n- Now the encFeatures[::-1] list contains the feature map outputs in reverse order (i.e., from the last to the first encoder block). Note that this is important since, on the decoder side, we will be utilizing the encoder feature maps starting from the last encoder block output to the first.","metadata":{}},{"cell_type":"code","source":"class UNet(Module):\n    \"\"\" Note make sure to change the i/p channel to later on ( #(1, 16, 32, 64, 128, 256, 512))\"\"\"\n        \n    def __init__(self, encChannels=(1, 16, 32, 64, 128, 256, 512),\n                 decChannels=(512, 256, 128, 64, 32, 16), \n                 nbClasses=1, retainDim=True,\n                 outSize=(INPUT_IMAGE_HEIGHT,  INPUT_IMAGE_WIDTH)):\n                 # outSize=(config.INPUT_IMAGE_HEIGHT,  config.INPUT_IMAGE_WIDTH)): if the config block converted into a utility file \n        super().__init__()\n        \n        # initialize the encoder and decoder\n        self.encoder = Encoder(encChannels)\n        self.decoder = Decoder(decChannels)\n        \n        # initialize the regression head and store the class variables: the final layer\n#         self.head = Conv2d(decChannels[-1], nbClasses, 1)\n        \n        self.head = nn.Sequential(\n                        nn.Conv2d(decChannels[-1], nbClasses, kernel_size=1),\n                        nn.Sigmoid()\n                        )\n        self.retainDim = retainDim\n        self.outSize = outSize\n        \n        \n    def forward(self, x):\n        # grab the features from the encoder\n        # Note that the encFeatures list contains \n        # all the feature maps starting from the first encoder block output to the last\n        encFeatures = self.encoder(x)\n        # pass the encoder features through decoder making sure that\n        # their dimensions are suited for concatenation\n        # since the encoder feature maps starting from the last encoder block output to the first of the decoder\n       \n        # output of the final encoder block \n        # (i.e., encFeatures[::-1][0]) and the feature map outputs of all intermediate encoder blocks \n        # (i.e., encFeatures[::-1][1:]) to the decoder \n        decFeatures = self.decoder(encFeatures[::-1][0],\n                                   encFeatures[::-1][1:])\n        \n        # pass the decoder features through the regression head to\n        # obtain the segmentation mask\n        map = self.head(decFeatures)\n        \n        # check to see if we are retaining the original output\n        # dimensions and if so, then resize the output to match them\n        if self.retainDim:\n            map = F.interpolate(map, self.outSize)\n        # return the segmentation map\n        return map\n","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:26:21.049299Z","iopub.execute_input":"2023-12-15T12:26:21.049561Z","iopub.status.idle":"2023-12-15T12:26:21.061643Z","shell.execute_reply.started":"2023-12-15T12:26:21.049538Z","shell.execute_reply":"2023-12-15T12:26:21.060771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(UNet().to(DEVICE), input_size=(1,PATCH_SIZE,PATCH_SIZE))","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:26:21.062751Z","iopub.execute_input":"2023-12-15T12:26:21.063061Z","iopub.status.idle":"2023-12-15T12:26:27.895634Z","shell.execute_reply.started":"2023-12-15T12:26:21.063037Z","shell.execute_reply":"2023-12-15T12:26:27.894618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"UNet().to(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:26:27.896970Z","iopub.execute_input":"2023-12-15T12:26:27.897314Z","iopub.status.idle":"2023-12-15T12:26:27.991690Z","shell.execute_reply.started":"2023-12-15T12:26:27.897284Z","shell.execute_reply":"2023-12-15T12:26:27.990795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the UNET model","metadata":{}},{"cell_type":"markdown","source":"- ToPILImage(): it enables us to convert our input images to PIL image format. Note that this is necessary since we used OpenCV to load images in our custom dataset, but PyTorch expects the input image samples to be in PIL format.\n- Resize(): allows us to resize our images to a particular input dimension (i.e., config.INPUT_IMAGE_HEIGHT, config.INPUT_IMAGE_WIDTH) that our model can accept\n- ToTensor(): enables us to convert input images to PyTorch tensors and convert the input PIL Image, which is originally in the range from [0, 255], to [0, 1].\n","metadata":{}},{"cell_type":"code","source":"# BATCH_SIZE = 5\n# VALIDATION_SPLIT = 20\ntrain_dataset = SpheroidDataset(input_folder, mask_folder, transforms = None)\ntrain_set, validation_set = torch.utils.data.random_split(train_dataset, \n                                                          [len(train_dataset) - len(train_dataset)//VALIDATION_SPLIT, \n                                                           len(train_dataset)//VALIDATION_SPLIT])\ntrainLoader = DataLoader(dataset = train_set, batch_size = BATCH_SIZE, num_workers = NUM_WORKERS, shuffle =True)\nvalidationLoader = DataLoader(dataset = validation_set, batch_size = BATCH_SIZE, num_workers = NUM_WORKERS, shuffle =True) \n\nprint(f\"Total {len(train_set)} examples in the training set\")\nprint(f\"Total {len(validation_set)} examples in the validation set\")\nprint(f\"Total number of train sets: {len(trainLoader)},and validation loader {len(validationLoader)}\")\n\ngpu_usage() ","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:26:27.992983Z","iopub.execute_input":"2023-12-15T12:26:27.993363Z","iopub.status.idle":"2023-12-15T12:26:28.015333Z","shell.execute_reply.started":"2023-12-15T12:26:27.993329Z","shell.execute_reply":"2023-12-15T12:26:28.014484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initialize our UNet model\nunet = UNet().to(DEVICE)\n# initialize loss function and optimizerimizer\nmetric_lossFunc = DiceBCELoss() #nn.BCELoss() #DiceLoss()\noptimizer = optim.Adam(unet.parameters(), lr= INIT_LR, weight_decay = WEIGHT_DECAY)\n# optimizer = torch.optim.SGD(unet.parameters(), lr=INIT_LR, momentum=0.9, weight_decay = WEIGHT_DECAY)\n# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor= 0.1, patience=Patience, verbose = True)\n# scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1, cycle_momentum = True, verbose = True)\n# calculate steps per epoch for training and test set\ntrainSteps = len(train_set) // BATCH_SIZE\ntestSteps = len(validation_set) // BATCH_SIZE\n# initialize a dictionary to store training history\nH = {\"train_loss\": [], \"validation_loss\": [], \"model_accuracy\":[]}\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:26:28.016455Z","iopub.execute_input":"2023-12-15T12:26:28.016729Z","iopub.status.idle":"2023-12-15T12:26:28.099971Z","shell.execute_reply.started":"2023-12-15T12:26:28.016705Z","shell.execute_reply":"2023-12-15T12:26:28.099070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loop over epochs\nprint(\"[INFO] training the network...\")\nstartTime = time.time()\nfor epoch in tqdm(range(NUM_EPOCHS)):\n    # set the model in training mode - initialise train and val loss - loop on train set\n    unet.train()\n    totalTrainLoss = 0\n    totalValidationLoss_model = 0\n    totalValidationLoss_metric = 0\n    gpu_usage() \n    for (i, (x, y, img_Path, mask_Path, imagePath, maskPath)) in enumerate(trainLoader):\n        # send the input to the device - forward pass - train loss - zero out prev gradients \n        # - back propagation - update model params - add train loss (.item makes it float)\n        #\"\"\" Note make sure to change the i/p channel to later on ([-1, 1, patch_size, patch_size])\"\"\"\n        x = torch.reshape(x,[-1,1,PATCH_SIZE,PATCH_SIZE]).float()\n        y = torch.reshape(y,[-1,1,PATCH_SIZE,PATCH_SIZE]).float()\n        (x, y) = (x.to(DEVICE), y.to(DEVICE))\n        #print(x.shape)\n        pred = unet(x) \n        loss = metric_lossFunc(pred.to(torch.float32), y.to(torch.float32))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        totalTrainLoss += loss.item()\n        \n    # switch off autograd\n    # Beging the validation set - inputs to device - val loss calculations\n    with torch.no_grad():\n        unet.eval()\n        for (x, y, img_Path, mask_Path, imagePath, maskPath) in validationLoader:\n            \"\"\" Note make sure to change the i/p channel to later on ([-1, 1, patch_size, patch_size])\"\"\"\n            x = torch.reshape(x,[-1,1,PATCH_SIZE,PATCH_SIZE]).float()\n            y = torch.reshape(y,[-1,1,PATCH_SIZE,PATCH_SIZE]).float()\n            (x, y) = (x.to(DEVICE), y.to(DEVICE))\n            pred = unet(x)\n            totalValidationLoss_metric += metric_lossFunc(pred.to(torch.float32), y.to(torch.float32)).item()\n            acc = accuracy(pred.to(torch.float32), y.to(torch.float32)).item()\n    # calculate the average training and validation loss and binary accuracy\n    avgTrainLoss = totalTrainLoss / trainSteps\n    avgValidationLoss = totalValidationLoss_metric / testSteps\n    avgAcc = acc / testSteps\n    \n    H[\"train_loss\"].append(avgTrainLoss)\n    H[\"validation_loss\"].append(avgValidationLoss) \n    H[\"model_accuracy\"].append(avgAcc) \n    \n    # print the model training and validation information\n    print(\"[INFO] EPOCH: {}/{}\".format(epoch + 1, NUM_EPOCHS))\n    print(\"Train loss: {:.6f}, Validation loss: {:.4f},Val accuracy: {:.2f}%\".format(avgTrainLoss, \n                                                                                     avgValidationLoss, \n                                                                                     avgAcc))\n    \n    # terminate the training loop\n#     if avgAcc > best_accuracy:\n#         best_accuracy = avgAcc\n#         best_epoch = epoch\n#         checkpoint(unet, MODEL_PATH)\n#     elif epoch - best_epoch > EARLY_STOP_THRES:\n#         print(\"Early stopped training at epoch %d\" % epoch)\n#         break  \n    \n    # Step the scheduler based on training loss \n#     scheduler.step(avgTrainLoss)\n    # Step the scheduler based on validation loss \n#     scheduler.step(avgValidationLoss)\n    \n# display the total time needed to perform the training\nendTime = time.time()\nprint(\"[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:26:28.101162Z","iopub.execute_input":"2023-12-15T12:26:28.101459Z","iopub.status.idle":"2023-12-15T12:48:01.404176Z","shell.execute_reply.started":"2023-12-15T12:26:28.101434Z","shell.execute_reply":"2023-12-15T12:48:01.403054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# H['train_loss']\ngpu_usage()","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:48:01.405788Z","iopub.execute_input":"2023-12-15T12:48:01.406139Z","iopub.status.idle":"2023-12-15T12:48:01.423717Z","shell.execute_reply.started":"2023-12-15T12:48:01.406107Z","shell.execute_reply":"2023-12-15T12:48:01.422860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# plot the training loss\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(H[\"train_loss\"], label=\"train_loss\")\nplt.plot(H[\"validation_loss\"], label=\"validation_loss\")\nplt.plot(H[\"avgAcc\"], label=\"accuracy\")\nplt.title(f\"Losses for version:-\\n{plotName}\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss\")\nplt.legend(loc=\"lower left\")\n# plt.savefig(config.PLOT_PATH)\n# serialize the model to disk\ntorch.save(unet, MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T06:20:33.380771Z","iopub.execute_input":"2023-12-19T06:20:33.381364Z","iopub.status.idle":"2023-12-19T06:20:33.883286Z","shell.execute_reply.started":"2023-12-19T06:20:33.381320Z","shell.execute_reply":"2023-12-19T06:20:33.881095Z"},"trusted":true},"execution_count":14,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend(loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlower left\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# plt.savefig(config.PLOT_PATH)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# serialize the model to disk\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39msave(unet, MODEL_PATH)\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"],"ename":"NameError","evalue":"name 'torch' is not defined","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model used for prediction of segemnted images","metadata":{}},{"cell_type":"code","source":"\ndef prepare_plot(origImage, origMask, predMask, threshold = None):\n#     print('prepare_plot predMask min and max:', predMask.min(), predMask.max())\n    # initialize our figure\n    print(threshold)\n    figure, ax = plt.subplots(nrows=1, ncols=4, figsize=(15, 15))\n    ax[0].imshow(origImage)\n    ax[1].imshow(origMask)\n    ax[2].imshow(predMask)\n    ax[3].imshow(predMask > (predMask.max() - threshold))\n#     ax[3].hist(predMask.flatten()*255)\n\n    ax[0].set_title(\"Image\")\n    ax[1].set_title(\"Original Mask\")\n    ax[2].set_title(\"P_Mask w/o thresh\")\n    ax[3].set_title('P_mask w threshold')\n\n    figure.tight_layout()\n    figure.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-15T13:36:36.520335Z","iopub.execute_input":"2023-12-15T13:36:36.520716Z","iopub.status.idle":"2023-12-15T13:36:36.528115Z","shell.execute_reply.started":"2023-12-15T13:36:36.520683Z","shell.execute_reply":"2023-12-15T13:36:36.527002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_predictions(model, imagePath, groundTruthPath, patch_size, threshold = None):\n    model.eval()\n    with torch.no_grad():\n        image = Image.open(imagePath).convert(\"L\")\n        image = np.float32(image)/255.0\n        \n        gtMask = np.array(Image.open(groundTruthPath).convert(\"L\"))\n        gtMask[gtMask > 0] = 1.0\n    \n        height, width = image.shape[:2]\n        segm_img = np.zeros((height, width), dtype=np.uint8)  # Array with zeros to be filled with segmented values\n    \n        patch_num = 1\n    \n        for i in range(0, height, patch_size):  \n            for j in range(0, width, patch_size):  \n                single_patch = image[i:i+patch_size, j:j+patch_size]\n                single_patch = np.expand_dims(single_patch, 0)\n                single_patch = np.expand_dims(single_patch, 0)\n                orig_patch = single_patch.copy()\n                single_patch = torch.from_numpy(single_patch).to(DEVICE)\n                \n                Mask_patch = gtMask[i:i+patch_size, j:j+patch_size]\n               \n                # make the prediction, pass the results through the sigmoid\n                # function, and convert the result to a NumPy array\n                single_patch_prediction = torch.sigmoid(model(single_patch)).squeeze().cpu().numpy()\n                \n                # filter out the weak predictions and convert them to integers\n                single_patch_prediction = (single_patch_prediction*255.0).astype(np.uint8)\n                \n                \n                single_patch_shape = single_patch_prediction.shape[:2]\n                segm_img[i:i+single_patch_shape[0], j:j+single_patch_shape[1]] += cv2.resize(single_patch_prediction, single_patch_shape[::-1])\n            \n            print(\"Finished processing patch number \", patch_num, \" at position \", i, j)\n            print('\\nmax value:-->', single_patch_prediction.max(),'\\nmin value:-->', single_patch_prediction.min() )\n            patch_num += 1\n            \n    prepare_plot(image, gtMask, segm_img,threshold)\n    return single_patch_prediction#, segm_img\n\n# load our model from disk and flash it to the current device\nprint(\"[INFO] load up model...\")\nunet = torch.load(MODEL_PATH).to(DEVICE)\n\nprint(\"[INFO] loading up test image paths...\")\nfiles = np.random.choice(os.listdir(input_folder), size = 1)\nimagePaths = []\n\n\nfor file in files:\n    imagePath = input_folder+ '/' + file\n    groundTruthPath = mask_folder + '/' + '.'.join(file.split('.')[:-1]) + '_bn.tif'\n    print(imagePath, groundTruthPath)\n    simg = make_predictions(unet, imagePath, groundTruthPath, PATCH_SIZE, threshold = 2)\n\n\n\n# img = simg.copy()\n\n# print( img.shape,img.dtype, img.max(), img.min())\n# plt.imshow(img>(img.max() - 1.5))\n# plt.hist(img.flatten())","metadata":{"execution":{"iopub.status.busy":"2023-12-15T13:38:15.097060Z","iopub.execute_input":"2023-12-15T13:38:15.097728Z","iopub.status.idle":"2023-12-15T13:38:17.431995Z","shell.execute_reply.started":"2023-12-15T13:38:15.097695Z","shell.execute_reply":"2023-12-15T13:38:17.430959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-12-15T13:33:38.972704Z","iopub.execute_input":"2023-12-15T13:33:38.973633Z","iopub.status.idle":"2023-12-15T13:33:39.223410Z","shell.execute_reply.started":"2023-12-15T13:33:38.973595Z","shell.execute_reply":"2023-12-15T13:33:39.222422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}