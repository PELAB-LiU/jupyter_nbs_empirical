{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d037800",
   "metadata": {},
   "source": [
    "# Resample to the desired sample sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c824705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(config)\n",
    "# import utils.config as config\n",
    "\n",
    "n_iter = 6 # 0, 1, 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb9b7a",
   "metadata": {},
   "source": [
    "## Exclude notebooks that do not fit into any ML pipeline stages, e.g., tutoral notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a01b9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390\n",
      "356\n",
      "390\n",
      "356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yirwa29\\AppData\\Local\\anaconda3\\envs\\chatgpt\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import utils.config as config\n",
    "import numpy as np\n",
    "\n",
    "# what have been labeled\n",
    "df_mlerr_labels = pd.read_excel(config.path_default.joinpath('tmp/cluster_sampled_labeled_{}.xlsx'.format(n_iter)),\n",
    "                                sheet_name = \"Del-All\",\n",
    "                                keep_default_na=False)\n",
    "\n",
    "df_mlerr_labels_g_sum = df_mlerr_labels[df_mlerr_labels.nb_source==config.NB_SOURCE[\"github\"]]\n",
    "df_mlerr_labels_k_sum = df_mlerr_labels[df_mlerr_labels.nb_source==config.NB_SOURCE[\"kaggle\"]]\n",
    "\n",
    "# before filtering\n",
    "print(df_mlerr_labels_g_sum.eid.nunique())\n",
    "print(df_mlerr_labels_k_sum.eid.nunique())\n",
    "\n",
    "exclude_other = ['should exclude', \"intentional\"] #[should exclude']\n",
    "exclude_ml_pipeline_sum = config.label_ML_pipeline[\"no ML pipeline\"]\n",
    "\n",
    "exclude_g_filenames = df_mlerr_labels_g_sum[(df_mlerr_labels_g_sum.label_ML_pipeline.isin(exclude_ml_pipeline_sum))|(df_mlerr_labels_g_sum.other.isin(exclude_other))].fname\n",
    "exclude_k_filenames = df_mlerr_labels_k_sum[(df_mlerr_labels_k_sum.label_ML_pipeline.isin(exclude_ml_pipeline_sum))|(df_mlerr_labels_k_sum.other.isin(exclude_other))].fname\n",
    "\n",
    "df_mlerr_labels_g_sum = df_mlerr_labels_g_sum[(~df_mlerr_labels_g_sum.label_ML_pipeline.isin(exclude_ml_pipeline_sum))&(~df_mlerr_labels_g_sum.other.isin(exclude_other))]\n",
    "df_mlerr_labels_k_sum = df_mlerr_labels_k_sum[(~df_mlerr_labels_k_sum.label_ML_pipeline.isin(exclude_ml_pipeline_sum))&(~df_mlerr_labels_k_sum.other.isin(exclude_other))]\n",
    "\n",
    "# after filtering\n",
    "print(df_mlerr_labels_g_sum.eid.nunique())\n",
    "print(df_mlerr_labels_k_sum.eid.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b2836e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388\n",
      "336\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# until the 6th iteration, the resampling ends\n",
    "print(df_mlerr_labels_g_sum.fname.nunique())\n",
    "print(df_mlerr_labels_k_sum.fname.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82c9dc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "my_file = open(config.path_default.joinpath('tmp/exclude_g_filenames.txt'), \"r\") \n",
    "exclude_g_filenames_exist = my_file.read() \n",
    "exclude_g_filenames_exist = exclude_g_filenames_exist.split(\"\\n\") \n",
    "my_file.close() \n",
    "\n",
    "my_file = open(config.path_default.joinpath('tmp/exclude_k_filenames.txt'), \"r\") \n",
    "exclude_k_filenames_exist = my_file.read() \n",
    "exclude_k_filenames_exist = exclude_k_filenames_exist.split(\"\\n\") \n",
    "my_file.close() \n",
    "\n",
    "for i in exclude_g_filenames:\n",
    "    exclude_g_filenames_exist.append(i)\n",
    "for i in exclude_k_filenames:\n",
    "    exclude_k_filenames_exist.append(i)\n",
    "    \n",
    "exclude_g_filenames_exist = set(exclude_g_filenames_exist)\n",
    "exclude_k_filenames_exist = set(exclude_k_filenames_exist)\n",
    "\n",
    "print(len(exclude_g_filenames_exist))\n",
    "print(len(exclude_k_filenames_exist))\n",
    "\n",
    "with open(config.path_default.joinpath('tmp/exclude_g_filenames.txt'), 'w') as f:\n",
    "    for line in exclude_g_filenames_exist:\n",
    "        f.write(f\"{line}\\n\")\n",
    "with open(config.path_default.joinpath('tmp/exclude_k_filenames.txt'), 'w') as f:\n",
    "    for line in exclude_k_filenames_exist:\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3333cb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need resample 1 more for GitHub, and 0 more for Kaggle.\n"
     ]
    }
   ],
   "source": [
    "print(\"need resample {} more for GitHub, and {} more for Kaggle.\".format(len(exclude_g_filenames), len(exclude_k_filenames)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ae6003",
   "metadata": {},
   "source": [
    "## Get the overall population from GitHub and Kaggle\n",
    "\n",
    "We sample for GitHub and Kaggle separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "851f4778",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_err_grouped_k = pd.read_excel(config.path_default.joinpath('Clustering/clusters_Kaggle.xlsx'))\n",
    "df_err_grouped_g = pd.read_excel(config.path_default.joinpath('Clustering/clusters_GitHub.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95c255c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88667\n",
      "61342\n",
      "3875\n",
      "2689\n"
     ]
    }
   ],
   "source": [
    "print((df_err_grouped_g.eid.nunique()))\n",
    "print((df_err_grouped_g.fname.nunique()))\n",
    "\n",
    "print((df_err_grouped_k.eid.nunique()))\n",
    "print((df_err_grouped_k.fname.nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7794e3",
   "metadata": {},
   "source": [
    "## Get \"proportional sampling to cluster size\" config\n",
    "\n",
    "390 GH, 356 Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb393364",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_clusters_g = pd.read_excel(config.path_default.joinpath('tmp/df_err_processed_pregroup_cluster_size_samples_g.xlsx'))\n",
    "selected_clusters_k = pd.read_excel(config.path_default.joinpath('tmp/df_err_processed_pregroup_cluster_size_samples_k.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3462799c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390\n",
      "356\n"
     ]
    }
   ],
   "source": [
    "print(sum(selected_clusters_g.sample_size))\n",
    "print(sum(selected_clusters_k.sample_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffce95f",
   "metadata": {},
   "source": [
    "## Resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "273bb90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_g_new = None\n",
    "sample_k_new = None\n",
    "\n",
    "for _, row in selected_clusters_g.iterrows():\n",
    "    sc_id = row.cluster_id\n",
    "    df_sample_size_g = selected_clusters_g[selected_clusters_g.cluster_id==sc_id]\n",
    "    sample_size_g = df_sample_size_g.sample_size.iloc[0] if len(df_sample_size_g) > 0 else 0\n",
    "    \n",
    "    # github\n",
    "    df_already_sampled_g = df_mlerr_labels_g_sum.loc[(df_mlerr_labels_g_sum.cluster_id == sc_id)]\n",
    "    if len(df_already_sampled_g) < sample_size_g:\n",
    "        # sample more, not repeat\n",
    "        tmp_g = df_err_grouped_g[(df_err_grouped_g.cluster_id == sc_id)]\n",
    "        tmp_g = tmp_g[~tmp_g[\"eid\"].isin(df_already_sampled_g.eid.tolist())]\n",
    "        tmp_g = tmp_g[~tmp_g[\"fname\"].isin(exclude_g_filenames_exist)]\n",
    "        sample_g = tmp_g.sample(n=(sample_size_g-len(df_already_sampled_g)), random_state=30)\n",
    "    else:\n",
    "        sample_g = None\n",
    "    \n",
    "    if sample_g is not None:\n",
    "        sample_g_new = pd.concat([sample_g_new, sample_g], ignore_index=True)\n",
    "    \n",
    "for _, row in selected_clusters_k.iterrows():\n",
    "    sc_id = row.cluster_id\n",
    "    df_sample_size_k = selected_clusters_k[selected_clusters_k.cluster_id==sc_id]\n",
    "    sample_size_k = df_sample_size_k.sample_size.iloc[0] if len(df_sample_size_k) > 0 else 0\n",
    "    \n",
    "    # kaggle\n",
    "    df_already_sampled_k = df_mlerr_labels_k_sum.loc[(df_mlerr_labels_k_sum.cluster_id == sc_id)]\n",
    "    if len(df_already_sampled_k) < sample_size_k:\n",
    "        # sample more, not repeat\n",
    "        tmp_k = df_err_grouped_k[(df_err_grouped_k.cluster_id == sc_id)]\n",
    "        tmp_k = tmp_k[~tmp_k[\"eid\"].isin(df_already_sampled_k.eid.tolist())]\n",
    "        tmp_k = tmp_k[~tmp_k[\"fname\"].isin(exclude_k_filenames_exist)]\n",
    "        sample_k = tmp_k.sample(n=sample_size_k-len(df_already_sampled_k), random_state=30)\n",
    "    else:\n",
    "        sample_k = None\n",
    "    \n",
    "    if sample_k is not None:\n",
    "        sample_k_new = pd.concat([sample_k_new, sample_k], ignore_index=True)\n",
    "        \n",
    "sample_new = pd.concat([sample_g_new, sample_k_new], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf5aa264",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_all = pd.concat([df_mlerr_labels_g_sum, df_mlerr_labels_k_sum, sample_new], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "268a3fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390 390\n",
      "356 356\n"
     ]
    }
   ],
   "source": [
    "assert(sample_all.cluster_id.nunique()==len(set(selected_clusters_g.cluster_id.values).union(set(selected_clusters_k.cluster_id.values))))\n",
    "print(sample_all[sample_all.nb_source==config.NB_SOURCE[\"github\"]].eid.nunique(), sum(selected_clusters_g.sample_size))\n",
    "print(sample_all[sample_all.nb_source==config.NB_SOURCE[\"kaggle\"]].eid.nunique(), sum(selected_clusters_k.sample_size))\n",
    "# assert(sample_all.eid.nunique()==(sum(selected_clusters_g.sample_size)+sum(selected_clusters_k.sample_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d37383d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_all = sample_all.drop(columns=sample_all.columns[~sample_all.columns.isin(df_mlerr_labels_k_sum.columns)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff12fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort based on cluster id for better manual labeling\n",
    "sample_all[\"size_counts\"] = sample_all[\"cluster_id\"].map(sample_all[\"cluster_id\"].value_counts())\n",
    "assert(len(sample_all[sample_all[\"size_counts\"]==1].cluster_id)==sample_all[sample_all[\"size_counts\"]==1].cluster_id.nunique())\n",
    "sample_all.sort_values(by=['size_counts',\"cluster_id\"], ascending=False, inplace=True) # by=['size_counts',\"cluster_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5403255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fname', 'eid', 'ename', 'evalue', 'traceback', 'ename_mapped',\n",
       "       'nb_source', 'evalue_processed', 'pregroup_cluster', 'cluster_id',\n",
       "       'label_ML_pipeline', 'label_if_ML_bug', 'label_refined_exp_type',\n",
       "       'label_if_runinfo_help', 'label_if_code_error_align',\n",
       "       'label_if_error_chain', 'label_root_cause', 'Comment', 'other',\n",
       "       'Labeler', 'Reviewer', 'Review_res', 'Review_note',\n",
       "       'Resolution comment', 'size_counts'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fceba8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_new.to_excel(config.path_default.joinpath(\"tmp/resampled_new.xlsx\"), index=False, engine='xlsxwriter')\n",
    "sample_new.to_csv(config.path_default.joinpath(\"tmp/resampled_new_{}.csv\".format(n_iter+1)), index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3fad61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with pd.ExcelWriter(config.path_default.joinpath(\"tmp/cluster_resampled_{}.xlsx\".format(n_iter+1))) as writer:\n",
    "    sample_all = sample_all.drop(['size_counts'], axis=1)\n",
    "    \n",
    "    sample_all.to_excel(writer, sheet_name=\"Default\", index=False)\n",
    "    sample_all.to_excel(writer, sheet_name=\"Del-All\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d419710",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
